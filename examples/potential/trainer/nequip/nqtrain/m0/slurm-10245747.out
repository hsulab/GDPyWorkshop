Torch device: cuda
Processing dataset...
Loaded data: Batch(atomic_numbers=[6144, 1], batch=[6144], cell=[1024, 3, 3], edge_cell_shift=[416666, 3], edge_index=[2, 416666], forces=[6144, 3], pbc=[1024, 3], pos=[6144, 3], ptr=[1025], total_energy=[1024, 1])
Cached processed data to disk
Done!
Successfully loaded the data set of type ASEDataset(1024)...
Replace string dataset_forces_rms to 0.3348463773727417
Replace string dataset_per_atom_total_energy_mean to -3.5640807151794434
Atomic outputs are scaled by: [Cu: 0.334846], shifted by [Cu: -3.564081].
Replace string dataset_forces_rms to 0.3348463773727417
Initially outputs are globally scaled by: 0.3348463773727417, total_energy are globally shifted by None.
Successfully built the network...
Number of weights: 333368
! Starting training ...

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      0     7          3.4         2.86         0.54         0.16        0.566        0.943          1.2


  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Initial Validation          0    6.139    0.005        0.976          6.9         7.87        0.145        0.302         1.62         3.86
Wall time: 6.1398971090093255
! Best model        0    7.871

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      1    10        0.337        0.258       0.0791       0.0707         0.17        0.469        0.601
      1    20        0.416        0.272        0.145       0.0894        0.175        0.715        0.728
      1    30         1.39         1.38      0.00696        0.228        0.393         0.14        0.158
      1    40        0.216          0.2       0.0153        0.102         0.15        0.176        0.222
      1    50       0.0725       0.0661      0.00636       0.0389       0.0861        0.179        0.214
      1    60         0.29        0.258       0.0327        0.111         0.17        0.301         0.33
      1    70        0.271        0.183        0.088       0.0893        0.143        0.483        0.795
      1    80       0.0026      0.00026      0.00234      0.00421       0.0054       0.0816       0.0872
      1    90      0.00965     0.000111      0.00954      0.00278      0.00352        0.197        0.208
      1   100        0.172        0.151       0.0203        0.101         0.13        0.315        0.382
      1   110        0.119       0.0917        0.027       0.0589        0.101        0.253        0.263
      1   120        0.199         0.13       0.0696       0.0693        0.121        0.607        0.674
      1   130        0.113       0.0715       0.0418        0.058       0.0895        0.365        0.428
      1   140        0.762        0.641        0.121        0.204        0.268        0.506        0.569
      1   150        0.388        0.212        0.176       0.0933        0.154        0.529        0.739
      1   160       0.0344       0.0138       0.0205        0.025       0.0394        0.245        0.292
      1   170       0.0816       0.0697       0.0119       0.0676       0.0884        0.222        0.269
      1   180        0.209        0.193       0.0165       0.0819        0.147        0.153        0.172
      1   190       0.0612       0.0439       0.0173        0.049       0.0702        0.272         0.32
      1   200        0.121       0.0941       0.0271       0.0606        0.103        0.413        0.441
      1   210       0.0691       0.0492       0.0199       0.0459       0.0743        0.224        0.271
      1   220       0.0767       0.0601       0.0166       0.0392       0.0821        0.199        0.266
      1   230        0.851        0.115        0.736       0.0763        0.114         1.73         1.84
      1   231        0.849      0.00168        0.847       0.0109       0.0137         1.23         1.23

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      1     7        0.832        0.781       0.0514       0.0844        0.296        0.427        0.484


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               1   65.771    0.005        0.284        0.835         1.12       0.0909        0.178        0.481         1.32
! Validation          1   65.771    0.005        0.168         6.09         6.26       0.0522        0.115        0.852         3.46
Wall time: 65.77226011548191
! Best model        1    6.256

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      2    10        0.331        0.311       0.0195        0.119        0.187        0.259        0.294
      2    20         0.48         0.12         0.36        0.092        0.116        0.703        0.867
      2    30         0.48       0.0459        0.435       0.0421       0.0717          1.1         1.25
      2    40        0.128     0.000423        0.128      0.00538      0.00689          0.6        0.637
      2    50        0.105        0.083       0.0215       0.0484       0.0965        0.295        0.393
      2    60        0.201        0.074        0.127       0.0398       0.0911        0.395        0.478
      2    70       0.0169      0.00883      0.00804       0.0184       0.0315        0.181         0.24
      2    80       0.0322       0.0174       0.0148       0.0299       0.0442        0.152        0.163
      2    90       0.0823       0.0816     0.000682       0.0636       0.0957       0.0534         0.07
      2   100       0.0121      0.00569      0.00642       0.0158       0.0252        0.173        0.215
      2   110        0.497        0.126        0.371       0.0707        0.119        0.725        0.816
      2   120        0.124       0.0452       0.0787       0.0415       0.0712        0.546        0.688
      2   130       0.0229        0.015      0.00784       0.0242        0.041        0.226        0.237
      2   140       0.0515       0.0332       0.0183       0.0394        0.061         0.25        0.292
      2   150        0.145         0.13       0.0152        0.087        0.121        0.239        0.245
      2   160      0.00826      0.00327      0.00498       0.0135       0.0192         0.12        0.181
      2   170        0.265       0.0575        0.207       0.0396       0.0803        0.364         0.61
      2   180        0.145        0.129       0.0157       0.0763         0.12        0.233        0.283
      2   190        0.147       0.0938       0.0536       0.0591        0.103        0.311        0.339
      2   200       0.0666       0.0538       0.0128       0.0451       0.0776         0.19        0.244
      2   210       0.0562        0.036       0.0202       0.0342       0.0635        0.293        0.381
      2   220         1.28         0.04         1.24       0.0248       0.0669        0.885         1.49
      2   230        0.081       0.0705       0.0105       0.0653       0.0889        0.163          0.2
      2   231        0.156        0.144       0.0126        0.105        0.127         0.15         0.15

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      2     7       0.0944       0.0746       0.0198       0.0356       0.0915        0.236        0.289


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               2  117.335    0.005        0.119        0.814        0.934       0.0612        0.115        0.428         1.28
! Validation          2  117.335    0.005       0.0499         6.13         6.18       0.0404       0.0733        0.713         3.47
Wall time: 117.33638869412243
! Best model        2    6.180

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      3    10       0.0307       0.0263      0.00438       0.0252       0.0543        0.131        0.176
      3    20        0.254        0.248      0.00577        0.107        0.167        0.134        0.148
      3    30        0.108       0.0703       0.0377       0.0539       0.0888        0.236        0.271
      3    40        0.381       0.0473        0.334       0.0519       0.0728         0.98         1.06
      3    50        0.187        0.177      0.00999       0.0793        0.141         0.22        0.268
      3    60        0.113       0.0705       0.0421       0.0492       0.0889         0.47        0.549
      3    70       0.0629     0.000153       0.0628      0.00286      0.00414        0.497        0.521
      3    80        0.102       0.0553       0.0464       0.0443       0.0788        0.534        0.577
      3    90        0.061       0.0325       0.0285       0.0386       0.0604        0.258        0.272
      3   100       0.0724       0.0549       0.0175       0.0512       0.0784        0.249        0.353
      3   110       0.0498       0.0376       0.0122       0.0394       0.0649        0.252         0.26
      3   120       0.0443       0.0277       0.0167        0.033       0.0557        0.172        0.189
      3   130        0.147       0.0889       0.0579        0.071       0.0998        0.431        0.611
      3   140       0.0584       0.0445       0.0139       0.0489       0.0706        0.158        0.165
      3   150       0.0766       0.0703      0.00628       0.0424       0.0888        0.143        0.179
      3   160       0.0179        0.011      0.00683       0.0244       0.0352        0.137        0.145
      3   170        0.034       0.0298      0.00421       0.0436       0.0578        0.149        0.173
      3   180      0.00923      0.00512      0.00411       0.0108        0.024        0.111        0.111
      3   190       0.0431       0.0376      0.00544       0.0297       0.0649        0.134        0.152
      3   200         0.47       0.0287        0.441        0.041       0.0567         1.06         1.32
      3   210        0.256        0.237        0.019          0.1        0.163        0.233        0.341
      3   220        0.386        0.353       0.0329        0.114        0.199         0.33         0.38
      3   230        0.128       0.0908       0.0374       0.0676        0.101        0.326        0.438
      3   231        0.651        0.546        0.105        0.182        0.247        0.434        0.434

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      3     7       0.0986       0.0407       0.0579       0.0265       0.0675        0.465        0.504


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               3  168.838    0.005       0.0781        0.803        0.881        0.052       0.0929        0.369         1.26
! Validation          3  168.838    0.005       0.0394         6.09         6.13       0.0369       0.0664        0.858         3.46
Wall time: 168.83964519109577
! Best model        3    6.126

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      4    10        0.609       0.0101        0.599       0.0198       0.0337         2.07         2.07
      4    20        0.285        0.249       0.0357         0.11        0.167        0.206        0.259
      4    30        0.372        0.174        0.198        0.114         0.14        0.814        0.953
      4    40       0.0373       0.0285      0.00875        0.025       0.0565        0.106        0.125
      4    50         0.13       0.0881       0.0421       0.0736       0.0994        0.406        0.504
      4    60       0.0522       0.0324       0.0198       0.0338       0.0603         0.16        0.189
      4    70        0.129        0.112        0.017       0.0678        0.112        0.189        0.225
      4    80        0.104        0.103      0.00147       0.0731        0.107       0.0503       0.0617
      4    90       0.0337       0.0292      0.00443       0.0383       0.0573       0.0901        0.103
      4   100        0.182      0.00678        0.175       0.0172       0.0276        0.814        0.858
      4   110        0.276        0.145         0.13       0.0811        0.128        0.423        0.499
      4   120       0.0558       0.0406       0.0152       0.0372       0.0675        0.252        0.315
      4   130        0.289        0.117        0.171       0.0736        0.115        0.494        0.613
      4   140       0.0451       0.0252       0.0199       0.0251       0.0531        0.219        0.251
      4   150       0.0548       0.0492      0.00561       0.0469       0.0743       0.0982        0.118
      4   160        0.076       0.0585       0.0175       0.0592        0.081         0.13        0.184
      4   170        0.049        0.038        0.011       0.0385       0.0653        0.138        0.151
      4   180       0.0547        0.045      0.00973        0.043        0.071        0.178        0.179
      4   190        0.454        0.076        0.378       0.0701       0.0923         0.87         1.64
      4   200        0.013       0.0058      0.00718       0.0116       0.0255        0.175        0.199
      4   210       0.0559       0.0532      0.00272       0.0465       0.0772       0.0951       0.0964
      4   220       0.0849       0.0781      0.00676       0.0543       0.0936        0.121        0.136
      4   230       0.0304       0.0281      0.00232       0.0381       0.0561       0.0698       0.0777
      4   231        0.566        0.345        0.222        0.155        0.197         1.26         1.26

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      4     7       0.0804       0.0603       0.0201       0.0316       0.0822        0.226        0.262


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               4  220.337    0.005       0.0774        0.789        0.866        0.051       0.0929        0.362         1.25
! Validation          4  220.337    0.005       0.0374         6.14         6.18       0.0353       0.0632         0.68         3.47
Wall time: 220.33816954866052

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      5    10        0.154        0.131       0.0236       0.0695        0.121        0.176        0.218
      5    20        0.138       0.0983       0.0392        0.083        0.105          0.4        0.529
      5    30       0.0435       0.0138       0.0296       0.0314       0.0394        0.227        0.231
      5    40         0.21       0.0272        0.183       0.0366       0.0552         0.65        0.858
      5    50       0.0648       0.0499       0.0149       0.0518       0.0748         0.21        0.218
      5    60       0.0482       0.0401      0.00814       0.0392        0.067        0.212        0.242
      5    70        0.101       0.0537       0.0472       0.0503       0.0776        0.424        0.566
      5    80         0.26       0.0888        0.171       0.0682       0.0998        0.683        0.712
      5    90        0.132       0.0572       0.0751        0.058       0.0801         0.46        0.522
      5   100        0.112        0.099       0.0134       0.0604        0.105        0.252        0.311
      5   110       0.0268       0.0181       0.0087       0.0188       0.0451        0.167        0.183
      5   120       0.0579        0.045       0.0129       0.0459        0.071        0.197        0.222
      5   130       0.0319       0.0157       0.0162       0.0258       0.0419        0.221        0.253
      5   140       0.0241      0.00466       0.0194       0.0163       0.0228        0.205        0.205
      5   150       0.0395        0.035      0.00451       0.0459       0.0626        0.112        0.145
      5   160        0.017       0.0149      0.00208       0.0219       0.0408       0.0714       0.0722
      5   170        0.115        0.111      0.00359       0.0472        0.112       0.0739       0.0803
      5   180      0.00718      0.00687     0.000308       0.0188       0.0277       0.0366        0.047
      5   190       0.0255         0.02      0.00551       0.0356       0.0474        0.139        0.159
      5   200       0.0506       0.0322       0.0184       0.0325       0.0601        0.207         0.22
      5   210       0.0212       0.0199      0.00131       0.0263       0.0472       0.0638       0.0816
      5   220       0.0233       0.0207      0.00256       0.0367       0.0482        0.101        0.125
      5   230       0.0126       0.0102      0.00234       0.0167       0.0338       0.0972        0.116
      5   231       0.0026      0.00171     0.000893       0.0113       0.0138         0.04         0.04

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      5     7       0.0356       0.0195       0.0161       0.0214       0.0467        0.197        0.225


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               5  271.806    0.005       0.0524        0.783        0.835       0.0427       0.0764        0.331         1.24
! Validation          5  271.806    0.005       0.0277         6.15         6.18       0.0327       0.0564         0.65         3.47
Wall time: 271.8073764750734

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      6    10       0.0117      0.00781      0.00388       0.0186       0.0296       0.0766       0.0834
      6    20       0.0433       0.0404       0.0029       0.0513       0.0673       0.0689       0.0816
      6    30       0.0194       0.0179       0.0015       0.0331       0.0448       0.0506       0.0536
      6    40       0.0231       0.0219      0.00123       0.0326       0.0496        0.078       0.0886
      6    50       0.0166       0.0154      0.00126       0.0279       0.0415       0.0559       0.0674
      6    60       0.0328       0.0278      0.00499       0.0358       0.0558        0.123        0.151
      6    70      0.00779       0.0073     0.000493       0.0146       0.0286       0.0519       0.0571
      6    80        0.044       0.0411      0.00292       0.0471       0.0679        0.124        0.144
      6    90      0.00777      0.00496      0.00281        0.012       0.0236         0.11        0.142
      6   100       0.0282       0.0269      0.00121       0.0316        0.055       0.0802        0.092
      6   110       0.0487       0.0466      0.00206        0.042       0.0723       0.0865         0.11
      6   120       0.0503       0.0474      0.00292       0.0405       0.0729       0.0724       0.0763
      6   130       0.0763       0.0736      0.00269       0.0671       0.0909        0.107        0.139
      6   140       0.0223       0.0203      0.00195       0.0277       0.0477       0.0823        0.114
      6   150      0.00108     0.000162     0.000918      0.00307      0.00426       0.0548       0.0631
      6   160      0.00407      0.00374     0.000327       0.0124       0.0205       0.0374       0.0437
      6   170        0.282       0.0173        0.265       0.0256        0.044        0.801        0.807
      6   180       0.0696       0.0283       0.0413       0.0352       0.0564        0.271        0.355
      6   190        0.195       0.0627        0.132       0.0711       0.0838        0.538        0.541
      6   200        0.214        0.038        0.176        0.038       0.0652        0.541        0.641
      6   210       0.0981       0.0468       0.0512       0.0492       0.0725        0.327        0.383
      6   220       0.0466       0.0113       0.0354       0.0178       0.0355        0.275        0.318
      6   230        0.304        0.201        0.102       0.0876         0.15         0.84        0.858
      6   231        0.351     0.000103         0.35      0.00269      0.00339        0.793        0.793

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      6     7        0.243        0.205        0.038       0.0453        0.151        0.321        0.347


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               6  323.346    0.005       0.0476        0.785        0.832       0.0402       0.0734        0.282         1.24
! Validation          6  323.346    0.005       0.0545         6.13         6.18       0.0343       0.0688        0.725         3.47
Wall time: 323.347473802045

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      7    10        0.136      0.00236        0.134      0.00918       0.0163        0.458         0.49
      7    20        0.453        0.058        0.395       0.0503       0.0806        0.633        0.926
      7    30       0.0422       0.0395      0.00267       0.0517       0.0666        0.125        0.138
      7    40       0.0819       0.0599        0.022       0.0537        0.082        0.268        0.307
      7    50        0.383        0.199        0.185       0.0988        0.149        0.836        0.877
      7    60       0.0708       0.0364       0.0344       0.0366       0.0639        0.259        0.301
      7    70       0.0485        0.028       0.0205        0.034       0.0561        0.255        0.354
      7    80       0.0489       0.0276       0.0213       0.0367       0.0556          0.3        0.391
      7    90        0.073       0.0577       0.0153       0.0448       0.0804        0.237        0.242
      7   100        0.045        0.042      0.00301       0.0544       0.0686       0.0986         0.14
      7   110         0.03       0.0253      0.00463       0.0318       0.0533       0.0745       0.0917
      7   120       0.0834       0.0794      0.00402       0.0714       0.0943       0.0922        0.109
      7   130       0.0194       0.0135      0.00589       0.0214       0.0389       0.0886        0.103
      7   140       0.0519       0.0468      0.00504       0.0535       0.0725        0.112        0.144
      7   150       0.0529        0.038       0.0149       0.0366       0.0653        0.207          0.3
      7   160       0.0177       0.0159       0.0018       0.0226       0.0422       0.0698       0.0883
      7   170        0.348       0.0152        0.333       0.0214       0.0413         0.77        0.773
      7   180       0.0393       0.0318      0.00745       0.0383       0.0597         0.16        0.203
      7   190        0.153       0.0492        0.104       0.0513       0.0743        0.535        0.794
      7   200       0.0379       0.0194       0.0186       0.0262       0.0466        0.273        0.294
      7   210         1.22     0.000428         1.22      0.00435      0.00693        0.965          1.5
      7   220       0.0386        0.031      0.00759       0.0397       0.0589        0.111        0.123
      7   230       0.0413       0.0376      0.00379       0.0444       0.0649        0.127        0.164
      7   231       0.0435       0.0411      0.00242       0.0537       0.0678       0.0659       0.0659

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      7     7        0.426        0.385       0.0405       0.0559        0.208        0.352        0.373


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               7  374.909    0.005       0.0634          0.8        0.864        0.046        0.084        0.382         1.25
! Validation          7  374.909    0.005       0.0779         6.12          6.2       0.0341       0.0767        0.744         3.46
Wall time: 374.9103375123814

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      8    10        0.174       0.0391        0.135       0.0481       0.0662        0.374        0.518
      8    20        0.246       0.0619        0.184       0.0587       0.0833         0.95         1.02
      8    30       0.0986       0.0568       0.0418       0.0538       0.0798        0.509        0.548
      8    40        0.213        0.201        0.012        0.105         0.15        0.135        0.155
      8    50       0.0574        0.048      0.00937       0.0519       0.0734        0.217        0.226
      8    60       0.0259       0.0157       0.0102       0.0244        0.042        0.239        0.268
      8    70        0.059       0.0515      0.00752       0.0593        0.076        0.155        0.232
      8    80       0.0305       0.0258      0.00474        0.034       0.0538        0.113        0.117
      8    90       0.0382       0.0339      0.00436       0.0389       0.0616        0.113        0.122
      8   100       0.0365       0.0334      0.00314       0.0328       0.0612       0.0782       0.0873
      8   110       0.0259       0.0241      0.00185       0.0388       0.0519       0.0826        0.111
      8   120       0.0374       0.0359      0.00146       0.0426       0.0635        0.065       0.0785
      8   130       0.0157       0.0134      0.00232       0.0296       0.0388        0.091         0.12
      8   140       0.0422       0.0329       0.0093        0.032       0.0608        0.187        0.244
      8   150      0.00836      0.00326      0.00509        0.011       0.0191       0.0947        0.106
      8   160        0.398       0.0649        0.333       0.0511       0.0853         1.37         1.45
      8   170        0.127       0.0129        0.114       0.0198       0.0381        0.809        0.861
      8   180       0.0685       0.0413       0.0272       0.0433       0.0681         0.23         0.26
      8   190        0.272        0.245        0.027        0.112        0.166        0.349        0.439
      8   200       0.0497       0.0189       0.0308       0.0294        0.046        0.328        0.337
      8   210        0.027      0.00476       0.0222      0.00996       0.0231        0.241        0.254
      8   220       0.0661       0.0463       0.0198       0.0502        0.072        0.211        0.247
      8   230       0.0206       0.0181      0.00252        0.029        0.045       0.0659       0.0673
      8   231       0.0437       0.0433     0.000314       0.0542       0.0697       0.0475       0.0475

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      8     7        0.203        0.143       0.0599       0.0394        0.127        0.486        0.513


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               8  426.489    0.005       0.0809        0.788        0.868       0.0462       0.0934        0.327         1.24
! Validation          8  426.489    0.005       0.0501         6.09         6.14       0.0362       0.0691        0.855         3.46
Wall time: 426.48989196959883

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      9    10       0.0647       0.0205       0.0442       0.0344       0.0479        0.243        0.282
      9    20        0.112       0.0508       0.0615       0.0538       0.0755        0.412        0.437
      9    30        0.134       0.0199        0.114       0.0286       0.0472        0.641        0.666
      9    40       0.0771       0.0255       0.0516       0.0345       0.0535        0.512        0.608
      9    50       0.0866       0.0645       0.0221       0.0628        0.085        0.323        0.383
      9    60        0.051       0.0445      0.00654       0.0491       0.0706        0.196        0.217
      9    70        0.221      0.00293        0.218       0.0116       0.0181        0.718         1.23
      9    80       0.0434       0.0315       0.0118       0.0335       0.0595        0.197        0.209
      9    90       0.0252      0.00754       0.0177       0.0168       0.0291        0.272        0.322
      9   100        0.028       0.0262      0.00183       0.0431       0.0542       0.0656        0.075
      9   110       0.0101      0.00588      0.00424        0.017       0.0257        0.127        0.152
      9   120       0.0546       0.0364       0.0182       0.0395       0.0639        0.138        0.187
      9   130        0.175       0.0995       0.0755       0.0743        0.106        0.251        0.373
      9   140        0.011      0.00904      0.00193       0.0187       0.0318       0.0746       0.0894
      9   150       0.0466       0.0421      0.00447       0.0518       0.0687       0.0862       0.0968
      9   160       0.0599       0.0513      0.00857       0.0497       0.0758        0.158        0.163
      9   170       0.0247       0.0163      0.00844       0.0293       0.0427        0.105        0.131
      9   180       0.0075       0.0069     0.000597       0.0183       0.0278       0.0458       0.0528
      9   190       0.0142      0.00887      0.00529       0.0158       0.0315        0.156        0.181
      9   200       0.0112      0.00959      0.00164        0.018       0.0328       0.0846        0.101
      9   210       0.0395       0.0386     0.000924        0.051       0.0658       0.0647       0.0664
      9   220       0.0256       0.0236      0.00199       0.0337       0.0515        0.058       0.0598
      9   230      0.00837      0.00782     0.000551       0.0149       0.0296       0.0425       0.0629
      9   231       0.0804        0.034       0.0464       0.0532       0.0617        0.289        0.289

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      9     7       0.0345        0.018       0.0165       0.0178       0.0449        0.207        0.228


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               9  478.099    0.005       0.0369        0.766        0.803       0.0367       0.0651        0.292         1.21
! Validation          9  478.099    0.005       0.0219         6.15         6.17       0.0287         0.05        0.635         3.47
Wall time: 478.1003071181476

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     10    10       0.0127       0.0102      0.00247       0.0236       0.0338       0.0798        0.115
     10    20       0.0118       0.0112     0.000521       0.0217       0.0355       0.0433       0.0612
     10    30      0.00609      0.00519     0.000893       0.0142       0.0241       0.0491        0.052
     10    40      0.00392      0.00143      0.00248      0.00773       0.0127       0.0857       0.0973
     10    50       0.0435       0.0374      0.00609       0.0436       0.0648        0.123        0.139
     10    60        0.293       0.0066        0.286       0.0182       0.0272         1.23         1.26
     10    70        0.133       0.0182        0.115       0.0255       0.0451        0.592        0.682
     10    80        0.135        0.125       0.0096       0.0883        0.118        0.142        0.261
     10    90       0.0469       0.0242       0.0227       0.0346       0.0521        0.255        0.374
     10   100       0.0733       0.0275       0.0458       0.0245       0.0555        0.437        0.471
     10   110        0.159        0.141       0.0179       0.0951        0.126        0.117         0.18
     10   120       0.0383        0.034      0.00432       0.0499       0.0617         0.15        0.166
     10   130        0.122       0.0101        0.112        0.021       0.0336         0.77        0.797
     10   140        0.312       0.0781        0.234       0.0528       0.0936        0.713        0.773
     10   150        0.102       0.0961      0.00627       0.0748        0.104         0.15        0.181
     10   160        0.117       0.0984       0.0191       0.0457        0.105        0.175        0.227
     10   170       0.0912       0.0874      0.00385       0.0636        0.099        0.122         0.15
     10   180        0.043       0.0394      0.00359        0.039       0.0664        0.144        0.155
     10   190        0.115       0.0946       0.0209       0.0711        0.103        0.242        0.248
     10   200       0.0331       0.0179       0.0153       0.0166       0.0448        0.237        0.262
     10   210        0.236        0.233      0.00267       0.0949        0.162       0.0858       0.0881
     10   220       0.0643       0.0351       0.0292       0.0473       0.0628        0.169        0.229
     10   230       0.0251       0.0223      0.00277       0.0289         0.05       0.0722       0.0983
     10   231      0.00337     1.59e-05      0.00335      0.00101      0.00133       0.0776       0.0776

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     10     7       0.0503       0.0261       0.0242        0.021       0.0541        0.268        0.288


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              10  529.615    0.005       0.0617        0.784        0.846       0.0413       0.0817        0.312         1.23
! Validation         10  529.615    0.005       0.0233         6.13         6.15       0.0288        0.051        0.678         3.46
Wall time: 529.6161579126492

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     11    10       0.0161        0.015      0.00113       0.0276        0.041       0.0768       0.0805
     11    20       0.0323       0.0258      0.00656       0.0305       0.0538        0.144        0.168
     11    30        0.387       0.0182        0.369       0.0283       0.0452        0.811        0.814
     11    40       0.0929       0.0627       0.0302       0.0496       0.0839        0.385        0.459
     11    50       0.0104      6.8e-05       0.0103      0.00188      0.00276        0.136        0.136
     11    60       0.0164       0.0149      0.00144       0.0195       0.0409       0.0401       0.0508
     11    70       0.0273       0.0165       0.0108       0.0321        0.043        0.144        0.168
     11    80        0.153       0.0277        0.125       0.0291       0.0558        0.388         0.51
     11    90       0.0609       0.0557      0.00521       0.0567        0.079        0.162        0.193
     11   100      0.00903      0.00783       0.0012       0.0167       0.0296       0.0399       0.0477
     11   110       0.0675       0.0496       0.0179       0.0538       0.0746        0.223        0.356
     11   120       0.0564        0.037       0.0194       0.0382       0.0644        0.266         0.36
     11   130         0.25      0.00537        0.244       0.0173       0.0245         1.15         1.18
     11   140       0.0942       0.0795       0.0147       0.0675       0.0944        0.225        0.264
     11   150       0.0439       0.0332       0.0107       0.0291        0.061        0.122        0.145
     11   160       0.0215       0.0159      0.00558       0.0221       0.0423       0.0884        0.104
     11   170       0.0454       0.0362      0.00915       0.0257       0.0637        0.141        0.164
     11   180        0.117       0.0581       0.0586        0.051       0.0807        0.303        0.366
     11   190        0.018       0.0126      0.00539       0.0237       0.0376        0.105        0.116
     11   200       0.0127      0.00866      0.00409        0.021       0.0312        0.122        0.171
     11   210       0.0339       0.0295      0.00446       0.0364       0.0575        0.109        0.134
     11   220       0.0409       0.0371      0.00372       0.0424       0.0645       0.0884       0.0964
     11   230       0.0229        0.012        0.011         0.02       0.0366        0.175        0.194
     11   231      0.00632     4.73e-05      0.00627      0.00218       0.0023        0.106        0.106

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     11     7       0.0545       0.0352       0.0193        0.022       0.0628        0.236        0.256


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              11  581.074    0.005       0.0349        0.781        0.816       0.0349        0.063        0.301         1.22
! Validation         11  581.074    0.005       0.0221         6.14         6.16       0.0274       0.0486        0.658         3.46
Wall time: 581.0752220135182

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     12    10       0.0203       0.0188      0.00145       0.0319        0.046       0.0482       0.0552
     12    20       0.0161       0.0143      0.00177       0.0257         0.04       0.0543       0.0585
     12    30       0.0238       0.0201      0.00369        0.036       0.0475        0.103         0.12
     12    40       0.0309        0.029      0.00196       0.0404        0.057       0.0884        0.118
     12    50         0.12       0.0106        0.109       0.0232       0.0345        0.657        0.693
     12    60        0.147       0.0096        0.138       0.0228       0.0328         0.64        0.656
     12    70        0.165      0.00429        0.161        0.016       0.0219        0.899         0.92
     12    80       0.0487        0.015       0.0337       0.0191        0.041        0.308        0.334
     12    90       0.0219      0.00687       0.0151       0.0119       0.0277        0.199         0.24
     12   100       0.0313       0.0167       0.0146       0.0241       0.0433        0.225        0.231
     12   110       0.0342       0.0186       0.0157       0.0232       0.0456         0.26        0.295
     12   120       0.0402       0.0213       0.0189       0.0309       0.0488        0.162        0.184
     12   130       0.0168       0.0141      0.00266       0.0344       0.0398       0.0644       0.0705
     12   140       0.0329       0.0317      0.00127       0.0413       0.0596       0.0637       0.0904
     12   150       0.0397       0.0302      0.00955       0.0322       0.0582         0.17         0.19
     12   160       0.0614       0.0512       0.0102       0.0484       0.0758        0.133        0.138
     12   170       0.0564       0.0247       0.0317       0.0297       0.0526        0.177        0.249
     12   180       0.0151       0.0138      0.00122       0.0163       0.0394       0.0542        0.066
     12   190       0.0051      0.00315      0.00195      0.00872       0.0188       0.0532       0.0603
     12   200       0.0785       0.0552       0.0233       0.0606       0.0787         0.23        0.409
     12   210       0.0141      0.00701      0.00712       0.0171        0.028        0.144        0.174
     12   220      0.00993      0.00934     0.000585       0.0161       0.0324       0.0283       0.0365
     12   230      0.00421      0.00213      0.00208      0.00933       0.0154       0.0679       0.0709
     12   231       0.0233       0.0232     0.000117       0.0423        0.051       0.0145       0.0145

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     12     7       0.0213      0.00751       0.0138       0.0135        0.029        0.194        0.214


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              12  632.667    0.005       0.0329        0.763        0.796       0.0322       0.0588        0.272          1.2
! Validation         12  632.667    0.005       0.0175         6.15         6.16       0.0263       0.0454        0.627         3.47
Wall time: 632.6679770676419

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     13    10        0.013       0.0118      0.00119        0.024       0.0364       0.0538       0.0556
     13    20       0.0118       0.0112     0.000583       0.0258       0.0355       0.0481       0.0631
     13    30       0.0202       0.0115      0.00866       0.0206        0.036       0.0841        0.125
     13    40        0.087       0.0847      0.00231       0.0697       0.0974       0.0803        0.118
     13    50       0.0355       0.0349     0.000605       0.0494       0.0626       0.0387       0.0461
     13    60       0.0137      0.00955      0.00417       0.0223       0.0327        0.131        0.149
     13    70       0.0258       0.0246      0.00123       0.0366       0.0525       0.0415        0.047
     13    80      0.00388      0.00258       0.0013       0.0126        0.017       0.0793       0.0929
     13    90        0.316       0.0101        0.305       0.0165       0.0337        0.397         0.74
     13   100       0.0179       0.0171     0.000807       0.0227       0.0438       0.0389       0.0444
     13   110       0.0212        0.021     0.000141       0.0285       0.0485       0.0266       0.0317
     13   120       0.0358      0.00754       0.0283       0.0156       0.0291        0.388        0.405
     13   130       0.0309       0.0283      0.00255       0.0272       0.0563       0.0634       0.0757
     13   140      0.00974      0.00942     0.000327       0.0215       0.0325       0.0317       0.0393
     13   150       0.0121      0.00959      0.00254       0.0191       0.0328       0.0864        0.126
     13   160       0.0535       0.0196       0.0338        0.026       0.0469        0.166         0.25
     13   170       0.0159       0.0148      0.00105       0.0212       0.0408       0.0661       0.0792
     13   180        0.317       0.0489        0.268       0.0518       0.0741         1.02         1.13
     13   190          0.3       0.0624        0.238       0.0635       0.0836        0.562        0.695
     13   200       0.0239       0.0227      0.00119       0.0333       0.0504        0.056       0.0697
     13   210       0.0337       0.0269      0.00675       0.0376        0.055        0.177        0.199
     13   220       0.0262       0.0151       0.0111       0.0282       0.0411        0.182        0.189
     13   230        0.267     0.000888        0.266      0.00728      0.00998         1.02         1.08
     13   231         0.31        0.182        0.128        0.123        0.143        0.479        0.479

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     13     7        0.107        0.058       0.0487       0.0274       0.0806        0.444        0.474


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              13  684.232    0.005       0.0285        0.776        0.805       0.0292       0.0549        0.257         1.22
! Validation         13  684.232    0.005       0.0234         6.09         6.11        0.026        0.048        0.841         3.46
Wall time: 684.2331611458212
! Best model       13    6.111

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     14    10       0.0585       0.0121       0.0464       0.0208       0.0368        0.439        0.499
     14    20       0.0962       0.0469       0.0492       0.0373       0.0725        0.222        0.302
     14    30       0.0685       0.0449       0.0236       0.0404       0.0709        0.336        0.412
     14    40        0.132        0.102       0.0297       0.0558        0.107         0.25        0.295
     14    50       0.0424       0.0292       0.0132       0.0342       0.0572        0.235        0.252
     14    60       0.0403       0.0181       0.0222       0.0341        0.045        0.223        0.257
     14    70        0.154       0.0445        0.109       0.0403       0.0707        0.831        0.885
     14    80        0.166       0.0166        0.149       0.0292       0.0431        0.346        0.528
     14    90        0.017      0.00862      0.00839       0.0166       0.0311        0.161        0.184
     14   100       0.0239       0.0165      0.00741       0.0269        0.043        0.132        0.155
     14   110      0.00381      0.00183      0.00197      0.00985       0.0143       0.0497       0.0608
     14   120       0.0224         0.02      0.00248       0.0308       0.0473       0.0481       0.0668
     14   130       0.0272       0.0252      0.00196       0.0364       0.0532       0.0741        0.102
     14   140        0.153      0.00957        0.143       0.0177       0.0327        0.645        0.701
     14   150        0.129      0.00126        0.128      0.00804       0.0119        0.565        0.574
     14   160         1.37         1.28       0.0864        0.194        0.379        0.232        0.395
     14   170       0.0177       0.0142      0.00351       0.0198       0.0399       0.0815       0.0834
     14   180       0.0157      0.00918      0.00648       0.0193       0.0321        0.149        0.187
     14   190       0.0148      0.00509      0.00971       0.0126       0.0239        0.204        0.233
     14   200       0.0202       0.0174      0.00277       0.0318       0.0441        0.117        0.127
     14   210        0.079       0.0632       0.0158       0.0557       0.0842        0.191        0.336
     14   220       0.0183       0.0119      0.00642       0.0222       0.0365        0.162        0.179
     14   230      0.00807      0.00257       0.0055       0.0116        0.017        0.126        0.132
     14   231       0.0247       0.0233      0.00138       0.0403       0.0511       0.0497       0.0497

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     14     7        0.133        0.102       0.0308        0.033        0.107        0.343        0.363


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              14  735.866    0.005       0.0364         0.78        0.816       0.0352       0.0634        0.316         1.22
! Validation         14  735.866    0.005       0.0309         6.11         6.14       0.0277       0.0529        0.742         3.46
Wall time: 735.8670202279463

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     15    10       0.0211       0.0193      0.00176       0.0351       0.0465       0.0722       0.0828
     15    20       0.0887       0.0308       0.0578        0.044       0.0588        0.202        0.325
     15    30        0.024       0.0205      0.00351       0.0392        0.048        0.103        0.106
     15    40        0.224        0.031        0.193       0.0414       0.0589        0.728        0.784
     15    50        0.108       0.0817       0.0261       0.0749       0.0957        0.235        0.243
     15    60       0.0242      0.00815       0.0161       0.0149       0.0302        0.213        0.259
     15    70       0.0145      0.00708      0.00743       0.0168       0.0282        0.147        0.182
     15    80        0.023       0.0194      0.00367       0.0294       0.0466       0.0734       0.0876
     15    90       0.0209       0.0131      0.00788       0.0222       0.0383        0.176        0.198
     15   100        0.102       0.0113       0.0906       0.0245       0.0356         0.45        0.459
     15   110        0.163       0.0777        0.085       0.0573       0.0934        0.466        0.518
     15   120       0.0624       0.0444        0.018       0.0464       0.0706         0.21        0.239
     15   130       0.0572       0.0495      0.00775       0.0399       0.0745        0.182        0.219
     15   140          0.1        0.055        0.045       0.0505       0.0785         0.39        0.455
     15   150       0.0665       0.0569      0.00962       0.0531       0.0799        0.162        0.201
     15   160       0.0171       0.0145      0.00268        0.023       0.0403        0.126        0.139
     15   170       0.0162       0.0135      0.00264       0.0275       0.0389         0.12        0.138
     15   180       0.0289       0.0273      0.00158       0.0364       0.0553       0.0903        0.107
     15   190       0.0312         0.03      0.00116       0.0425        0.058       0.0655       0.0825
     15   200       0.0168       0.0142      0.00263       0.0225       0.0399       0.0891       0.0893
     15   210       0.0241       0.0197      0.00433       0.0238        0.047        0.127        0.148
     15   220       0.0122      0.00556      0.00665       0.0166        0.025        0.105        0.119
     15   230       0.0518       0.0393       0.0125       0.0458       0.0664        0.183         0.19
     15   231       0.0553       0.0451       0.0102       0.0533       0.0711        0.271        0.271

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     15     7       0.0673       0.0487       0.0186       0.0253       0.0739         0.24        0.259


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              15  787.493    0.005       0.0398        0.776        0.815       0.0342       0.0653        0.287         1.21
! Validation         15  787.493    0.005       0.0232         6.13         6.16       0.0274       0.0487        0.658         3.46
Wall time: 787.493922624737

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     16    10       0.0542       0.0516      0.00255       0.0526       0.0761       0.0658       0.0734
     16    20       0.0523       0.0106       0.0417        0.022       0.0345        0.542        0.547
     16    30        0.146       0.0185        0.128       0.0295       0.0456        0.472        0.478
     16    40       0.0303      0.00127       0.0291      0.00957       0.0119        0.228        0.228
     16    50       0.0749       0.0481       0.0267       0.0436       0.0734        0.309        0.415
     16    60        0.105       0.0752       0.0293       0.0494       0.0918        0.326        0.458
     16    70       0.0196       0.0161      0.00352        0.023       0.0425       0.0958        0.108
     16    80      0.00903      0.00607      0.00296       0.0117       0.0261        0.126        0.146
     16    90        0.181        0.162       0.0195       0.0563        0.135        0.121        0.187
     16   100        0.054        0.051      0.00299        0.043       0.0756       0.0565       0.0732
     16   110       0.0068       0.0066     0.000201       0.0162       0.0272       0.0283       0.0345
     16   120       0.0248       0.0235      0.00133       0.0305       0.0513       0.0683        0.073
     16   130        0.213       0.0458        0.167       0.0483       0.0717         1.09         1.09
     16   140        0.235        0.151       0.0845        0.073         0.13        0.508        0.514
     16   150       0.0362       0.0343      0.00188       0.0316        0.062       0.0952        0.114
     16   160       0.0529       0.0237       0.0292       0.0319       0.0516        0.255        0.291
     16   170       0.0348       0.0234       0.0113       0.0364       0.0513        0.136        0.147
     16   180       0.0399       0.0349      0.00499       0.0371       0.0626        0.113        0.125
     16   190       0.0386       0.0348      0.00376       0.0348       0.0625       0.0975        0.104
     16   200       0.0126      0.00992      0.00266       0.0241       0.0333        0.067       0.0738
     16   210       0.0686       0.0407       0.0279       0.0442       0.0676        0.223        0.253
     16   220       0.0206      0.00491       0.0157       0.0111       0.0235        0.291        0.306
     16   230         1.31         1.19        0.123        0.149        0.365        0.353        0.469
     16   231      0.00896      0.00896     4.02e-06       0.0238       0.0317      0.00269      0.00269

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     16     7       0.0461       0.0224       0.0237       0.0192       0.0501         0.29        0.308


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              16  839.160    0.005       0.0395        0.776        0.815       0.0348       0.0655        0.284         1.21
! Validation         16  839.160    0.005        0.018         6.12         6.14       0.0253       0.0445        0.705         3.46
Wall time: 839.1614351570606

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     17    10       0.0554       0.0376       0.0177       0.0507       0.0649        0.231        0.342
     17    20       0.0134      0.00628      0.00708       0.0152       0.0265        0.163        0.185
     17    30       0.0172       0.0136       0.0036       0.0245       0.0391       0.0791       0.0917
     17    40        0.261       0.0181        0.243       0.0277        0.045        0.648         0.66
     17    50       0.0572       0.0105       0.0467        0.024       0.0343        0.304         0.33
     17    60       0.0169       0.0135      0.00335       0.0252        0.039       0.0784       0.0837
     17    70      0.00476      0.00191      0.00285      0.00883       0.0146        0.113        0.132
     17    80       0.0928       0.0519       0.0409       0.0519       0.0763        0.229        0.307
     17    90       0.0541       0.0428       0.0113       0.0332       0.0693          0.2        0.235
     17   100       0.0794       0.0697      0.00973       0.0548       0.0884        0.174        0.223
     17   110       0.0591       0.0577      0.00144       0.0442       0.0804         0.07       0.0721
     17   120       0.0403       0.0379      0.00235       0.0458       0.0652       0.0929        0.104
     17   130       0.0445       0.0434      0.00112       0.0508       0.0698       0.0352       0.0456
     17   140        0.165       0.0206        0.145       0.0276        0.048        0.743        0.778
     17   150        0.179       0.0683         0.11       0.0467       0.0875         0.61        0.662
     17   160       0.0316       0.0102       0.0214       0.0202       0.0338        0.184        0.198
     17   170        0.165        0.155       0.0103       0.0881        0.132        0.233         0.27
     17   180       0.0428       0.0212       0.0217       0.0278       0.0487        0.178        0.215
     17   190         0.11       0.0544       0.0553       0.0422       0.0781        0.279        0.345
     17   200       0.0525       0.0435      0.00899       0.0484       0.0698         0.17        0.191
     17   210       0.0159       0.0102      0.00567       0.0224       0.0338        0.112        0.116
     17   220         1.12         1.02       0.0958        0.159        0.339         0.28         0.42
     17   230       0.0137       0.0131     0.000513       0.0263       0.0384       0.0469       0.0598
     17   231       0.0173       0.0172     0.000121        0.033       0.0439       0.0295       0.0295

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     17     7        0.239        0.213       0.0263       0.0415        0.154        0.288        0.308


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              17  890.849    0.005       0.0335         0.78        0.814       0.0334       0.0612        0.282         1.21
! Validation         17  890.849    0.005       0.0455         6.12         6.17       0.0271       0.0594        0.693         3.46
Wall time: 890.8504961710423

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     18    10        0.325       0.0431        0.282       0.0465       0.0695        0.446        0.716
     18    20       0.0336       0.0144       0.0193       0.0238       0.0402        0.245        0.372
     18    30      0.00785      0.00629      0.00155       0.0143       0.0266       0.0707       0.0976
     18    40       0.0112       0.0109     0.000265        0.019        0.035       0.0264       0.0289
     18    50       0.0144       0.0133      0.00105        0.025       0.0387       0.0668       0.0848
     18    60       0.0258       0.0237      0.00205       0.0253       0.0516       0.0619       0.0741
     18    70       0.0184       0.0175     0.000935       0.0357       0.0443       0.0581         0.08
     18    80        0.015       0.0131      0.00182       0.0266       0.0384       0.0723       0.0823
     18    90      0.00925      0.00883     0.000419       0.0224       0.0315       0.0407       0.0491
     18   100       0.0149       0.0142     0.000742       0.0278       0.0398       0.0361       0.0443
     18   110        0.012       0.0112     0.000846        0.018       0.0354       0.0592       0.0686
     18   120       0.0166       0.0156     0.000939       0.0298       0.0419        0.078       0.0821
     18   130       0.0149       0.0133       0.0016       0.0236       0.0386        0.061       0.0675
     18   140        0.146       0.0384        0.108       0.0488       0.0657        0.651        0.681
     18   150         0.11       0.0297       0.0803       0.0385       0.0577        0.598         0.66
     18   160       0.0191      0.00311        0.016       0.0104       0.0187        0.209        0.256
     18   170       0.0321       0.0226      0.00958       0.0307       0.0503        0.132        0.165
     18   180       0.0221         0.02      0.00212       0.0248       0.0474       0.0623        0.066
     18   190       0.0118       0.0108     0.000973       0.0188       0.0349       0.0728       0.0835
     18   200         2.24         2.18       0.0633        0.215        0.494        0.486        0.576
     18   210        0.111       0.0588       0.0525       0.0579       0.0812        0.366        0.409
     18   220       0.0641       0.0585      0.00566       0.0483        0.081        0.102        0.114
     18   230       0.0213      0.00019       0.0211      0.00329      0.00462        0.244        0.261
     18   231       0.0149      0.00679       0.0081        0.015       0.0276        0.241        0.241

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     18     7        0.063       0.0371       0.0258       0.0227       0.0645        0.312         0.33


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              18  942.453    0.005       0.0363        0.774         0.81       0.0314       0.0639        0.244         1.21
! Validation         18  942.453    0.005       0.0198         6.12         6.13       0.0253       0.0453         0.72         3.46
Wall time: 942.4541359758005

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     19    10       0.0161       0.0126      0.00348         0.02       0.0376        0.124        0.124
     19    20       0.0503       0.0461      0.00416       0.0408       0.0719       0.0931        0.112
     19    30       0.0141      0.00813      0.00599       0.0194       0.0302       0.0906        0.104
     19    40        0.175      0.00701        0.168       0.0191        0.028         1.09          1.1
     19    50       0.0815        0.038       0.0435       0.0328       0.0652        0.354        0.442
     19    60       0.0446       0.0343       0.0104       0.0445        0.062        0.208        0.223
     19    70        0.145       0.0521       0.0933       0.0434       0.0764        0.271        0.409
     19    80       0.0279       0.0222      0.00576       0.0265       0.0499        0.178        0.203
     19    90       0.0455       0.0407      0.00478       0.0383       0.0676        0.106        0.112
     19   100       0.0121       0.0092      0.00287       0.0229       0.0321       0.0905        0.109
     19   110       0.0206       0.0178      0.00283       0.0305       0.0447       0.0635       0.0713
     19   120        0.302        0.292      0.00988       0.0654        0.181       0.0965        0.134
     19   130        0.249       0.0838        0.165       0.0589       0.0969        0.538        0.544
     19   140        0.152       0.0976        0.054       0.0757        0.105        0.361        0.387
     19   150        0.041       0.0343      0.00677        0.033        0.062        0.119        0.131
     19   160        0.111       0.0905       0.0201        0.059        0.101        0.313         0.38
     19   170       0.0169       0.0118      0.00513       0.0204       0.0363       0.0702        0.096
     19   180       0.0552        0.047      0.00819       0.0511       0.0726        0.195        0.227
     19   190       0.0388       0.0275       0.0113       0.0391       0.0555         0.17        0.174
     19   200       0.0861       0.0314       0.0547       0.0304       0.0593        0.188        0.313
     19   210       0.0141      0.00869      0.00544        0.021       0.0312        0.149        0.158
     19   220       0.0423       0.0221       0.0202       0.0327       0.0497        0.197        0.216
     19   230       0.0405       0.0377      0.00274       0.0476       0.0651        0.117        0.122
     19   231     0.000483      0.00031     0.000173      0.00498       0.0059       0.0176       0.0176

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     19     7        0.205        0.181       0.0243        0.039        0.142        0.279        0.296


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              19  994.012    0.005       0.0416        0.772        0.814       0.0355       0.0662        0.289         1.21
! Validation         19  994.012    0.005       0.0395         6.13         6.16       0.0262       0.0557        0.686         3.46
Wall time: 994.0131975291297

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     20    10         1.35       0.0326         1.32       0.0208       0.0604        0.876         1.54
     20    20         0.18       0.0154        0.165       0.0304       0.0416        0.673        0.707
     20    30        0.116       0.0448       0.0712       0.0447       0.0708        0.477        0.507
     20    40        0.101       0.0506       0.0503       0.0534       0.0753        0.469        0.565
     20    50       0.0111      0.00694      0.00415       0.0177       0.0279        0.134        0.159
     20    60       0.0094      0.00476      0.00464        0.011       0.0231        0.109        0.118
     20    70       0.0353        0.034      0.00125       0.0411       0.0618        0.063       0.0721
     20    80       0.0123      0.00921      0.00314       0.0203       0.0321        0.053        0.075
     20    90       0.0164       0.0143      0.00202       0.0271       0.0401       0.0575       0.0727
     20   100       0.0136       0.0116      0.00204       0.0243        0.036        0.081        0.113
     20   110      0.00858      0.00653      0.00205       0.0138       0.0271       0.0662       0.0694
     20   120       0.0366       0.0306      0.00603        0.044       0.0586        0.149        0.186
     20   130       0.0106        0.006      0.00461       0.0154       0.0259        0.109        0.134
     20   140       0.0221       0.0197       0.0024       0.0312       0.0469       0.0687       0.0754
     20   150         0.02       0.0192     0.000874       0.0296       0.0463       0.0593       0.0683
     20   160       0.0574       0.0407       0.0167       0.0438       0.0675        0.132        0.181
     20   170       0.0341       0.0323      0.00179       0.0451       0.0602       0.0792       0.0911
     20   180       0.0173       0.0136      0.00374        0.022        0.039       0.0722       0.0821
     20   190       0.0438       0.0412       0.0027       0.0358       0.0679       0.0861        0.124
     20   200         0.15        0.124       0.0268       0.0858        0.118        0.274        0.389
     20   210        0.241       0.0192        0.222       0.0258       0.0464        0.944        0.996
     20   220         0.12       0.0597       0.0604       0.0464       0.0818        0.486         0.54
     20   230       0.0664       0.0597      0.00667       0.0531       0.0818         0.13        0.214
     20   231      0.00662     0.000278      0.00634      0.00476      0.00559        0.213        0.213

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     20     7       0.0608       0.0402       0.0206       0.0243       0.0672        0.278        0.297


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              20 1045.589    0.005       0.0346        0.772        0.806       0.0312       0.0613        0.266         1.21
! Validation         20 1045.589    0.005       0.0197         6.12         6.14       0.0253       0.0448          0.7         3.46
Wall time: 1045.5897820694372

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     21    10       0.0637       0.0262       0.0375       0.0364       0.0542        0.372        0.392
     21    20       0.0174      0.00852      0.00889       0.0169       0.0309        0.161        0.187
     21    30       0.0202      0.00887       0.0113       0.0193       0.0315         0.21        0.232
     21    40       0.0437       0.0421      0.00167         0.05       0.0687       0.0969        0.109
     21    50       0.0125      0.00949      0.00299       0.0168       0.0326        0.104        0.143
     21    60       0.0201      0.00788       0.0122       0.0179       0.0297        0.222        0.237
     21    70       0.0553        0.024       0.0314       0.0364       0.0519        0.295         0.44
     21    80      0.00267     0.000801      0.00187      0.00668      0.00948       0.0614       0.0721
     21    90         0.18      0.00858        0.171       0.0189        0.031        0.912        0.957
     21   100       0.0798        0.015       0.0647        0.019       0.0411         0.49        0.532
     21   110       0.0158      0.00567       0.0101       0.0141       0.0252        0.154        0.177
     21   120       0.0334       0.0189       0.0145       0.0349        0.046        0.214        0.233
     21   130         1.24       0.0174         1.22       0.0324       0.0441        0.778         1.48
     21   140       0.0191       0.0101      0.00895       0.0234       0.0337        0.147         0.15
     21   150       0.0691       0.0554       0.0137       0.0558       0.0788        0.209        0.295
     21   160      0.00937      0.00683      0.00253       0.0174       0.0277        0.094       0.0995
     21   170       0.0109       0.0097      0.00122       0.0238        0.033       0.0474       0.0592
     21   180         86.4      0.00325         86.4        0.013       0.0191         6.54         12.5
     21   190        0.186       0.0479        0.138       0.0471       0.0733        0.621        0.677
     21   200       0.0554       0.0481      0.00728       0.0568       0.0734        0.164        0.172
     21   210        0.023       0.0173      0.00569       0.0264       0.0441        0.161        0.176
     21   220        0.102       0.0911       0.0111        0.069        0.101         0.25        0.264
     21   230       0.0877       0.0405       0.0472       0.0434       0.0673        0.399        0.582
     21   231        0.118       0.0579       0.0602       0.0652       0.0805        0.657        0.657

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     21     7       0.0379       0.0141       0.0238       0.0152       0.0398        0.294        0.318


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              21 1097.204    0.005       0.0347        0.775         0.81       0.0332       0.0632        0.285         1.21
! Validation         21 1097.204    0.005       0.0162         6.12         6.13       0.0244       0.0428        0.707         3.46
Wall time: 1097.205665083602

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     22    10       0.0248       0.0208      0.00403       0.0331       0.0483        0.072       0.0891
     22    20       0.0101      0.00797      0.00212       0.0205       0.0299       0.0707       0.0826
     22    30       0.0113      0.00607      0.00519       0.0152       0.0261        0.109        0.111
     22    40       0.0259       0.0221      0.00383       0.0338       0.0498        0.122        0.142
     22    50       0.0206       0.0156      0.00506       0.0256       0.0418        0.138        0.144
     22    60       0.0155       0.0149     0.000644       0.0305       0.0408       0.0528       0.0628
     22    70       0.0151       0.0116      0.00342        0.025       0.0361         0.11         0.13
     22    80       0.0703       0.0573        0.013       0.0427       0.0802        0.264        0.306
     22    90        0.136        0.132      0.00444       0.0776        0.122       0.0959       0.0972
     22   100       0.0927       0.0833      0.00937       0.0575       0.0967        0.192        0.248
     22   110       0.0646       0.0601       0.0045       0.0627       0.0821         0.14         0.18
     22   120       0.0598        0.044       0.0158       0.0517       0.0703        0.251        0.265
     22   130         0.46       0.0532        0.407       0.0465       0.0772         1.27         1.34
     22   140        0.118       0.0136        0.105       0.0225        0.039        0.492        0.552
     22   150         0.12       0.0676       0.0526       0.0671       0.0871        0.443        0.614
     22   160       0.0383        0.028       0.0103       0.0386       0.0561        0.153        0.187
     22   170        0.128       0.0191        0.109       0.0275       0.0462        0.744        0.768
     22   180        0.119        0.031       0.0876       0.0379       0.0589        0.467         0.48
     22   190       0.0242      0.00299       0.0212       0.0119       0.0183        0.299        0.325
     22   200       0.0264       0.0132       0.0132       0.0199       0.0385        0.152        0.154
     22   210        0.181        0.169       0.0114       0.0916        0.138        0.227        0.285
     22   220       0.0712         0.06       0.0111       0.0337        0.082        0.127        0.141
     22   230       0.0896       0.0825      0.00704       0.0774       0.0962        0.164        0.216
     22   231        0.111        0.102      0.00953        0.091        0.107        0.131        0.131

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     22     7        0.925        0.871       0.0536        0.078        0.312        0.354         0.39


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              22 1148.837    0.005       0.0664        0.781        0.848       0.0422       0.0858         0.31         1.22
! Validation         22 1148.837    0.005        0.148         6.12         6.27        0.035       0.0991        0.715         3.46
Wall time: 1148.8378799567

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     23    10        0.184       0.0468        0.137       0.0421       0.0725        0.575         0.58
     23    20         0.18        0.017        0.162       0.0336       0.0437        0.452        0.573
     23    30        0.168       0.0452        0.123       0.0254       0.0712        0.481        0.535
     23    40        0.022       0.0209      0.00107       0.0297       0.0484       0.0564       0.0874
     23    50      0.00891      0.00558      0.00333       0.0162        0.025         0.09        0.155
     23    60      0.00658      0.00166      0.00492      0.00874       0.0136        0.161        0.188
     23    70       0.0339       0.0271      0.00685       0.0338       0.0551        0.192        0.213
     23    80       0.0287       0.0257      0.00307       0.0359       0.0537       0.0847        0.117
     23    90         0.24       0.0489        0.191       0.0499        0.074        0.634         0.81
     23   100       0.0147       0.0097      0.00496       0.0175        0.033        0.144        0.157
     23   110       0.0225       0.0218     0.000634       0.0314       0.0495       0.0519       0.0574
     23   120         0.03       0.0233      0.00668        0.037       0.0512        0.148        0.212
     23   130         0.11       0.0327       0.0772       0.0394       0.0605         0.23        0.372
     23   140         1.29      0.00165         1.29      0.00861       0.0136         0.82         1.52
     23   150         83.9       0.0122         83.8       0.0223        0.037         6.22         12.3
     23   160        0.154      0.00892        0.145       0.0169       0.0316         0.88        0.911
     23   170        0.168        0.037        0.131       0.0477       0.0644        0.321        0.485
     23   180       0.0576       0.0493      0.00833       0.0567       0.0744        0.189        0.222
     23   190       0.0314       0.0204        0.011       0.0313       0.0478        0.164        0.216
     23   200       0.0361       0.0196       0.0165        0.031       0.0469         0.19        0.191
     23   210       0.0172        0.017     0.000206        0.031       0.0437       0.0283       0.0384
     23   220       0.0158       0.0128        0.003       0.0198       0.0379       0.0928        0.122
     23   230       0.0198       0.0183      0.00143       0.0324       0.0453        0.044       0.0507
     23   231       0.0073     6.15e-05      0.00723      0.00239      0.00263        0.114        0.114

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     23     7        0.255        0.232       0.0223       0.0459        0.161        0.235        0.259


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              23 1200.264    0.005       0.0592        0.771         0.83       0.0369       0.0791        0.277         1.21
! Validation         23 1200.264    0.005       0.0494         6.14         6.19       0.0285       0.0617         0.64         3.47
Wall time: 1200.2649168204516

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     24    10       0.0574       0.0358       0.0217        0.036       0.0633        0.283        0.303
     24    20       0.0226      0.00832       0.0142        0.019       0.0305        0.183        0.314
     24    30       0.0124      0.00542      0.00701       0.0128       0.0247       0.0944        0.122
     24    40      0.00834      0.00607      0.00227       0.0203       0.0261       0.0944         0.11
     24    50        0.186      0.00198        0.184      0.00965       0.0149        0.328        0.576
     24    60       0.0244      0.00559       0.0188       0.0137        0.025        0.162        0.188
     24    70       0.0188       0.0162      0.00263       0.0268       0.0426       0.0852        0.096
     24    80       0.0145       0.0133      0.00122        0.025       0.0386       0.0676       0.0816
     24    90       0.0385       0.0371      0.00148        0.039       0.0645       0.0867        0.103
     24   100       0.0136      0.00943      0.00421       0.0227       0.0325        0.102        0.123
     24   110      0.00677      0.00622     0.000545       0.0191       0.0264       0.0469       0.0591
     24   120      0.00161      0.00102     0.000594       0.0073       0.0107       0.0336       0.0376
     24   130        0.172       0.0106        0.161        0.021       0.0345        0.941        0.971
     24   140        0.167     0.000288        0.166      0.00424      0.00569        0.824        0.873
     24   150        0.126       0.0865       0.0398         0.05       0.0985        0.394        0.517
     24   160       0.0316       0.0293      0.00224       0.0356       0.0574        0.082       0.0975
     24   170       0.0155       0.0132      0.00221       0.0228       0.0385        0.113        0.126
     24   180       0.0941        0.028       0.0661       0.0409       0.0561        0.502         0.52
     24   190       0.0979      0.00889        0.089       0.0225       0.0316        0.473        0.481
     24   200       0.0167      0.00173       0.0149      0.00817       0.0139        0.198        0.225
     24   210       0.0304       0.0184       0.0121       0.0272       0.0454       0.0889        0.147
     24   220       0.0414       0.0159       0.0255       0.0294       0.0423        0.151        0.214
     24   230      0.00709      0.00197      0.00512       0.0101       0.0149         0.14        0.147
     24   231        0.124        0.123      0.00116       0.0998        0.118       0.0914       0.0914

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     24     7       0.0483       0.0334        0.015       0.0219       0.0612        0.226        0.242


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              24 1251.736    0.005       0.0293        0.775        0.804        0.029       0.0561        0.278         1.22
! Validation         24 1251.736    0.005       0.0173         6.14         6.16       0.0238       0.0422        0.655         3.46
Wall time: 1251.7366879619658

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     25    10       0.0145       0.0121      0.00242        0.019       0.0368       0.0541       0.0668
     25    20       0.0359       0.0352     0.000642       0.0441       0.0629       0.0522       0.0677
     25    30        0.188       0.0134        0.175       0.0252       0.0387        0.708        0.765
     25    40       0.0997       0.0619       0.0379       0.0595       0.0833        0.336        0.342
     25    50       0.0504        0.042      0.00842       0.0493       0.0686        0.145        0.148
     25    60       0.0154      0.00623      0.00912       0.0138       0.0264        0.126        0.128
     25    70       0.0196       0.0146      0.00503        0.029       0.0404        0.124        0.146
     25    80       0.0144      0.00962      0.00476       0.0236       0.0328        0.145        0.154
     25    90      0.00575      0.00449      0.00126       0.0141       0.0224       0.0528       0.0543
     25   100      0.00879      0.00701      0.00178       0.0211        0.028       0.0588        0.066
     25   110      0.00883      0.00643       0.0024       0.0163       0.0268       0.0654       0.0656
     25   120      0.00475      0.00234      0.00241        0.009       0.0162       0.0827        0.096
     25   130       0.0119       0.0074      0.00451        0.017       0.0288        0.146        0.164
     25   140       0.0109      0.00834      0.00256       0.0215       0.0306       0.0538       0.0677
     25   150       0.0299      0.00385        0.026       0.0153       0.0208        0.123        0.216
     25   160      0.00399      0.00398     2.46e-06       0.0118       0.0211      0.00364       0.0042
     25   170      0.00631      0.00279      0.00352       0.0104       0.0177       0.0769       0.0811
     25   180       0.0022      0.00178     0.000414      0.00956       0.0141       0.0367       0.0385
     25   190       0.0155       0.0151     0.000457       0.0335       0.0411       0.0311       0.0314
     25   200        0.185     0.000883        0.184      0.00683      0.00995         0.71        0.744
     25   210       0.0618       0.0235       0.0383       0.0342       0.0513         0.36        0.397
     25   220        0.027       0.0246      0.00247       0.0244       0.0525       0.0741       0.0826
     25   230        0.092       0.0405       0.0516        0.045       0.0674        0.236        0.307
     25   231       0.0027     2.39e-05      0.00268       0.0011      0.00164       0.0693       0.0693

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     25     7       0.0252       0.0134       0.0118       0.0164       0.0388        0.194        0.209


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              25 1303.097    0.005       0.0288        0.773        0.802       0.0293       0.0557        0.254         1.21
! Validation         25 1303.097    0.005       0.0132         6.15         6.16       0.0222       0.0384        0.631         3.47
Wall time: 1303.0977829024196

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     26    10       0.0117       0.0105      0.00123       0.0246       0.0342       0.0365       0.0473
     26    20       0.0241       0.0197      0.00441       0.0277        0.047        0.148        0.161
     26    30      0.00541     0.000299      0.00511       0.0041      0.00579        0.109        0.112
     26    40       0.0387       0.0124       0.0263       0.0233       0.0372        0.296        0.329
     26    50        0.296         0.15        0.146       0.0619        0.129        0.862        0.885
     26    60         0.68        0.667       0.0136         0.17        0.273         0.24        0.302
     26    70       0.0349      0.00594        0.029       0.0138       0.0258        0.306        0.363
     26    80        0.145       0.0222        0.123       0.0321       0.0499        0.821        0.864
     26    90       0.0725       0.0484       0.0241       0.0505       0.0737        0.351        0.381
     26   100       0.0758       0.0673      0.00845       0.0648       0.0869        0.221        0.239
     26   110       0.0407        0.036      0.00471         0.05       0.0635       0.0982        0.103
     26   120       0.0108      0.00363      0.00713       0.0128       0.0202        0.143        0.185
     26   130       0.0316       0.0235      0.00806       0.0311       0.0514        0.141        0.151
     26   140       0.0949       0.0874      0.00757       0.0679        0.099        0.121        0.136
     26   150       0.0185       0.0164      0.00218       0.0242       0.0428       0.0759       0.0833
     26   160       0.0462       0.0423      0.00389       0.0486       0.0689         0.12        0.139
     26   170       0.0161       0.0096      0.00649       0.0213       0.0328        0.163        0.177
     26   180       0.0322       0.0319     0.000298       0.0431       0.0598       0.0314       0.0428
     26   190       0.0114       0.0112     0.000131       0.0272       0.0355       0.0195       0.0252
     26   200       0.0438       0.0418      0.00201       0.0425       0.0685       0.0659       0.0727
     26   210       0.0366       0.0364     0.000206       0.0379       0.0639        0.024       0.0262
     26   220       0.0096      0.00812      0.00148       0.0199       0.0302        0.094        0.103
     26   230      0.00949      0.00885     0.000641       0.0168       0.0315       0.0454        0.048
     26   231       0.0784       0.0782     0.000266       0.0645       0.0936       0.0219       0.0219

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     26     7         0.65        0.628       0.0211       0.0679        0.265        0.189        0.227


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              26 1354.340    0.005       0.0702        0.768        0.838       0.0437       0.0871        0.266         1.21
! Validation         26 1354.340    0.005        0.107         6.16         6.27       0.0308       0.0846        0.607         3.47
Wall time: 1354.3407065309584

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     27    10       0.0187       0.0185      0.00017       0.0368       0.0456       0.0247       0.0303
     27    20       0.0114       0.0108     0.000567       0.0201       0.0348       0.0455       0.0518
     27    30        0.112       0.0178       0.0944       0.0307       0.0447        0.521        0.562
     27    40        0.339        0.135        0.204       0.0914        0.123         0.55        0.633
     27    50        0.153       0.0217        0.131       0.0286       0.0493        0.649        0.698
     27    60       0.0482       0.0207       0.0275       0.0315       0.0482        0.245        0.277
     27    70       0.0458       0.0344       0.0114       0.0418       0.0621        0.226        0.278
     27    80        0.109         0.01       0.0992       0.0226       0.0336        0.503        0.719
     27    90        0.047       0.0374      0.00964       0.0498       0.0648        0.146        0.162
     27   100        0.013      0.00408      0.00896       0.0114       0.0214        0.194        0.219
     27   110       0.0133        0.011      0.00234       0.0243       0.0351        0.107         0.13
     27   120       0.0104      0.00368      0.00668       0.0139       0.0203        0.175        0.219
     27   130      0.00751     0.000454      0.00705      0.00393      0.00713        0.177        0.199
     27   140       0.0756        0.069      0.00662       0.0539        0.088        0.132        0.207
     27   150       0.0342       0.0329      0.00131        0.035       0.0607       0.0635       0.0963
     27   160        0.041       0.0405     0.000464       0.0288       0.0674       0.0489       0.0546
     27   170       0.0221       0.0178      0.00428       0.0168       0.0447       0.0858       0.0979
     27   180        0.341       0.0837        0.257        0.065       0.0969        0.818         1.34
     27   190       0.0374       0.0241       0.0133       0.0348        0.052        0.208        0.211
     27   200       0.0452       0.0381      0.00706       0.0368       0.0654        0.139        0.161
     27   210       0.0228       0.0211      0.00166       0.0311       0.0486       0.0884       0.0981
     27   220       0.0122       0.0109      0.00125        0.021        0.035       0.0378       0.0475
     27   230       0.0268       0.0234      0.00341       0.0352       0.0512       0.0787       0.0902
     27   231       0.0163       0.0157      0.00065       0.0365       0.0419       0.0341       0.0341

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     27     7       0.0396       0.0274       0.0122       0.0179       0.0554        0.171        0.194


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              27 1405.764    0.005       0.0435        0.763        0.806        0.034       0.0687         0.26          1.2
! Validation         27 1405.764    0.005       0.0185         6.15         6.17       0.0256       0.0444        0.604         3.47
Wall time: 1405.7651308197528

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     28    10      0.00712      0.00581      0.00131       0.0146       0.0255       0.0699       0.0876
     28    20      0.00629      0.00381      0.00248        0.015       0.0207       0.0962          0.1
     28    30       0.0174       0.0139      0.00353       0.0251       0.0395       0.0977        0.114
     28    40      0.00892      0.00583      0.00309       0.0184       0.0256       0.0667       0.0744
     28    50      0.00518      0.00458     0.000594       0.0134       0.0227       0.0409       0.0464
     28    60      0.00905      0.00688      0.00217       0.0153       0.0278        0.067       0.0812
     28    70       0.0246       0.0237     0.000876       0.0326       0.0516       0.0498       0.0793
     28    80       0.0167       0.0137      0.00301       0.0238       0.0391       0.0866        0.146
     28    90      0.00175     0.000831     0.000914      0.00656      0.00965       0.0573       0.0633
     28   100      0.00232      0.00105      0.00126      0.00455       0.0109       0.0727       0.0783
     28   110        0.243       0.0195        0.224       0.0304       0.0467         0.96         1.03
     28   120       0.0475      0.00275       0.0448      0.00778       0.0175         0.35          0.4
     28   130       0.0774       0.0422       0.0353        0.046       0.0688        0.426        0.503
     28   140        0.129       0.0512       0.0774       0.0585       0.0757        0.385        0.407
     28   150        0.023       0.0114       0.0116       0.0235       0.0358        0.166        0.169
     28   160       0.0257       0.0172      0.00854       0.0294       0.0439        0.237        0.247
     28   170      0.00464      0.00151      0.00314      0.00645        0.013       0.0925        0.113
     28   180      0.00112     0.000474     0.000644      0.00501      0.00729       0.0454       0.0646
     28   190       0.0186       0.0176     0.000927       0.0353       0.0445       0.0525       0.0735
     28   200       0.0037      0.00343     0.000268       0.0159       0.0196       0.0274        0.034
     28   210        0.137      0.00822        0.128       0.0159       0.0304        0.835        0.863
     28   220       0.0849        0.028       0.0569       0.0303        0.056        0.504        0.618
     28   230       0.0168      0.00155       0.0152      0.00912       0.0132        0.243        0.264
     28   231       0.0216     1.44e-05       0.0216      0.00104      0.00127        0.197        0.197

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     28     7       0.0439       0.0267       0.0172       0.0195       0.0547        0.252        0.269


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              28 1457.128    0.005        0.024        0.774        0.798       0.0269       0.0498        0.256         1.21
! Validation         28 1457.128    0.005       0.0137         6.13         6.14       0.0216       0.0375        0.678         3.46
Wall time: 1457.1292443377897

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     29    10       0.0282      0.00641       0.0218       0.0184       0.0268         0.24        0.298
     29    20       0.0127      0.00931      0.00343       0.0249       0.0323       0.0848         0.11
     29    30       0.0126      0.00739      0.00524       0.0215       0.0288       0.0924        0.097
     29    40       0.0416       0.0185        0.023       0.0263       0.0456        0.135        0.204
     29    50       0.0509       0.0186       0.0322       0.0286       0.0457        0.223        0.259
     29    60       0.0013     0.000335     0.000962      0.00466      0.00612       0.0508        0.053
     29    70      0.00582        0.005     0.000814       0.0113       0.0237       0.0576       0.0612
     29    80      0.00982      0.00922     0.000606       0.0254       0.0321       0.0651        0.066
     29    90      0.00678      0.00568       0.0011       0.0192       0.0252        0.065       0.0757
     29   100       0.0313      0.00392       0.0274       0.0149        0.021        0.276        0.296
     29   110         0.15      0.00939        0.141       0.0184       0.0324        0.618        0.644
     29   120       0.0626       0.0526      0.00997       0.0615       0.0768        0.208        0.235
     29   130        0.109       0.0819       0.0273       0.0633       0.0958        0.254        0.297
     29   140       0.0396       0.0316      0.00798        0.033       0.0596        0.113        0.128
     29   150       0.0513       0.0459      0.00537       0.0532       0.0718        0.178        0.196
     29   160       0.0323       0.0246      0.00771       0.0231       0.0525        0.218        0.235
     29   170       0.0442       0.0389      0.00524       0.0353       0.0661        0.134        0.142
     29   180         86.4       0.0117         86.4       0.0263       0.0361         6.32         12.5
     29   190        0.185      0.00261        0.182      0.00974       0.0171        0.837        0.873
     29   200         1.13       0.0132         1.12       0.0272       0.0385        0.877         1.42
     29   210       0.0545        0.018       0.0365       0.0328        0.045        0.243        0.289
     29   220       0.0259       0.0214      0.00449        0.031        0.049        0.125        0.159
     29   230      0.00643      0.00376      0.00267       0.0126       0.0205        0.122        0.138
     29   231       0.0534       0.0533     8.85e-05       0.0732       0.0773       0.0126       0.0126

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     29     7       0.0854        0.062       0.0233       0.0258       0.0834        0.274        0.292


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              29 1508.522    0.005       0.0458        0.773        0.819       0.0334       0.0702        0.278         1.21
! Validation         29 1508.522    0.005       0.0195         6.13         6.14       0.0232       0.0423         0.68         3.46
Wall time: 1508.523288924247

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     30    10       0.0357       0.0306      0.00507       0.0404       0.0586        0.137        0.173
     30    20       0.0403       0.0162       0.0242       0.0271       0.0426        0.248        0.262
     30    30        0.125      0.00629        0.118       0.0203       0.0266        0.566        0.608
     30    40        0.038       0.0329      0.00504       0.0386       0.0608        0.144        0.182
     30    50       0.0474       0.0215       0.0259       0.0284       0.0491        0.301        0.404
     30    60       0.0273       0.0222      0.00509       0.0344       0.0499       0.0868         0.11
     30    70        0.105       0.0141       0.0907       0.0266       0.0398        0.618        0.675
     30    80       0.0646       0.0507       0.0139       0.0611       0.0754        0.222        0.251
     30    90       0.0554       0.0253       0.0301       0.0352       0.0533        0.362        0.442
     30   100       0.0655       0.0339       0.0316       0.0338       0.0617        0.376        0.475
     30   110       0.0142       0.0113      0.00288       0.0233       0.0356        0.098        0.116
     30   120        0.017       0.0158      0.00118       0.0338       0.0421       0.0375        0.053
     30   130       0.0127       0.0111      0.00163        0.028       0.0352         0.05       0.0582
     30   140       0.0821       0.0805      0.00161       0.0516        0.095       0.0776       0.0958
     30   150      0.00549      0.00435      0.00114       0.0122       0.0221       0.0701       0.0752
     30   160       0.0481       0.0444      0.00372         0.04       0.0705       0.0927        0.116
     30   170      0.00358      0.00231      0.00127       0.0108       0.0161       0.0545        0.065
     30   180      0.00743      0.00577      0.00166       0.0147       0.0254       0.0605       0.0638
     30   190       0.0136       0.0106      0.00297       0.0234       0.0345       0.0823       0.0904
     30   200      0.00996      0.00977     0.000198        0.022       0.0331       0.0272       0.0311
     30   210      0.00202     0.000366      0.00165      0.00511      0.00641       0.0736       0.0866
     30   220         0.18      0.00547        0.175       0.0193       0.0248        0.337        0.563
     30   230        0.023      0.00609       0.0169       0.0182       0.0261        0.159        0.183
     30   231      0.00466      0.00194      0.00272       0.0133       0.0147       0.0699       0.0699

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     30     7       0.0291       0.0174       0.0117       0.0169       0.0441        0.171         0.19


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              30 1559.966    0.005       0.0238        0.763        0.787       0.0265       0.0496         0.25          1.2
! Validation         30 1559.966    0.005       0.0117         6.15         6.17       0.0206       0.0356        0.599         3.47
Wall time: 1559.967057815753

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     31    10       0.0736      0.00431       0.0693       0.0155        0.022         0.38        0.702
     31    20          1.3      0.00341          1.3       0.0107       0.0195        0.829         1.53
     31    30         0.18        0.024        0.156       0.0255       0.0519        0.811         0.88
     31    40          0.1       0.0367       0.0637       0.0382       0.0642        0.399        0.432
     31    50       0.0093      0.00706      0.00224       0.0159       0.0281       0.0469        0.065
     31    60       0.0216       0.0135      0.00811       0.0251       0.0388        0.187        0.224
     31    70        0.164       0.0129        0.151       0.0284       0.0381        0.312        0.528
     31    80          0.1       0.0157       0.0844       0.0335        0.042        0.572        0.602
     31    90       0.0702       0.0167       0.0534       0.0274       0.0433        0.534        0.619
     31   100        0.025      0.00936       0.0156       0.0196       0.0324        0.202         0.22
     31   110       0.0592       0.0493      0.00987       0.0521       0.0744        0.156          0.2
     31   120       0.0483       0.0202       0.0281       0.0299       0.0476        0.392         0.43
     31   130        0.025       0.0163      0.00873       0.0289       0.0427        0.131        0.139
     31   140       0.0161       0.0129      0.00323       0.0291        0.038       0.0682       0.0856
     31   150       0.0207       0.0171      0.00358       0.0291       0.0438       0.0635       0.0806
     31   160       0.0461       0.0175       0.0286       0.0278       0.0443         0.19        0.227
     31   170       0.0119       0.0103      0.00167       0.0245       0.0339       0.0353       0.0547
     31   180       0.0173       0.0161      0.00121       0.0268       0.0425       0.0558       0.0801
     31   190      0.00375      0.00134      0.00241      0.00763       0.0123        0.106        0.118
     31   200       0.0133       0.0106      0.00275        0.021       0.0344        0.105        0.123
     31   210       0.0131       0.0111      0.00199       0.0254       0.0352       0.0556       0.0598
     31   220      0.00635      0.00418      0.00217       0.0143       0.0217       0.0912       0.0968
     31   230      0.00423      0.00228      0.00195      0.00972        0.016       0.0995        0.114
     31   231       0.0019       0.0019     2.26e-06       0.0113       0.0146      0.00202      0.00202

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     31     7       0.0824       0.0703       0.0121        0.026       0.0888        0.177        0.195


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              31 1611.491    0.005         0.02        0.767        0.787       0.0247       0.0466        0.262          1.2
! Validation         31 1611.491    0.005       0.0183         6.15         6.17       0.0204       0.0395        0.606         3.47
Wall time: 1611.4921537563205

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     32    10        0.119        0.102       0.0168       0.0846        0.107        0.269        0.309
     32    20       0.0203       0.0182      0.00208       0.0253       0.0452        0.111        0.122
     32    30       0.0246       0.0235       0.0011       0.0316       0.0514       0.0499       0.0544
     32    40       0.0275       0.0267     0.000768       0.0292       0.0547       0.0532       0.0654
     32    50       0.0638       0.0582       0.0056       0.0456       0.0808        0.124        0.188
     32    60       0.0299       0.0281      0.00177        0.035       0.0562       0.0538       0.0577
     32    70       0.0468       0.0365       0.0103       0.0456        0.064       0.0943         0.14
     32    80      0.00798      0.00691      0.00106       0.0217       0.0278       0.0635       0.0731
     32    90       0.0124      0.00827      0.00412       0.0174       0.0304       0.0619       0.0861
     32   100      0.00526      0.00509     0.000175       0.0146       0.0239         0.02       0.0232
     32   110       0.0123       0.0119     0.000357       0.0255       0.0366       0.0412       0.0416
     32   120      0.00716      0.00654     0.000617       0.0157       0.0271       0.0602       0.0666
     32   130        0.185       0.0278        0.157       0.0386       0.0559         1.05         1.06
     32   140       0.0662       0.0446       0.0215       0.0551       0.0707        0.335        0.357
     32   150       0.0323      0.00229         0.03      0.00949        0.016        0.219        0.232
     32   160       0.0975        0.045       0.0525       0.0411       0.0711        0.537        0.561
     32   170       0.0411       0.0267       0.0144        0.044       0.0547        0.199        0.254
     32   180       0.0498        0.038       0.0118       0.0402       0.0653        0.174        0.208
     32   190       0.0163       0.0132      0.00307       0.0244       0.0385       0.0892        0.108
     32   200        0.017       0.0124       0.0046       0.0299       0.0373        0.123        0.131
     32   210        0.015      0.00926      0.00571       0.0158       0.0322        0.118        0.131
     32   220       0.0303       0.0243      0.00597       0.0311       0.0522        0.137        0.205
     32   230      0.00363      0.00176      0.00187       0.0091        0.014       0.0688        0.104
     32   231      0.00472      0.00453      0.00019       0.0171       0.0225       0.0369       0.0369

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     32     7       0.0245      0.00705       0.0175       0.0134       0.0281        0.251        0.271


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              32 1662.974    0.005       0.0243        0.764        0.789       0.0274       0.0524         0.24          1.2
! Validation         32 1662.974    0.005       0.0103         6.13         6.14       0.0205       0.0345        0.675         3.46
Wall time: 1662.9749858230352

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     33    10       0.0298      0.00195       0.0278      0.00854       0.0148        0.199        0.245
     33    20       0.0141       0.0124      0.00165        0.019       0.0373       0.0746       0.0865
     33    30      0.00556      0.00334      0.00222      0.00825       0.0194       0.0834       0.0881
     33    40      0.00356      0.00277     0.000786       0.0118       0.0176       0.0339       0.0413
     33    50        0.164       0.0162        0.147        0.034       0.0427        0.644        0.683
     33    60       0.0382       0.0244       0.0137       0.0407       0.0523        0.206        0.231
     33    70        0.107        0.019       0.0879       0.0311       0.0462        0.505        0.789
     33    80       0.0284       0.0252       0.0032       0.0345       0.0531       0.0704        0.091
     33    90       0.0136     0.000879       0.0127      0.00683      0.00992        0.145        0.151
     33   100       0.0173       0.0136      0.00363       0.0268       0.0391       0.0722       0.0849
     33   110       0.0239       0.0218      0.00209       0.0315       0.0494        0.102        0.117
     33   120        0.014       0.0127      0.00131       0.0208       0.0377       0.0728       0.0911
     33   130      0.00934      0.00396      0.00538       0.0125       0.0211        0.134        0.161
     33   140      0.00522       0.0041      0.00112       0.0139       0.0214       0.0468       0.0532
     33   150      0.00287      0.00199     0.000888      0.00722       0.0149       0.0618       0.0787
     33   160      0.00627      0.00399      0.00228      0.00984       0.0211       0.0599        0.073
     33   170       0.0111       0.0105     0.000662       0.0246       0.0342       0.0475       0.0541
     33   180      0.00716      0.00542      0.00174       0.0165       0.0247        0.064       0.0687
     33   190       0.0112       0.0093      0.00195       0.0219       0.0323       0.0711       0.0942
     33   200         0.19       0.0636        0.126       0.0543       0.0845        0.466        0.476
     33   210       0.0698       0.0234       0.0464       0.0329       0.0512        0.496        0.538
     33   220        0.102       0.0448       0.0571       0.0485       0.0709        0.299         0.35
     33   230        0.134       0.0774       0.0563       0.0609       0.0932         0.32        0.381
     33   231       0.0187       0.0177     0.000976       0.0327       0.0446       0.0837       0.0837

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     33     7       0.0417       0.0178       0.0239       0.0176       0.0447        0.301        0.326


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              33 1714.573    0.005       0.0183         0.77        0.788       0.0241       0.0462        0.245          1.2
! Validation         33 1714.573    0.005       0.0109         6.11         6.12       0.0198        0.034        0.722         3.46
Wall time: 1714.5741831175983

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     34    10        0.112       0.0341       0.0777       0.0427       0.0619        0.456        0.476
     34    20        0.167        0.112        0.055       0.0832        0.112        0.414        0.418
     34    30       0.0135      0.00463      0.00887       0.0148       0.0228        0.244        0.252
     34    40       0.0129      0.00662      0.00626       0.0194       0.0273       0.0829        0.107
     34    50       0.0145      0.00739       0.0071       0.0159       0.0288        0.131        0.144
     34    60      0.00709      0.00246      0.00464       0.0106       0.0166        0.135         0.14
     34    70       0.0421       0.0272       0.0149       0.0286       0.0553        0.181        0.191
     34    80       0.0278       0.0216      0.00622       0.0385       0.0492        0.138        0.169
     34    90        0.045       0.0281       0.0169        0.046       0.0561        0.221        0.242
     34   100        0.352        0.023        0.329       0.0317       0.0508         1.19         1.48
     34   110       0.0421      0.00894       0.0331         0.02       0.0317        0.352        0.404
     34   120       0.0762       0.0355       0.0407       0.0506       0.0631        0.379         0.49
     34   130       0.0101        0.004      0.00609       0.0126       0.0212        0.122        0.135
     34   140       0.0186       0.0131      0.00551       0.0292       0.0383        0.113        0.134
     34   150       0.0191       0.0082       0.0109       0.0217       0.0303        0.184        0.201
     34   160        0.111       0.0485       0.0621       0.0393       0.0738        0.314        0.375
     34   170      0.00912      0.00751      0.00161       0.0188        0.029       0.0516       0.0616
     34   180       0.0247       0.0167      0.00796       0.0274       0.0433        0.181        0.233
     34   190       0.0167       0.0157      0.00102       0.0272        0.042       0.0642       0.0711
     34   200       0.0151       0.0126      0.00249       0.0241       0.0376       0.0626       0.0668
     34   210       0.0151       0.0139      0.00122       0.0272       0.0395        0.045       0.0467
     34   220      0.00883      0.00769      0.00114       0.0172       0.0294       0.0777       0.0886
     34   230       0.0314       0.0219      0.00945       0.0375       0.0496        0.104        0.132
     34   231       0.0149       0.0148     2.17e-05       0.0282       0.0408       0.0125       0.0125

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     34     7       0.0216       0.0079       0.0137       0.0136       0.0298        0.216        0.235


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              34 1766.108    0.005       0.0239        0.764        0.788       0.0273        0.051        0.273          1.2
! Validation         34 1766.108    0.005         0.01         6.14         6.15         0.02       0.0338        0.648         3.46
Wall time: 1766.1090119564906

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     35    10      0.00716      0.00468      0.00248        0.013       0.0229       0.0627         0.07
     35    20      0.00688      0.00663     0.000246       0.0188       0.0273       0.0261       0.0364
     35    30      0.00303      0.00183       0.0012      0.00931       0.0143       0.0448       0.0493
     35    40      0.00679      0.00592     0.000866       0.0127       0.0258       0.0577       0.0607
     35    50       0.0893      0.00234        0.087       0.0105       0.0162        0.596        0.632
     35    60        0.462        0.236        0.226        0.116        0.163         0.86         0.87
     35    70         1.14        0.965        0.173        0.142        0.329        0.706        0.887
     35    80       0.0831       0.0737      0.00938       0.0508       0.0909        0.153        0.199
     35    90        0.022       0.0153      0.00671       0.0226       0.0414       0.0993         0.11
     35   100       0.0503        0.035       0.0153       0.0424       0.0627        0.313        0.331
     35   110       0.0765       0.0522       0.0244       0.0401       0.0765        0.168        0.209
     35   120       0.0235       0.0151      0.00848       0.0291       0.0411        0.165         0.21
     35   130       0.0411       0.0329       0.0082       0.0437       0.0607        0.173         0.23
     35   140      0.00916      0.00857     0.000582       0.0181        0.031       0.0567       0.0595
     35   150       0.0999       0.0678       0.0322       0.0698       0.0872        0.239        0.272
     35   160      0.00686      0.00508      0.00178       0.0155       0.0239       0.0714        0.103
     35   170       0.0248       0.0176      0.00722       0.0314       0.0444       0.0925        0.122
     35   180        0.258        0.218       0.0409         0.11        0.156        0.196        0.283
     35   190       0.0121      0.00863      0.00344       0.0233       0.0311        0.102        0.103
     35   200      0.00577      0.00327       0.0025       0.0107       0.0191       0.0964        0.119
     35   210       0.0166       0.0102      0.00646       0.0213       0.0338        0.165        0.182
     35   220      0.00904      0.00576      0.00329        0.016       0.0254        0.116        0.127
     35   230      0.00942      0.00844     0.000972       0.0207       0.0308       0.0535         0.06
     35   231      0.00256     2.22e-05      0.00253      0.00131      0.00158        0.135        0.135

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     35     7        0.112       0.0885        0.024       0.0278       0.0996        0.304        0.321


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              35 1817.695    0.005       0.0397        0.765        0.805       0.0323       0.0674        0.254         1.21
! Validation         35 1817.695    0.005        0.021         6.12         6.14       0.0204       0.0415        0.717         3.46
Wall time: 1817.6960874926299

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     36    10      0.00725      0.00665     0.000599        0.018       0.0273       0.0369       0.0378
     36    20       0.0165       0.0158     0.000708       0.0292       0.0421       0.0594       0.0713
     36    30      0.00209      0.00139     0.000701      0.00789       0.0125       0.0485       0.0519
     36    40        0.428       0.0215        0.407       0.0293       0.0491         1.29          1.4
     36    50        0.132        0.033       0.0993       0.0347       0.0608        0.493        0.653
     36    60       0.0317      0.00135       0.0303      0.00843       0.0123        0.293        0.316
     36    70         0.36       0.0962        0.263       0.0804        0.104        0.587        0.761
     36    80       0.0897       0.0763       0.0134       0.0592       0.0925        0.186        0.245
     36    90       0.0277       0.0145       0.0132       0.0263       0.0403        0.142        0.164
     36   100       0.0586        0.017       0.0416       0.0307       0.0437        0.414        0.474
     36   110        0.119       0.0482       0.0712       0.0522       0.0735        0.347        0.366
     36   120        0.196      0.00034        0.196       0.0043      0.00617        0.791         1.11
     36   130       0.0408       0.0168        0.024       0.0253       0.0434        0.243        0.277
     36   140       0.0627       0.0154       0.0473       0.0286       0.0415        0.316        0.344
     36   150       0.0405       0.0321      0.00834       0.0331         0.06        0.212        0.224
     36   160       0.0274       0.0197      0.00771         0.03        0.047        0.172        0.203
     36   170      0.00708      0.00405      0.00304       0.0133       0.0213       0.0838        0.147
     36   180       0.0175       0.0101      0.00734       0.0218       0.0337        0.122        0.171
     36   190       0.0133       0.0124     0.000813       0.0254       0.0373       0.0569       0.0666
     36   200        0.014       0.0125      0.00146       0.0246       0.0374       0.0579       0.0648
     36   210      0.00247      0.00126      0.00121      0.00734       0.0119       0.0511       0.0581
     36   220      0.00303     0.000764      0.00227      0.00605      0.00925       0.0642       0.0727
     36   230        0.136        0.133      0.00326       0.0793        0.122       0.0917        0.134
     36   231       0.0088      0.00848     0.000318       0.0246       0.0308       0.0477       0.0477

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     36     7        0.109        0.091       0.0182       0.0297        0.101        0.241        0.258


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              36 1869.164    0.005       0.0259        0.779        0.805       0.0274       0.0532        0.303         1.23
! Validation         36 1869.164    0.005       0.0216         6.14         6.16        0.021        0.042        0.656         3.46
Wall time: 1869.1649414366111

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     37    10       0.0147       0.0128      0.00183        0.023       0.0379       0.0606        0.078
     37    20       0.0101      0.00716      0.00298       0.0184       0.0283        0.107        0.114
     37    30       0.0163       0.0153      0.00093        0.029       0.0415       0.0704       0.0817
     37    40       0.0601       0.0286       0.0315       0.0245       0.0566        0.271        0.273
     37    50        0.143      0.00504        0.138        0.014       0.0238        0.632        0.687
     37    60       0.0324       0.0024         0.03       0.0107       0.0164        0.299        0.303
     37    70        0.191        0.183      0.00842       0.0868        0.143        0.167        0.233
     37    80        0.141         0.12       0.0211        0.079        0.116        0.246        0.302
     37    90        0.226        0.221      0.00456       0.0981        0.158        0.138        0.179
     37   100       0.0644       0.0589      0.00548       0.0592       0.0813        0.128         0.16
     37   110       0.0252       0.0189      0.00635       0.0284        0.046        0.134        0.171
     37   120       0.0209       0.0181      0.00278        0.029        0.045       0.0831        0.137
     37   130       0.0332       0.0248      0.00838       0.0338       0.0527        0.127        0.166
     37   140       0.0115      0.00634      0.00516        0.018       0.0267        0.149        0.161
     37   150        0.186     0.000938        0.185      0.00696       0.0103         1.01         1.05
     37   160       0.0564        0.026       0.0304       0.0326        0.054        0.272        0.311
     37   170       0.0179      0.00017       0.0178      0.00312      0.00437        0.261        0.272
     37   180      0.00934      0.00674       0.0026       0.0199       0.0275       0.0916        0.106
     37   190       0.0138      0.00996      0.00379       0.0171       0.0334        0.046       0.0825
     37   200       0.0443       0.0291       0.0152         0.03       0.0571        0.278        0.331
     37   210      0.00335     0.000316      0.00303      0.00398      0.00595       0.0581        0.074
     37   220      0.00226      0.00176     0.000494      0.00838       0.0141       0.0365       0.0383
     37   230      0.00803      0.00696      0.00106       0.0218       0.0279       0.0456       0.0591
     37   231     0.000658     0.000418      0.00024      0.00511      0.00684       0.0415       0.0415

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     37     7       0.0713       0.0534       0.0179       0.0241       0.0774        0.229         0.25


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              37 1920.765    0.005       0.0512        0.775        0.826       0.0354       0.0741         0.28         1.21
! Validation         37 1920.765    0.005       0.0179         6.14         6.16       0.0223       0.0409        0.646         3.46
Wall time: 1920.765720336698

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     38    10      0.00249       0.0012      0.00129      0.00685       0.0116       0.0587       0.0631
     38    20      0.00875      0.00677      0.00198       0.0171       0.0275        0.102        0.114
     38    30       0.0125       0.0104      0.00215       0.0251       0.0341        0.093        0.113
     38    40       0.0112      0.00937      0.00185       0.0227       0.0324       0.0613       0.0723
     38    50       0.0333       0.0223       0.0111        0.036         0.05        0.179        0.282
     38    60        0.153      0.00877        0.145       0.0168       0.0314        0.781        0.841
     38    70       0.0546       0.0159       0.0387        0.027       0.0422        0.364        0.385
     38    80       0.0625       0.0268       0.0356       0.0373       0.0549        0.197        0.253
     38    90       0.0711       0.0112       0.0599       0.0226       0.0355        0.495        0.539
     38   100        0.211       0.0476        0.163       0.0412       0.0731        0.418        0.541
     38   110        0.143         0.14      0.00328       0.0981        0.125        0.114        0.129
     38   120       0.0985       0.0971      0.00142       0.0721        0.104       0.0646       0.0779
     38   130       0.0362       0.0241       0.0121       0.0228        0.052        0.136        0.147
     38   140       0.0181        0.015      0.00314       0.0283        0.041       0.0935        0.104
     38   150      0.00602      0.00458      0.00144       0.0158       0.0227       0.0781        0.102
     38   160       0.0308       0.0284      0.00239       0.0417       0.0564       0.0809        0.128
     38   170        0.186        0.021        0.165       0.0262       0.0485        0.321        0.545
     38   180       0.0263       0.0222      0.00404       0.0265       0.0499        0.134        0.136
     38   190      0.00717      0.00659     0.000579       0.0207       0.0272       0.0429       0.0594
     38   200       0.0209       0.0182      0.00268       0.0361       0.0452        0.105        0.138
     38   210         0.32       0.0134        0.306       0.0256       0.0387        0.417        0.742
     38   220       0.0341       0.0134       0.0207       0.0269       0.0387         0.14        0.193
     38   230       0.0038      0.00167      0.00213       0.0073       0.0137       0.0512       0.0618
     38   231       0.0273        0.025      0.00235       0.0452       0.0529       0.0649       0.0649

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     38     7        0.107       0.0944       0.0128       0.0299        0.103        0.181          0.2


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              38 1972.308    0.005       0.0246        0.762        0.786       0.0269       0.0514        0.238         1.19
! Validation         38 1972.308    0.005       0.0217         6.15         6.17       0.0205       0.0417        0.609         3.47
Wall time: 1972.3090819409117

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     39    10       0.0052      0.00382      0.00138       0.0149       0.0207       0.0479       0.0498
     39    20       0.0169       0.0109       0.0059        0.019        0.035        0.086        0.103
     39    30        0.103       0.0137       0.0896       0.0245       0.0392         0.58         0.61
     39    40        0.094       0.0126       0.0815       0.0199       0.0375        0.488        0.556
     39    50        0.133       0.0278        0.105       0.0329       0.0558        0.384        0.462
     39    60       0.0131      0.00921      0.00392       0.0209       0.0321        0.114        0.128
     39    70       0.0192       0.0134      0.00575        0.027       0.0388        0.145        0.179
     39    80         0.02       0.0143      0.00568       0.0238         0.04        0.165          0.2
     39    90       0.0185       0.0129      0.00553       0.0209       0.0381        0.111        0.124
     39   100       0.0101      0.00692      0.00321       0.0172       0.0279        0.107        0.128
     39   110        0.113      0.00112        0.112      0.00821       0.0112        0.661        0.689
     39   120        0.217        0.138        0.079       0.0847        0.124        0.322        0.379
     39   130       0.0549       0.0473      0.00763        0.038       0.0728        0.181        0.221
     39   140       0.0359       0.0152       0.0206        0.026       0.0413        0.269        0.361
     39   150       0.0149      0.00902      0.00584       0.0192       0.0318       0.0806        0.102
     39   160       0.0395       0.0317      0.00775       0.0425       0.0596        0.153        0.226
     39   170      0.00912      0.00848     0.000643       0.0214       0.0308       0.0369       0.0437
     39   180       0.0243        0.022      0.00232       0.0228       0.0497       0.0682       0.0687
     39   190       0.0275       0.0238      0.00367       0.0261       0.0517        0.106        0.126
     39   200      0.00826       0.0017      0.00656      0.00838       0.0138         0.16        0.175
     39   210       0.0191       0.0181     0.000953       0.0292       0.0451       0.0414       0.0483
     39   220       0.0125       0.0105      0.00201       0.0221       0.0343       0.0823        0.106
     39   230      0.00814      0.00477      0.00336        0.015       0.0231       0.0821       0.0897
     39   231      0.00583     2.18e-05      0.00581      0.00139      0.00156        0.102        0.102

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     39     7       0.0179       0.0099      0.00801       0.0148       0.0333        0.151        0.171


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              39 2023.890    0.005        0.022        0.765        0.787       0.0255       0.0487        0.247          1.2
! Validation         39 2023.890    0.005         0.01         6.15         6.16       0.0198       0.0335        0.595         3.47
Wall time: 2023.8912719469517

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     40    10      0.00777      0.00584      0.00193       0.0135       0.0256       0.0754       0.0929
     40    20         0.26     0.000766        0.259      0.00698      0.00927         0.71         1.36
     40    30       0.0106      0.00954       0.0011       0.0173       0.0327       0.0391       0.0463
     40    40      0.00502      0.00403     0.000992       0.0164       0.0212       0.0479        0.081
     40    50      0.00475      0.00387     0.000876      0.00999       0.0208       0.0636       0.0754
     40    60        0.115       0.0243       0.0904       0.0382       0.0522        0.769        0.806
     40    70        0.112       0.0898       0.0219       0.0683          0.1        0.184        0.222
     40    80       0.0285       0.0132       0.0153       0.0231       0.0385        0.288        0.295
     40    90      0.00408       0.0013      0.00278       0.0089       0.0121       0.0669       0.0796
     40   100       0.0306      0.00427       0.0263       0.0149       0.0219        0.269        0.286
     40   110       0.0829       0.0124       0.0705       0.0224       0.0373        0.516        0.569
     40   120       0.0281      0.00876       0.0194       0.0164       0.0313        0.348        0.373
     40   130       0.0171      0.00683       0.0102       0.0143       0.0277        0.152        0.171
     40   140       0.0486       0.0141       0.0345       0.0223       0.0398        0.372        0.454
     40   150       0.0108      0.00431      0.00646       0.0149        0.022       0.0993        0.119
     40   160       0.0223       0.0142      0.00817       0.0231       0.0398        0.168        0.201
     40   170        0.166       0.0071        0.159        0.015       0.0282        0.429        0.558
     40   180       0.0104      0.00394      0.00647       0.0133        0.021         0.13        0.138
     40   190       0.0179       0.0127      0.00516       0.0209       0.0377       0.0968        0.105
     40   200      0.00431      0.00172       0.0026      0.00785       0.0139       0.0881        0.109
     40   210       0.0124      0.00988      0.00252       0.0199       0.0333       0.0894        0.133
     40   220       0.0105      0.00674      0.00373       0.0174       0.0275       0.0916       0.0936
     40   230       0.0131       0.0106      0.00246       0.0199       0.0345       0.0751       0.0907
     40   231     0.000456     0.000452     3.54e-06       0.0053      0.00712      0.00504      0.00504

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     40     7       0.0251       0.0164      0.00875       0.0162       0.0428        0.161        0.185


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              40 2075.493    0.005       0.0177        0.765        0.783       0.0231       0.0436        0.242          1.2
! Validation         40 2075.493    0.005         0.01         6.15         6.16       0.0188       0.0325        0.608         3.47
Wall time: 2075.4939874773845

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     41    10      0.00504      0.00425     0.000796       0.0142       0.0218       0.0537       0.0748
     41    20      0.00685      0.00634      0.00051       0.0208       0.0267       0.0362       0.0372
     41    30      0.00968      0.00859      0.00109       0.0192        0.031       0.0412       0.0463
     41    40       0.0688        0.065      0.00376       0.0499       0.0854       0.0513       0.0829
     41    50       0.0118        0.009      0.00282       0.0217       0.0318        0.114        0.135
     41    60      0.00955       0.0066      0.00295       0.0198       0.0272       0.0826       0.0868
     41    70      0.00334      0.00183       0.0015       0.0088       0.0143        0.059       0.0759
     41    80       0.0107      0.00444      0.00622       0.0132       0.0223        0.082        0.109
     41    90       0.0192       0.0191     0.000174       0.0351       0.0462       0.0268       0.0318
     41   100       0.0818       0.0169       0.0649       0.0263       0.0436        0.399        0.479
     41   110        0.124       0.0474        0.077       0.0421       0.0729        0.364         0.38
     41   120       0.0454       0.0406      0.00474       0.0491       0.0675         0.16        0.177
     41   130        0.043       0.0284       0.0147       0.0383       0.0564        0.185        0.207
     41   140      0.00549      0.00309      0.00241        0.012       0.0186       0.0947        0.101
     41   150       0.0449       0.0277       0.0172       0.0401       0.0557        0.123        0.179
     41   160       0.0218       0.0208      0.00101       0.0345       0.0482       0.0575       0.0838
     41   170      0.00583      0.00513       0.0007       0.0157        0.024        0.041       0.0557
     41   180      0.00999      0.00321      0.00679      0.00969        0.019        0.162         0.17
     41   190       0.0136      0.00538      0.00826       0.0121       0.0246        0.164        0.181
     41   200       0.0201      0.00952       0.0106        0.024       0.0327       0.0832        0.138
     41   210      0.00184      0.00104     0.000798       0.0075       0.0108       0.0409       0.0433
     41   220       0.0035      0.00294     0.000564       0.0116       0.0182       0.0328       0.0383
     41   230       0.0344      0.00301       0.0314      0.00929       0.0184        0.303        0.343
     41   231       0.0722       0.0174       0.0548       0.0359       0.0441        0.314        0.314

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     41     7       0.0732       0.0619       0.0113       0.0254       0.0833        0.196        0.207


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              41 2127.045    0.005       0.0153        0.762        0.777       0.0216       0.0421        0.194         1.19
! Validation         41 2127.045    0.005       0.0159         6.14         6.16       0.0189       0.0366        0.633         3.46
Wall time: 2127.046235010028

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     42    10       0.0958        0.043       0.0529       0.0462       0.0694        0.446        0.481
     42    20        0.032       0.0276      0.00444       0.0381       0.0556        0.113        0.149
     42    30       0.0284       0.0186      0.00977       0.0345       0.0457        0.199        0.213
     42    40        0.025       0.0126       0.0124       0.0194       0.0375        0.165        0.171
     42    50       0.0136      0.00571      0.00788       0.0165       0.0253        0.164        0.181
     42    60      0.00769      0.00494      0.00275       0.0122       0.0235         0.12        0.136
     42    70      0.00288     0.000887      0.00199      0.00585      0.00997        0.078        0.098
     42    80       0.0153       0.0144     0.000812       0.0274       0.0402       0.0448       0.0475
     42    90      0.00668      0.00552      0.00117       0.0182       0.0249       0.0501       0.0537
     42   100       0.0182       0.0134      0.00477       0.0261       0.0387        0.123        0.126
     42   110       0.0494      0.00299       0.0464       0.0115       0.0183        0.506        0.528
     42   120        0.115       0.0236       0.0916       0.0301       0.0515        0.583        0.641
     42   130        0.122       0.0162        0.106       0.0215       0.0426        0.418        0.435
     42   140        0.011      0.00896      0.00201       0.0211       0.0317       0.0612       0.0769
     42   150       0.0133      0.00285       0.0105       0.0112       0.0179        0.201        0.231
     42   160       0.0122      0.00729       0.0049       0.0205       0.0286        0.131        0.154
     42   170       0.0311       0.0232      0.00788       0.0331        0.051        0.139        0.166
     42   180       0.0164       0.0152      0.00119       0.0209       0.0413       0.0392       0.0491
     42   190       0.0115      0.00845      0.00301       0.0167       0.0308        0.111         0.13
     42   200      0.00625      0.00389      0.00236       0.0109       0.0209       0.0967        0.106
     42   210      0.00765      0.00587      0.00178       0.0137       0.0256       0.0862       0.0942
     42   220        0.125       0.0114        0.114       0.0195       0.0358        0.792        0.816
     42   230       0.0593       0.0154       0.0439       0.0267       0.0416        0.366        0.428
     42   231       0.0543      0.00161       0.0527       0.0107       0.0134        0.307        0.307

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     42     7       0.0622       0.0417       0.0206       0.0224       0.0684        0.279        0.302


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              42 2178.529    0.005       0.0195         0.77         0.79       0.0248       0.0472        0.279         1.21
! Validation         42 2178.529    0.005       0.0127         6.12         6.13       0.0183       0.0339        0.704         3.46
Wall time: 2178.5306089958176

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     43    10        0.027      0.00896       0.0181       0.0183       0.0317        0.307         0.36
     43    20       0.0948       0.0868      0.00802       0.0653       0.0986        0.184        0.231
     43    30       0.0364       0.0296      0.00679       0.0301       0.0576        0.151        0.183
     43    40       0.0284      0.00226       0.0262      0.00661       0.0159        0.272        0.296
     43    50       0.0361      0.00431       0.0318        0.013        0.022        0.188        0.256
     43    60       0.0183       0.0173     0.000998       0.0365       0.0441       0.0708       0.0841
     43    70       0.0328       0.0251       0.0077       0.0286        0.053        0.186        0.221
     43    80       0.0101      0.00797      0.00213       0.0182       0.0299       0.0714       0.0733
     43    90        0.009      0.00645      0.00255       0.0172       0.0269       0.0736       0.0742
     43   100      0.00292     0.000125       0.0028      0.00202      0.00374        0.069       0.0709
     43   110        0.122       0.0445       0.0775       0.0433       0.0706        0.636        0.669
     43   120         0.13        0.094       0.0355       0.0697        0.103        0.353         0.39
     43   130        0.092       0.0696       0.0224       0.0591       0.0883        0.382        0.401
     43   140       0.0251       0.0248     0.000253       0.0361       0.0527       0.0375       0.0409
     43   150       0.0208       0.0198     0.000974       0.0339       0.0471       0.0615       0.0698
     43   160      0.00489      0.00277      0.00212       0.0123       0.0176       0.0818        0.086
     43   170      0.00634      0.00468      0.00167       0.0183       0.0229        0.082       0.0884
     43   180       0.0441       0.0293       0.0148       0.0457       0.0573        0.222        0.252
     43   190      0.00709      0.00584      0.00124        0.014       0.0256       0.0569       0.0652
     43   200       0.0143       0.0139     0.000412       0.0271       0.0395        0.024       0.0279
     43   210        0.127       0.0163         0.11       0.0331       0.0427        0.664        0.701
     43   220         0.05       0.0204       0.0296        0.033       0.0478          0.4        0.461
     43   230       0.0619       0.0534       0.0085       0.0507       0.0774        0.104        0.138
     43   231       0.0237     5.49e-06       0.0237     0.000681     0.000785        0.206        0.206

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     43     7        0.142        0.117       0.0247       0.0319        0.115        0.294        0.309


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              43 2229.951    0.005       0.0278        0.767        0.795       0.0292       0.0554        0.251          1.2
! Validation         43 2229.951    0.005       0.0243         6.12         6.15       0.0199        0.043        0.701         3.46
Wall time: 2229.9524269774556

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     44    10      0.00767      0.00393      0.00374       0.0122        0.021        0.119        0.161
     44    20      0.00298     0.000291      0.00268      0.00459      0.00571       0.0536       0.0721
     44    30        0.016       0.0147      0.00137       0.0324       0.0406       0.0373       0.0506
     44    40      0.00978      0.00772      0.00206       0.0139       0.0294       0.0666       0.0688
     44    50       0.0111      0.00896      0.00214       0.0224       0.0317       0.0502       0.0625
     44    60       0.0245       0.0231      0.00137       0.0313       0.0509       0.0713        0.074
     44    70         1.33       0.0176         1.31       0.0224       0.0444         0.88         1.53
     44    80       0.0112      0.00798      0.00321       0.0208       0.0299       0.0984        0.116
     44    90       0.0109      0.00505      0.00589       0.0153       0.0238        0.104        0.113
     44   100       0.0527        0.043      0.00974       0.0282       0.0694        0.128        0.156
     44   110       0.0154        0.014      0.00142       0.0259       0.0396       0.0626       0.0775
     44   120        0.271     0.000163        0.271      0.00314      0.00427        0.749         1.39
     44   130      0.00944       0.0083      0.00115       0.0223       0.0305       0.0797       0.0854
     44   140      0.00426      0.00205      0.00221      0.00802       0.0152       0.0877        0.104
     44   150       0.0178       0.0176     0.000251       0.0347       0.0444       0.0231       0.0314
     44   160      0.00631      0.00577     0.000537       0.0159       0.0254       0.0338       0.0382
     44   170        0.116      0.00474        0.112       0.0178        0.023        0.679        0.723
     44   180         1.11         1.06       0.0531        0.115        0.344        0.352        0.353
     44   190       0.0448       0.0388      0.00596       0.0413        0.066        0.136        0.166
     44   200       0.0729        0.048       0.0248        0.033       0.0734        0.357        0.402
     44   210        0.112       0.0838       0.0282       0.0639       0.0969        0.232        0.243
     44   220       0.0967       0.0703       0.0264       0.0525       0.0888        0.196        0.224
     44   230         0.05       0.0219       0.0281       0.0312       0.0495        0.338         0.36
     44   231       0.0166       0.0139      0.00279       0.0304       0.0394        0.141        0.141

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     44     7       0.0298       0.0109        0.019       0.0155       0.0349        0.261        0.291


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              44 2281.434    0.005       0.0292        0.764        0.793       0.0264       0.0559        0.224         1.19
! Validation         44 2281.434    0.005       0.0114         6.12         6.13       0.0204       0.0358         0.69         3.46
Wall time: 2281.434745805338

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     45    10       0.0278       0.0124       0.0154       0.0205       0.0373        0.194        0.196
     45    20        0.245       0.0158        0.229       0.0245       0.0421        0.481        0.641
     45    30       0.0958      0.00298       0.0928       0.0119       0.0183        0.492         0.51
     45    40        0.121       0.0459       0.0747       0.0555       0.0718        0.318        0.366
     45    50       0.0231       0.0041        0.019       0.0156       0.0215        0.318        0.369
     45    60       0.0332       0.0164       0.0168       0.0292       0.0429        0.214        0.239
     45    70       0.0596       0.0259       0.0336       0.0269       0.0539        0.424        0.433
     45    80        0.126       0.0957       0.0306       0.0642        0.104        0.297        0.321
     45    90       0.0161      0.00565       0.0105        0.011       0.0252        0.243        0.257
     45   100      0.00645      0.00414      0.00231       0.0137       0.0215        0.111        0.123
     45   110       0.0454       0.0451     0.000339       0.0439       0.0711       0.0203       0.0253
     45   120       0.0231        0.017      0.00611       0.0253       0.0436        0.105        0.124
     45   130       0.0201       0.0177      0.00246        0.026       0.0445       0.0796        0.117
     45   140       0.0045      0.00139      0.00312      0.00805       0.0125       0.0714       0.0748
     45   150       0.0281       0.0262      0.00188       0.0377       0.0542       0.0822       0.0879
     45   160       0.0231       0.0195      0.00367       0.0273       0.0467       0.0793       0.0812
     45   170       0.0154       0.0143      0.00112       0.0244       0.0401       0.0562       0.0703
     45   180       0.0152       0.0145     0.000691       0.0245       0.0403       0.0393       0.0476
     45   190       0.0639        0.015       0.0489       0.0279        0.041        0.262        0.309
     45   200         0.14        0.021        0.119        0.026       0.0485        0.786        0.834
     45   210       0.0482       0.0211       0.0271       0.0374       0.0486        0.276        0.408
     45   220       0.0779       0.0463       0.0316       0.0424       0.0721        0.234        0.274
     45   230       0.0189       0.0103      0.00862       0.0186        0.034        0.169        0.176
     45   231       0.0417       0.0348      0.00696       0.0459       0.0625        0.224        0.224

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     45     7        0.613        0.576       0.0364       0.0631        0.254        0.311        0.337


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              45 2332.970    0.005       0.0473        0.769        0.817       0.0356        0.073        0.273         1.21
! Validation         45 2332.970    0.005       0.0931         6.13         6.22       0.0252       0.0768        0.694         3.46
Wall time: 2332.971261125058

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     46    10         85.6      0.00718         85.6       0.0177       0.0284         6.35         12.4
     46    20       0.0734       0.0135       0.0599        0.028       0.0389        0.588        0.656
     46    30       0.0428        0.013       0.0298       0.0265       0.0382        0.303        0.334
     46    40       0.0641        0.036       0.0281       0.0349       0.0635        0.292        0.404
     46    50       0.0207     0.000146       0.0205      0.00278      0.00405        0.241        0.258
     46    60      0.00761      0.00115      0.00647      0.00793       0.0113         0.13        0.179
     46    70       0.0143       0.0128      0.00148       0.0286       0.0379       0.0698        0.103
     46    80       0.0107      0.00607      0.00459       0.0139       0.0261        0.115        0.126
     46    90      0.00473      0.00372      0.00102       0.0156       0.0204       0.0598       0.0647
     46   100        0.013      0.00784      0.00519       0.0198       0.0296       0.0766       0.0992
     46   110        0.013       0.0119      0.00106       0.0255       0.0366       0.0688       0.0872
     46   120       0.0137       0.0128     0.000943       0.0255       0.0379       0.0756       0.0823
     46   130          1.3     0.000558          1.3        0.005      0.00791        0.824         1.53
     46   140        0.014      0.00898      0.00498       0.0206       0.0317       0.0998        0.102
     46   150      0.00634      0.00314      0.00321       0.0121       0.0187       0.0802       0.0837
     46   160       0.0714       0.0628       0.0086       0.0461       0.0839       0.0955        0.124
     46   170       0.0516       0.0307       0.0209       0.0387       0.0587        0.155        0.194
     46   180       0.0664       0.0132       0.0532       0.0292       0.0385        0.377        0.414
     46   190        0.101        0.079       0.0218       0.0371       0.0941        0.274         0.32
     46   200       0.0774       0.0308       0.0466       0.0331       0.0587        0.185        0.293
     46   210        0.032       0.0265      0.00548       0.0445       0.0545        0.136         0.14
     46   220       0.0125       0.0115     0.000911       0.0205        0.036       0.0546        0.056
     46   230      0.00713       0.0061      0.00103       0.0145       0.0262       0.0498       0.0607
     46   231      0.00117      0.00078     0.000391      0.00752      0.00935       0.0265       0.0265

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     46     7        0.151        0.127        0.024       0.0359        0.119        0.287        0.306


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              46 2384.484    0.005       0.0363        0.763        0.799       0.0289       0.0624         0.24         1.19
! Validation         46 2384.484    0.005       0.0294         6.13         6.16       0.0241       0.0488        0.695         3.46
Wall time: 2384.48476816155

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     47    10        0.019       0.0106      0.00841        0.024       0.0344        0.177        0.188
     47    20       0.0141      0.00553       0.0086       0.0167       0.0249         0.17        0.171
     47    30      0.00588      0.00543     0.000448       0.0175       0.0247       0.0324       0.0551
     47    40       0.0165       0.0154      0.00108       0.0293       0.0415       0.0757       0.0803
     47    50       0.0287       0.0202      0.00855       0.0363       0.0476        0.184        0.194
     47    60      0.00143     0.000224      0.00121      0.00385      0.00501       0.0743       0.0892
     47    70       0.0738       0.0218        0.052       0.0317       0.0495        0.381        0.416
     47    80         0.16       0.0295         0.13       0.0244       0.0575        0.762        0.837
     47    90        0.107       0.0345       0.0725       0.0343       0.0622        0.444         0.47
     47   100        0.155        0.144       0.0116       0.0544        0.127        0.218        0.258
     47   110       0.0356       0.0334      0.00213       0.0358       0.0612        0.112        0.124
     47   120       0.0185       0.0136      0.00493       0.0285        0.039        0.126        0.136
     47   130           84      0.00841           84       0.0146       0.0307         6.23         12.3
     47   140        0.151       0.0303        0.121       0.0324       0.0583        0.672        0.707
     47   150        0.032       0.0184       0.0136       0.0253       0.0454        0.194        0.226
     47   160         0.11       0.0473       0.0628       0.0539       0.0728        0.445        0.641
     47   170       0.0944       0.0449       0.0495       0.0436        0.071        0.272        0.333
     47   180       0.0103      0.00221      0.00812        0.011       0.0157       0.0914        0.121
     47   190      0.00965      0.00939     0.000264       0.0242       0.0324       0.0235       0.0245
     47   200      0.00714      0.00438      0.00276       0.0135       0.0222       0.0884        0.116
     47   210         0.01      0.00757      0.00244       0.0204       0.0291        0.105        0.109
     47   220        0.033       0.0311      0.00191       0.0455       0.0591       0.0803       0.0844
     47   230      0.00434       0.0024      0.00194      0.00944       0.0164       0.0804       0.0976
     47   231      0.00355     0.000279      0.00327       0.0042      0.00559       0.0766       0.0766

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     47     7       0.0591       0.0402       0.0189       0.0232       0.0671        0.261        0.288


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              47 2435.925    0.005       0.0231        0.772        0.795       0.0269       0.0502        0.268         1.21
! Validation         47 2435.925    0.005       0.0145         6.13         6.14       0.0206       0.0372        0.686         3.46
Wall time: 2435.926185532473

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     48    10       0.0102      0.00899      0.00126       0.0228       0.0317       0.0588       0.0634
     48    20      0.00866      0.00846     0.000203       0.0214       0.0308       0.0373       0.0381
     48    30       0.0134      0.00937      0.00402        0.026       0.0324       0.0669       0.0849
     48    40       0.0254       0.0173      0.00815       0.0298        0.044        0.174        0.209
     48    50      0.00912      0.00865     0.000475       0.0195       0.0311        0.038       0.0491
     48    60       0.0104      0.00956     0.000851       0.0184       0.0327       0.0466       0.0528
     48    70       0.0246       0.0137       0.0109       0.0258       0.0392        0.102        0.144
     48    80        0.257      0.00678        0.251       0.0201       0.0276        0.702         1.34
     48    90       0.0268       0.0258      0.00102       0.0271       0.0537       0.0526       0.0731
     48   100      0.00703      0.00574      0.00129       0.0199       0.0254        0.084        0.096
     48   110       0.0076      0.00483      0.00277       0.0155       0.0233       0.0993        0.101
     48   120      0.00713       0.0014      0.00573      0.00842       0.0125        0.125        0.135
     48   130      0.00439      0.00326      0.00114       0.0121       0.0191       0.0393       0.0452
     48   140       0.0123       0.0119      0.00041       0.0199       0.0365       0.0321       0.0392
     48   150      0.00795       0.0071     0.000854       0.0192       0.0282       0.0373       0.0391
     48   160       0.0432       0.0105       0.0328       0.0174       0.0342        0.302        0.324
     48   170        0.107      0.00208        0.105      0.00869       0.0153        0.518        0.527
     48   180       0.0238       0.0173      0.00649       0.0287       0.0441       0.0756        0.108
     48   190       0.0828      0.00368       0.0791       0.0114       0.0203        0.535        0.559
     48   200       0.0356        0.018       0.0175       0.0346       0.0449         0.25        0.305
     48   210       0.0225        0.011       0.0115       0.0256       0.0351        0.163        0.183
     48   220       0.0134      0.00279       0.0106       0.0104       0.0177         0.17        0.217
     48   230       0.0125      0.00839      0.00407       0.0161       0.0307       0.0911         0.17
     48   231      0.00652       0.0065     2.37e-05       0.0235        0.027      0.00651      0.00651

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     48     7       0.0371       0.0203       0.0168       0.0171       0.0477        0.242        0.275


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              48 2487.422    0.005       0.0141        0.763        0.777        0.021         0.04        0.225         1.19
! Validation         48 2487.422    0.005      0.00984         6.13         6.14       0.0178       0.0317         0.68         3.46
Wall time: 2487.423054225743

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     49    10        0.366        0.334       0.0321        0.135        0.194        0.346        0.432
     49    20       0.0348       0.0194       0.0154       0.0283       0.0466        0.234        0.249
     49    30       0.0273       0.0214      0.00582       0.0262        0.049        0.133        0.156
     49    40        0.264       0.0136        0.251       0.0256       0.0391        0.735         1.34
     49    50       0.0653       0.0639      0.00148       0.0554       0.0846       0.0584       0.0651
     49    60        0.102      0.00822       0.0933       0.0172       0.0304        0.409        0.409
     49    70       0.0679      0.00718       0.0607        0.015       0.0284        0.461         0.49
     49    80       0.0361       0.0201        0.016       0.0325       0.0474        0.214        0.288
     49    90       0.0149       0.0113       0.0036       0.0268       0.0356        0.102        0.115
     49   100       0.0115       0.0106     0.000893       0.0256       0.0345       0.0729       0.0801
     49   110       0.0208      0.00856       0.0123       0.0187        0.031        0.205        0.226
     49   120       0.0662      0.00917        0.057       0.0224       0.0321        0.395        0.414
     49   130       0.0907      0.00969        0.081       0.0168        0.033        0.574        0.655
     49   140       0.0306      0.00558        0.025       0.0155        0.025         0.27        0.338
     49   150       0.0378       0.0151       0.0227       0.0291       0.0412        0.253        0.352
     49   160       0.0126      0.00729      0.00533       0.0187       0.0286        0.127        0.192
     49   170       0.0485       0.0198       0.0287       0.0316       0.0471        0.135        0.228
     49   180        0.053       0.0455      0.00746       0.0406       0.0714        0.103        0.117
     49   190       0.0301        0.027      0.00314       0.0409        0.055       0.0634       0.0798
     49   200       0.0112      0.00746      0.00374       0.0177       0.0289       0.0832       0.0887
     49   210       0.0134       0.0119      0.00145       0.0262       0.0365       0.0402       0.0513
     49   220        0.032       0.0272      0.00485       0.0345       0.0552         0.11        0.148
     49   230        0.013      0.00915      0.00385        0.023        0.032       0.0621       0.0855
     49   231      0.00302       0.0014      0.00162        0.009       0.0125        0.108        0.108

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     49     7        0.235        0.219       0.0169       0.0399        0.157        0.216        0.242


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              49 2538.962    0.005       0.0235        0.762        0.786       0.0272       0.0522        0.251         1.19
! Validation         49 2538.962    0.005       0.0396         6.14         6.18       0.0217       0.0524        0.636         3.46
Wall time: 2538.9628760926425

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     50    10       0.0111      0.00771      0.00336       0.0184       0.0294       0.0726       0.0792
     50    20       0.0403       0.0385      0.00176       0.0393       0.0657       0.0545       0.0742
     50    30       0.0178       0.0148        0.003       0.0243       0.0407       0.0619       0.0746
     50    40      0.00226      0.00107      0.00119      0.00857        0.011       0.0617       0.0741
     50    50       0.0126       0.0114      0.00123       0.0277       0.0357        0.081       0.0905
     50    60      0.00657      0.00448      0.00209       0.0138       0.0224       0.0509       0.0612
     50    70      0.00899      0.00771      0.00129       0.0207       0.0294        0.084       0.0892
     50    80      0.00192     6.65e-05      0.00186      0.00203      0.00273          0.1        0.103
     50    90      0.00629      0.00471      0.00158       0.0116        0.023       0.0868       0.0933
     50   100      0.00775      0.00696     0.000791       0.0185       0.0279       0.0323       0.0399
     50   110      0.00637      0.00539     0.000978       0.0155       0.0246       0.0585       0.0695
     50   120      0.00905      0.00736      0.00169       0.0166       0.0287       0.0705       0.0874
     50   130       0.0116       0.0088      0.00276       0.0223       0.0314       0.0565       0.0703
     50   140       0.0129       0.0127     0.000287        0.028       0.0377       0.0202       0.0227
     50   150      0.00311      0.00263     0.000483       0.0115       0.0172       0.0396       0.0533
     50   160      0.00677      0.00407       0.0027       0.0138       0.0214        0.105        0.122
     50   170        0.455       0.0347        0.421         0.04       0.0624         1.06          1.1
     50   180        0.129       0.0195         0.11       0.0189       0.0467        0.648        0.768
     50   190       0.0705       0.0297       0.0408       0.0367       0.0577        0.441         0.54
     50   200       0.0217      0.00314       0.0185       0.0119       0.0188        0.265        0.296
     50   210        0.217        0.179       0.0384       0.0896        0.142         0.36        0.498
     50   220        0.323        0.292        0.031        0.102        0.181         0.39        0.395
     50   230      0.00478       0.0015      0.00328      0.00886        0.013        0.118        0.133
     50   231      0.00366      0.00061      0.00305      0.00713      0.00827       0.0739       0.0739

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     50     7        0.338        0.308       0.0299       0.0493        0.186        0.332        0.357


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              50 2590.559    0.005       0.0515        0.767        0.818       0.0281       0.0764        0.248         1.21
! Validation         50 2590.559    0.005       0.0554         6.11         6.16       0.0251       0.0619        0.737         3.46
Wall time: 2590.5601694164798

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     51    10       0.0045       0.0039     0.000594       0.0146       0.0209       0.0385       0.0424
     51    20       0.0587      0.00708       0.0516       0.0174       0.0282        0.241        0.316
     51    30       0.0125       0.0108       0.0017       0.0234       0.0348       0.0673       0.0788
     51    40         1.22         1.15       0.0663        0.131         0.36        0.189        0.346
     51    50        0.033       0.0185       0.0144       0.0275       0.0456        0.244        0.263
     51    60        0.054       0.0218       0.0323       0.0389       0.0494        0.149        0.241
     51    70      0.00546      0.00479     0.000676       0.0122       0.0232       0.0308       0.0348
     51    80       0.0433       0.0315       0.0119        0.043       0.0594        0.191        0.276
     51    90        0.127        0.042       0.0851        0.055       0.0686        0.562        0.583
     51   100        0.145        0.042        0.103       0.0501       0.0686        0.727        0.742
     51   110         0.15       0.0261        0.124       0.0377       0.0541        0.838        0.894
     51   120        0.111       0.0714       0.0397       0.0469       0.0895        0.396        0.533
     51   130       0.0594       0.0369       0.0226        0.022       0.0643         0.25        0.272
     51   140       0.0452       0.0367      0.00851        0.048       0.0641        0.148        0.155
     51   150        0.072       0.0231       0.0489       0.0312       0.0509         0.38        0.565
     51   160      0.00747      0.00277       0.0047       0.0133       0.0176        0.136        0.184
     51   170       0.0135      0.00799      0.00554       0.0218       0.0299        0.114        0.127
     51   180        0.021       0.0143      0.00675       0.0185         0.04       0.0947         0.11
     51   190       0.0105      0.00421      0.00632      0.00926       0.0217         0.15        0.179
     51   200       0.0247       0.0216      0.00306        0.036       0.0493       0.0976        0.105
     51   210      0.00499     0.000185      0.00481      0.00217      0.00455        0.166        0.179
     51   220        0.015       0.0115       0.0035       0.0256       0.0359       0.0631       0.0792
     51   230       0.0107      0.00499      0.00568       0.0179       0.0237        0.138        0.139
     51   231       0.0128       0.0103      0.00242       0.0268        0.034        0.132        0.132

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     51     7       0.0903       0.0711       0.0193       0.0281       0.0893        0.261        0.291


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              51 2642.100    0.005       0.0336        0.759        0.792       0.0294       0.0603        0.269          1.2
! Validation         51 2642.100    0.005       0.0191         6.12         6.14       0.0212       0.0406        0.684         3.46
Wall time: 2642.1010862542316

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     52    10      0.00338       0.0026     0.000783       0.0126       0.0171       0.0719        0.075
     52    20       0.0122       0.0115     0.000728       0.0251       0.0359       0.0413       0.0469
     52    30      0.00735      0.00549      0.00186        0.014       0.0248       0.0482       0.0578
     52    40       0.0983       0.0038       0.0945        0.013       0.0206        0.513        0.542
     52    50       0.0743        0.024       0.0503       0.0366       0.0519        0.463         0.52
     52    60       0.0371       0.0073       0.0298       0.0196       0.0286        0.236        0.251
     52    70       0.0357       0.0302      0.00548        0.039       0.0582        0.111        0.123
     52    80        0.044       0.0089       0.0351       0.0198       0.0316        0.376        0.488
     52    90       0.0201       0.0166      0.00351       0.0263       0.0431       0.0736       0.0906
     52   100       0.0164      0.00362       0.0128       0.0109       0.0202        0.213        0.223
     52   110       0.0154       0.0144     0.000967         0.03       0.0402       0.0583       0.0726
     52   120       0.0153       0.0137      0.00166       0.0294       0.0392       0.0529       0.0654
     52   130       0.0212       0.0185      0.00265       0.0319       0.0456       0.0515       0.0715
     52   140      0.00562      0.00356      0.00206       0.0117         0.02       0.0727       0.0805
     52   150       0.0122       0.0107      0.00152       0.0213       0.0347       0.0879        0.104
     52   160      0.00701      0.00601     0.000996       0.0159        0.026       0.0506       0.0623
     52   170       0.0077      0.00541      0.00229       0.0153       0.0246        0.068       0.0819
     52   180     0.000988     0.000287     0.000701      0.00335      0.00567       0.0441       0.0515
     52   190       0.0801       0.0194       0.0607       0.0306       0.0467        0.365        0.365
     52   200       0.0794       0.0394         0.04       0.0525       0.0665         0.34        0.369
     52   210       0.0374       0.0315      0.00587       0.0391       0.0595        0.185          0.2
     52   220      0.00353       0.0032     0.000324       0.0115        0.019       0.0389       0.0482
     52   230       0.0182        0.012      0.00619       0.0279       0.0367        0.129        0.151
     52   231      0.00827     0.000506      0.00776      0.00671      0.00753        0.118        0.118

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     52     7        0.157         0.14       0.0168       0.0353        0.125        0.239        0.258


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              52 2693.689    0.005       0.0175        0.766        0.783       0.0227       0.0432        0.245          1.2
! Validation         52 2693.689    0.005       0.0262         6.13         6.16       0.0188       0.0433        0.659         3.46
Wall time: 2693.69005168695

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     53    10        0.015      0.00878      0.00619       0.0146       0.0314        0.152        0.172
     53    20       0.0153       0.0123      0.00307       0.0276       0.0371       0.0979        0.108
     53    30      0.00509      0.00481     0.000279       0.0166       0.0232       0.0185       0.0224
     53    40      0.00684      0.00556      0.00128       0.0178        0.025       0.0413       0.0482
     53    50      0.00583      0.00407      0.00176       0.0128       0.0213       0.0707       0.0753
     53    60      0.00364      0.00148      0.00216      0.00664       0.0129       0.0569       0.0634
     53    70        0.335        0.071        0.264        0.065       0.0892         1.02         1.07
     53    80        0.221       0.0898        0.132       0.0549          0.1        0.708        0.756
     53    90        0.072       0.0033       0.0687        0.012       0.0192        0.548        0.629
     53   100       0.0821       0.0467       0.0354       0.0332       0.0724        0.315         0.46
     53   110       0.0158      0.00721      0.00855       0.0161       0.0284        0.144        0.157
     53   120       0.0323      0.00809       0.0242       0.0176       0.0301        0.338         0.36
     53   130       0.0312       0.0166       0.0145       0.0274       0.0432        0.172        0.179
     53   140       0.0142      0.00818      0.00597        0.016       0.0303        0.158         0.17
     53   150       0.0114      0.00819      0.00321       0.0187       0.0303       0.0971        0.107
     53   160      0.00288     0.000876        0.002      0.00722      0.00991       0.0665        0.118
     53   170       0.0117      0.00492      0.00675       0.0152       0.0235        0.159        0.168
     53   180         0.01      0.00926     0.000747       0.0251       0.0322       0.0547       0.0732
     53   190       0.0134       0.0126     0.000758       0.0287       0.0376       0.0561       0.0703
     53   200       0.0143       0.0136     0.000606        0.032       0.0391       0.0375       0.0459
     53   210       0.0315       0.0188       0.0127       0.0296        0.046        0.192         0.29
     53   220       0.0123       0.0118     0.000496       0.0282       0.0364       0.0291       0.0369
     53   230       0.0116       0.0115     8.24e-05       0.0286        0.036      0.00859       0.0122
     53   231       0.0404       0.0319      0.00849       0.0455       0.0598        0.247        0.247

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     53     7       0.0844       0.0719       0.0124       0.0278       0.0898          0.2         0.22


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              53 2745.208    0.005       0.0203        0.761        0.781       0.0241       0.0467        0.246          1.2
! Validation         53 2745.208    0.005       0.0171         6.14         6.16       0.0187       0.0373        0.629         3.46
Wall time: 2745.2084973547608

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     54    10      0.00216      0.00142      0.00074      0.00843       0.0126       0.0508       0.0545
     54    20        0.103      0.00196        0.101       0.0106       0.0148         0.52        0.539
     54    30       0.0964       0.0359       0.0605       0.0375       0.0634        0.341        0.369
     54    40       0.0278       0.0191      0.00865        0.033       0.0463        0.181        0.195
     54    50        0.022      0.00851       0.0135       0.0206       0.0309        0.172        0.174
     54    60       0.0157      0.00905      0.00668       0.0208       0.0319        0.138        0.183
     54    70       0.0107       0.0065      0.00421       0.0182        0.027       0.0629       0.0869
     54    80       0.0314       0.0232      0.00814       0.0289        0.051        0.107        0.121
     54    90       0.0119      0.00966      0.00223       0.0213       0.0329       0.0863        0.126
     54   100      0.00476       0.0046     0.000157       0.0147       0.0227       0.0162       0.0189
     54   110      0.00693      0.00381      0.00312       0.0119       0.0207       0.0631       0.0748
     54   120      0.00678        0.006     0.000784       0.0186       0.0259       0.0535       0.0706
     54   130      0.00504      0.00146      0.00358      0.00729       0.0128       0.0871        0.104
     54   140      0.00586      0.00318      0.00268       0.0112       0.0189       0.0651       0.0693
     54   150      0.00685      0.00446      0.00239       0.0148       0.0224       0.0976        0.112
     54   160       0.0201       0.0119      0.00822       0.0262       0.0365        0.171        0.175
     54   170      0.00692      0.00318      0.00374       0.0129       0.0189        0.161        0.164
     54   180       0.0665       0.0198       0.0468       0.0348       0.0471        0.515        0.576
     54   190       0.0503       0.0146       0.0357       0.0252       0.0405        0.357        0.421
     54   200       0.0275      0.00783       0.0197       0.0218       0.0296        0.218        0.227
     54   210      0.00937      0.00165      0.00772      0.00802       0.0136        0.142        0.188
     54   220       0.0119      0.00785      0.00403       0.0168       0.0297        0.137        0.167
     54   230      0.00921      0.00869     0.000517        0.023       0.0312       0.0464       0.0559
     54   231       0.0726       0.0653      0.00733       0.0642       0.0856        0.229        0.229

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     54     7        0.127        0.113       0.0145       0.0334        0.112        0.222        0.246


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              54 2796.797    0.005       0.0151        0.764        0.779       0.0214        0.041        0.241         1.19
! Validation         54 2796.797    0.005       0.0224         6.13         6.16       0.0184       0.0407        0.652         3.46
Wall time: 2796.79816769436

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     55    10       0.0181      0.00515        0.013       0.0149        0.024        0.115        0.153
     55    20      0.00507      0.00149      0.00359         0.01       0.0129       0.0639       0.0802
     55    30       0.0301       0.0178       0.0123       0.0301       0.0446        0.122        0.149
     55    40      0.00819      0.00404      0.00415       0.0141       0.0213       0.0996        0.125
     55    50      0.00437      0.00375     0.000616       0.0152       0.0205       0.0378       0.0398
     55    60      0.00928       0.0087     0.000576       0.0228       0.0312        0.036       0.0399
     55    70      0.00709      0.00656     0.000523       0.0185       0.0271       0.0415       0.0593
     55    80       0.0141       0.0132      0.00098       0.0255       0.0384       0.0425       0.0506
     55    90      0.00562      0.00284      0.00278       0.0125       0.0178        0.052       0.0706
     55   100       0.0229     0.000506       0.0224      0.00445      0.00753        0.232        0.255
     55   110       0.0905       0.0174        0.073       0.0295       0.0442        0.545        0.582
     55   120       0.0218      0.00473       0.0171       0.0141        0.023        0.233        0.283
     55   130         0.02       0.0152      0.00485       0.0258       0.0413        0.101        0.117
     55   140        0.017       0.0115      0.00558       0.0257       0.0359         0.12        0.153
     55   150      0.00247      0.00226     0.000211      0.00879       0.0159       0.0153       0.0198
     55   160       0.0195       0.0155        0.004       0.0246       0.0417       0.0735       0.0868
     55   170      0.00844      0.00752     0.000916       0.0208        0.029       0.0433       0.0494
     55   180       0.0357       0.0339      0.00181       0.0415       0.0616       0.0586       0.0655
     55   190       0.0131       0.0123     0.000715       0.0263       0.0372       0.0542       0.0716
     55   200       0.0225      0.00735       0.0152       0.0176       0.0287        0.214        0.256
     55   210       0.0643       0.0057       0.0586       0.0159       0.0253        0.486        0.529
     55   220       0.0302     0.000518       0.0297       0.0049      0.00762        0.286        0.314
     55   230        0.015      0.00443       0.0106       0.0135       0.0223        0.114        0.138
     55   231      0.00929      0.00884      0.00045       0.0249       0.0315       0.0284       0.0284

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     55     7       0.0914       0.0792       0.0122       0.0292       0.0942        0.204        0.227


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              55 2848.360    0.005       0.0131        0.759        0.772         0.02       0.0385        0.213         1.19
! Validation         55 2848.360    0.005       0.0173         6.14         6.15       0.0178       0.0368         0.64         3.46
Wall time: 2848.360868068412

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     56    10       0.0148      0.00177        0.013      0.00746       0.0141        0.185        0.217
     56    20         81.7       0.0163         81.7       0.0315       0.0428          6.3         12.1
     56    30        0.165       0.0447         0.12       0.0389       0.0708        0.592        0.716
     56    40        0.109        0.012       0.0974       0.0184       0.0366        0.727        0.749
     56    50       0.0213       0.0159       0.0053       0.0324       0.0423        0.106        0.117
     56    60       0.0216      0.00529       0.0163       0.0154       0.0243        0.178        0.215
     56    70       0.0383       0.0106       0.0277       0.0197       0.0345        0.275        0.313
     56    80       0.0177      0.00789      0.00984       0.0189       0.0297        0.171        0.248
     56    90         1.19      0.00134         1.19      0.00934       0.0123        0.907         1.47
     56   100      0.00803      0.00368      0.00435       0.0139       0.0203       0.0922        0.102
     56   110      0.00826      0.00691      0.00135        0.017       0.0278       0.0824       0.0984
     56   120       0.0119      0.00997      0.00195       0.0221       0.0334       0.0785        0.118
     56   130      0.00798      0.00515      0.00284       0.0165        0.024        0.109        0.114
     56   140      0.00873       0.0062      0.00252       0.0167       0.0264       0.0849         0.11
     56   150       0.0117      0.00969      0.00199       0.0241        0.033        0.056       0.0621
     56   160       0.0078      0.00558      0.00222       0.0162        0.025       0.0716       0.0792
     56   170      0.00538      0.00336      0.00202       0.0101       0.0194         0.05       0.0603
     56   180         0.01      0.00887      0.00116       0.0208       0.0315       0.0422       0.0515
     56   190      0.00624      0.00456      0.00169       0.0135       0.0226       0.0948         0.11
     56   200      0.00655      0.00432      0.00223        0.012        0.022       0.0893       0.0995
     56   210      0.00825      0.00689      0.00136       0.0177       0.0278       0.0727       0.0858
     56   220      0.00638      0.00474      0.00164       0.0114       0.0231       0.0709       0.0793
     56   230          0.4       0.0047        0.395       0.0111        0.023        0.913         1.68
     56   231      0.00492     1.84e-06      0.00492     0.000335     0.000454        0.094        0.094

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     56     7       0.0869       0.0791       0.0079       0.0277       0.0941        0.145         0.17


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              56 2899.870    0.005       0.0163        0.753        0.769       0.0223       0.0417        0.236         1.19
! Validation         56 2899.870    0.005       0.0177         6.16         6.18       0.0179       0.0374        0.583         3.47
Wall time: 2899.8715961044654

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     57    10       0.0089      0.00476      0.00413       0.0147       0.0231        0.102        0.106
     57    20      0.00509      0.00427     0.000821       0.0147       0.0219       0.0583        0.063
     57    30      0.00959      0.00931     0.000277       0.0224       0.0323       0.0325       0.0446
     57    40      0.00491      0.00447      0.00044       0.0178       0.0224       0.0292       0.0311
     57    50       0.0725      0.00802       0.0645       0.0189         0.03        0.587        0.601
     57    60       0.0737       0.0135       0.0602       0.0236       0.0389          0.3        0.329
     57    70       0.0677       0.0209       0.0468         0.03       0.0484         0.24         0.29
     57    80       0.0307       0.0177       0.0129       0.0289       0.0446        0.241        0.282
     57    90       0.0233       0.0129       0.0104       0.0231        0.038         0.17         0.18
     57   100       0.0072      0.00373      0.00347       0.0128       0.0205       0.0974        0.148
     57   110      0.00447     0.000604      0.00386       0.0047      0.00823       0.0942        0.134
     57   120       0.0187       0.0156      0.00307       0.0271       0.0419         0.11         0.14
     57   130       0.0122        0.012     0.000144       0.0277       0.0367       0.0181       0.0321
     57   140       0.0779      0.00279       0.0751      0.00858       0.0177        0.439        0.449
     57   150       0.0641       0.0219       0.0422       0.0214       0.0496        0.378        0.438
     57   160       0.0268       0.0128        0.014       0.0257       0.0378        0.206        0.253
     57   170        0.876        0.796       0.0805       0.0942        0.299        0.264        0.388
     57   180       0.0159      0.00846       0.0074        0.019       0.0308       0.0965        0.115
     57   190       0.0155       0.0116      0.00389       0.0267        0.036       0.0843       0.0956
     57   200       0.0145       0.0112      0.00333       0.0167       0.0354       0.0683       0.0785
     57   210       0.0251        0.019      0.00613        0.032       0.0462        0.153        0.209
     57   220      0.00271      0.00176     0.000952       0.0108       0.0141       0.0483       0.0611
     57   230        0.414      0.00897        0.405       0.0234       0.0317        0.876          1.7
     57   231      0.00612      0.00611     6.47e-06       0.0211       0.0262      0.00681      0.00681

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     57     7       0.0797       0.0667        0.013       0.0272       0.0865        0.207        0.234


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              57 2951.492    0.005        0.015        0.762        0.777       0.0215       0.0416        0.235         1.19
! Validation         57 2951.492    0.005       0.0154         6.14         6.15       0.0175       0.0352        0.639         3.46
Wall time: 2951.4931410914287

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     58    10       0.0088      0.00655      0.00225       0.0165       0.0271       0.0855        0.103
     58    20      0.00244      0.00202     0.000417        0.011       0.0151       0.0401       0.0465
     58    30      0.00439      0.00395     0.000442       0.0157        0.021        0.031       0.0388
     58    40      0.00287       0.0019     0.000968      0.00794       0.0146       0.0462       0.0553
     58    50      0.00277     0.000612      0.00216      0.00444      0.00828       0.0743       0.0757
     58    60      0.00439       0.0027      0.00169      0.00996       0.0174        0.101         0.11
     58    70      0.00743      0.00603      0.00139       0.0176        0.026       0.0565       0.0604
     58    80       0.0148       0.0101      0.00463       0.0202       0.0337        0.148        0.169
     58    90        0.377       0.0117        0.366       0.0232       0.0363        0.905         1.62
     58   100        0.168      0.00782         0.16       0.0202       0.0296        0.325        0.538
     58   110      0.00862      0.00799     0.000633       0.0203       0.0299       0.0427       0.0489
     58   120      0.00781      0.00676      0.00105       0.0157       0.0275       0.0674       0.0779
     58   130      0.00866       0.0082     0.000459       0.0218       0.0303       0.0434       0.0477
     58   140        0.156       0.0149        0.142       0.0283       0.0409         1.01         1.01
     58   150        0.069       0.0286       0.0404       0.0411       0.0567        0.409        0.528
     58   160       0.0326       0.0173       0.0153       0.0239        0.044        0.249        0.276
     58   170       0.0354       0.0192       0.0162       0.0301       0.0464        0.141        0.175
     58   180       0.0084      0.00644      0.00196       0.0209       0.0269         0.07       0.0757
     58   190      0.00878      0.00597       0.0028       0.0179       0.0259       0.0692       0.0735
     58   200       0.0433       0.0065       0.0368       0.0199        0.027         0.28        0.282
     58   210       0.0958      0.00162       0.0942      0.00573       0.0135        0.731        0.764
     58   220       0.0395       0.0186       0.0209       0.0342       0.0456        0.189        0.202
     58   230       0.0241       0.0192      0.00497       0.0366       0.0464       0.0714        0.101
     58   231       0.0205     0.000877       0.0196      0.00725      0.00991        0.375        0.375

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     58     7        0.254        0.227       0.0269       0.0429        0.159        0.323        0.347


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              58 3003.057    0.005       0.0194        0.763        0.782       0.0226       0.0463         0.23         1.19
! Validation         58 3003.057    0.005       0.0381         6.11         6.15       0.0185       0.0499         0.74         3.46
Wall time: 3003.0583141567186

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     59    10         0.02      0.00987       0.0102       0.0192       0.0333        0.203        0.223
     59    20        0.385       0.0166        0.368       0.0313       0.0432        0.871         1.62
     59    30       0.0109      0.00395      0.00695       0.0148       0.0211        0.104        0.112
     59    40        0.077       0.0528       0.0242       0.0386       0.0769        0.308        0.372
     59    50       0.0165       0.0128      0.00368       0.0228       0.0378       0.0984        0.125
     59    60       0.0497       0.0472      0.00254       0.0388       0.0727       0.0894         0.12
     59    70      0.00301      0.00065      0.00236       0.0062      0.00854        0.124         0.13
     59    80      0.00418      0.00122      0.00296      0.00666       0.0117        0.116        0.144
     59    90       0.0387        0.029      0.00974        0.027        0.057        0.116        0.137
     59   100       0.0103      0.00947     0.000876       0.0259       0.0326       0.0478       0.0614
     59   110       0.0125       0.0119     0.000597       0.0281       0.0365       0.0295       0.0346
     59   120       0.0167        0.013      0.00364       0.0272       0.0382        0.103         0.16
     59   130        0.125      0.00943        0.116       0.0171       0.0325        0.808        0.841
     59   140       0.0578       0.0446       0.0131       0.0441       0.0707        0.195         0.22
     59   150        0.029       0.0128       0.0163       0.0266       0.0378        0.266        0.335
     59   160       0.0668     0.000979       0.0659      0.00615       0.0105        0.525        0.568
     59   170       0.0406       0.0134       0.0272       0.0268       0.0387        0.251        0.256
     59   180       0.0301      0.00369       0.0264       0.0129       0.0203        0.303        0.351
     59   190       0.0249       0.0229      0.00203       0.0385       0.0507       0.0963          0.1
     59   200       0.0141       0.0104       0.0037       0.0279       0.0341       0.0574       0.0838
     59   210       0.0238       0.0187      0.00517       0.0305       0.0457       0.0931        0.101
     59   220       0.0794      0.00869       0.0707       0.0199       0.0312         0.43        0.712
     59   230      0.00529      0.00304      0.00225       0.0121       0.0185       0.0933        0.123
     59   231      0.00567     6.21e-05      0.00561      0.00237      0.00264        0.201        0.201

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     59     7       0.0493       0.0342       0.0151       0.0221       0.0619        0.225        0.257


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              59 3054.424    0.005        0.023         0.76        0.783       0.0251       0.0506        0.241         1.19
! Validation         59 3054.424    0.005       0.0114         6.13         6.14        0.018       0.0326        0.659         3.46
Wall time: 3054.425208051689

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     60    10       0.0211       0.0193      0.00175       0.0315       0.0466       0.0593       0.0727
     60    20       0.0129       0.0104      0.00256       0.0196       0.0341       0.0989        0.107
     60    30      0.00878       0.0067      0.00208       0.0127       0.0274       0.0697       0.0828
     60    40      0.00357      0.00193      0.00164      0.00666       0.0147       0.0655       0.0738
     60    50      0.00853      0.00689      0.00164       0.0155       0.0278       0.0911       0.0925
     60    60       0.0136       0.0128     0.000837       0.0308       0.0378       0.0423       0.0465
     60    70       0.0082      0.00765     0.000549       0.0218       0.0293       0.0428       0.0563
     60    80      0.00519      0.00391      0.00128       0.0149       0.0209       0.0573       0.0626
     60    90       0.0018     0.000678      0.00112      0.00625      0.00872       0.0684       0.0733
     60   100       0.0116       0.0102      0.00145       0.0233       0.0338       0.0453       0.0535
     60   110      0.00321      0.00179      0.00142      0.00851       0.0142       0.0498       0.0504
     60   120       0.0747       0.0497        0.025       0.0421       0.0747        0.299        0.334
     60   130       0.0529        0.021        0.032       0.0369       0.0485         0.31        0.318
     60   140       0.0533       0.0365       0.0168       0.0345       0.0639        0.193        0.251
     60   150       0.0222       0.0104       0.0118       0.0189       0.0341        0.136        0.145
     60   160       0.0674       0.0175       0.0499       0.0339       0.0443         0.35        0.382
     60   170       0.0373      0.00863       0.0287       0.0218       0.0311        0.263        0.282
     60   180       0.0443      0.00706       0.0372       0.0228       0.0281        0.242        0.263
     60   190       0.0368     0.000599       0.0362      0.00548       0.0082        0.385        0.409
     60   200       0.0601       0.0126       0.0475       0.0149       0.0376        0.315        0.351
     60   210       0.0619       0.0508       0.0111       0.0473       0.0755        0.154        0.183
     60   220       0.0211      0.00277       0.0183       0.0125       0.0176        0.242        0.245
     60   230       0.0467       0.0461     0.000618       0.0467       0.0719       0.0446       0.0563
     60   231       0.0392       0.0372      0.00202       0.0429       0.0646         0.12         0.12

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     60     7         0.14        0.121       0.0194       0.0335        0.116        0.259        0.287


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              60 3106.002    0.005       0.0167        0.758        0.775       0.0218       0.0423        0.224         1.19
! Validation         60 3106.002    0.005       0.0231         6.13         6.15       0.0184       0.0409        0.679         3.46
Wall time: 3106.002783903852

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     61    10         1.03        0.946       0.0864        0.127        0.326        0.215        0.394
     61    20         1.25      0.00519         1.24       0.0131       0.0241        0.851          1.5
     61    30       0.0147       0.0117      0.00306       0.0234       0.0362       0.0746       0.0858
     61    40      0.00947      0.00557      0.00391       0.0133        0.025       0.0767       0.0848
     61    50       0.0119       0.0109     0.000978       0.0261        0.035       0.0498        0.066
     61    60       0.0128       0.0113       0.0015       0.0267       0.0356       0.0714       0.0799
     61    70       0.0058      0.00484     0.000962       0.0168       0.0233       0.0286       0.0415
     61    80       0.0106      0.00938      0.00124       0.0231       0.0324       0.0433       0.0499
     61    90       0.0282       0.0267      0.00145       0.0289       0.0547       0.0458       0.0515
     61   100      0.00627      0.00531     0.000961       0.0138       0.0244       0.0711       0.0728
     61   110       0.0045      0.00367     0.000833       0.0137       0.0203       0.0587       0.0654
     61   120        0.024        0.023     0.000994       0.0357       0.0508        0.059       0.0726
     61   130        0.107       0.0134       0.0939       0.0285       0.0388        0.614        0.649
     61   140       0.0914       0.0552       0.0362       0.0597       0.0786        0.291        0.413
     61   150       0.0453       0.0393      0.00603       0.0502       0.0664        0.129        0.133
     61   160       0.0434       0.0235       0.0199       0.0327       0.0514        0.294        0.344
     61   170       0.0231      0.00263       0.0205      0.00795       0.0172        0.293        0.322
     61   180       0.0206      0.00874       0.0118       0.0168       0.0313        0.184         0.21
     61   190       0.0193        0.017      0.00222       0.0297       0.0437       0.0561       0.0652
     61   200       0.0326       0.0297       0.0029       0.0272       0.0577        0.111        0.142
     61   210       0.0161      0.00553       0.0106       0.0153       0.0249        0.189        0.211
     61   220          0.1     0.000193          0.1       0.0034      0.00465        0.526        0.553
     61   230       0.0342       0.0127       0.0216       0.0289       0.0377        0.201        0.263
     61   231       0.0442       0.0272       0.0169       0.0477       0.0553        0.349        0.349

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     61     7        0.149        0.134        0.015       0.0362        0.122        0.231        0.257


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              61 3157.563    0.005       0.0271        0.761        0.788       0.0252       0.0555        0.232         1.19
! Validation         61 3157.563    0.005       0.0249         6.13         6.16       0.0182        0.042        0.659         3.46
Wall time: 3157.564030081965

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     62    10       0.0123      0.00982      0.00245       0.0201       0.0332       0.0904        0.104
     62    20      0.00952      0.00249      0.00703       0.0108       0.0167        0.141        0.171
     62    30        0.041       0.0214       0.0197       0.0269        0.049        0.209        0.242
     62    40       0.0122      0.00624      0.00601       0.0152       0.0265        0.153        0.174
     62    50      0.00779      0.00412      0.00367       0.0156       0.0215       0.0828       0.0925
     62    60       0.0122       0.0114     0.000835       0.0267       0.0358       0.0752       0.0774
     62    70      0.00994      0.00858      0.00136       0.0222        0.031        0.067       0.0964
     62    80       0.0269       0.0248      0.00209       0.0398       0.0527       0.0774       0.0839
     62    90      0.00868      0.00607      0.00261       0.0183       0.0261       0.0851        0.109
     62   100       0.0105      0.00476      0.00573       0.0143       0.0231        0.151        0.158
     62   110      0.00559      0.00357      0.00202       0.0124         0.02       0.0735       0.0844
     62   120      0.00487      0.00433     0.000539       0.0158        0.022       0.0272       0.0311
     62   130       0.0271       0.0073       0.0198       0.0199       0.0286        0.294        0.331
     62   140       0.0427      0.00628       0.0364       0.0171       0.0265        0.388        0.419
     62   150       0.0365       0.0048       0.0317       0.0124       0.0232        0.384        0.419
     62   160       0.0203      0.00729        0.013       0.0154       0.0286         0.25        0.271
     62   170       0.0136      0.00974      0.00389       0.0237        0.033        0.087        0.103
     62   180       0.0271       0.0256      0.00152       0.0377       0.0536       0.0717       0.0788
     62   190      0.00933      0.00869     0.000644       0.0216       0.0312       0.0495       0.0676
     62   200       0.0765      0.00207       0.0744       0.0088       0.0152        0.622        0.635
     62   210        0.051      0.00796        0.043       0.0186       0.0299        0.302        0.309
     62   220       0.0208      0.00375        0.017       0.0161       0.0205        0.145        0.176
     62   230       0.0225       0.0108       0.0117       0.0263       0.0348        0.254        0.288
     62   231     0.000611     9.71e-05     0.000514      0.00294       0.0033       0.0608       0.0608

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     62     7        0.105       0.0891        0.016       0.0309          0.1        0.239        0.271


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              62 3209.047    0.005       0.0154        0.759        0.775       0.0216       0.0405        0.235         1.19
! Validation         62 3209.047    0.005       0.0182         6.13         6.15       0.0176        0.037        0.673         3.46
Wall time: 3209.048608349636

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     63    10        0.386      0.00537        0.381       0.0161       0.0245        0.971         1.64
     63    20       0.0225      0.00443       0.0181       0.0124       0.0223        0.242         0.26
     63    30      0.00731      0.00205      0.00526       0.0117       0.0152       0.0834       0.0975
     63    40       0.0106       0.0066        0.004       0.0184       0.0272       0.0884       0.0934
     63    50       0.0091      0.00523      0.00387       0.0156       0.0242        0.118        0.127
     63    60       0.0129      0.00782       0.0051       0.0222       0.0296        0.162        0.185
     63    70       0.0102      0.00828      0.00196       0.0212       0.0305       0.0681       0.0788
     63    80      0.00562      0.00359      0.00203       0.0144       0.0201        0.106        0.112
     63    90      0.00333       0.0022      0.00113       0.0107       0.0157       0.0498       0.0561
     63   100       0.0196       0.0175      0.00214       0.0333       0.0442       0.0823       0.0845
     63   110          1.4         1.31       0.0862         0.16        0.383        0.228        0.393
     63   120       0.0178       0.0161      0.00172        0.027       0.0424       0.0721       0.0766
     63   130      0.00489      0.00358      0.00131       0.0143         0.02       0.0836       0.0925
     63   140      0.00483      0.00437     0.000458       0.0158       0.0221       0.0428       0.0573
     63   150      0.00419      0.00405     0.000136       0.0143       0.0213       0.0193       0.0237
     63   160       0.0123      0.00875      0.00352       0.0198       0.0313       0.0888       0.0919
     63   170       0.0666        0.017       0.0496       0.0332       0.0437        0.276        0.322
     63   180       0.0388      0.00916       0.0296       0.0227       0.0321        0.329        0.376
     63   190       0.0122      0.00664      0.00557       0.0146       0.0273        0.179          0.2
     63   200       0.0102      0.00657      0.00362       0.0202       0.0271       0.0865       0.0996
     63   210       0.0378       0.0113       0.0265       0.0225       0.0356        0.309        0.355
     63   220        0.116       0.0116        0.105       0.0208       0.0361          0.5        0.504
     63   230       0.0364      0.00296       0.0334      0.00963       0.0182        0.327        0.375
     63   231       0.0297     0.000149       0.0296      0.00323      0.00409        0.461        0.461

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     63     7        0.164        0.141       0.0227       0.0357        0.126        0.294         0.33


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              63 3260.657    0.005       0.0164        0.758        0.774       0.0214         0.04        0.229         1.19
! Validation         63 3260.657    0.005       0.0256         6.12         6.14       0.0179       0.0422        0.726         3.46
Wall time: 3260.6583657916635

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     64    10       0.0219       0.0163      0.00562        0.032       0.0427        0.196        0.201
     64    20       0.0271       0.0177      0.00936       0.0274       0.0446        0.246        0.259
     64    30       0.0535       0.0143       0.0392       0.0299       0.0401         0.52         0.53
     64    40        0.049      0.00867       0.0403        0.018       0.0312        0.251        0.273
     64    50       0.0612       0.0182        0.043        0.033       0.0452        0.175        0.278
     64    60        0.022        0.014      0.00806       0.0262       0.0396        0.158         0.16
     64    70       0.0138       0.0114      0.00248       0.0271       0.0357        0.109        0.133
     64    80       0.0323       0.0307      0.00159        0.046       0.0586       0.0665       0.0983
     64    90       0.0061       0.0015       0.0046      0.00732        0.013        0.113        0.136
     64   100       0.0101      0.00626      0.00382       0.0189       0.0265       0.0926       0.0991
     64   110      0.00972       0.0023      0.00742      0.00939       0.0161        0.129        0.134
     64   120       0.0152       0.0142     0.000997       0.0282       0.0399       0.0349       0.0436
     64   130      0.00636      0.00554     0.000817       0.0173       0.0249       0.0379       0.0423
     64   140      0.00598      0.00555     0.000434       0.0184       0.0249       0.0424       0.0557
     64   150       0.0277       0.0256      0.00209       0.0331       0.0536       0.0744        0.107
     64   160         0.25       0.0204        0.229       0.0354       0.0479        0.616        0.702
     64   170        0.063       0.0189       0.0441       0.0233        0.046        0.261        0.297
     64   180       0.0202      0.00774       0.0125       0.0236       0.0295        0.222        0.241
     64   190       0.0299       0.0121       0.0178        0.027       0.0368        0.171        0.191
     64   200       0.0145       0.0135        0.001       0.0247       0.0389       0.0602        0.063
     64   210       0.0139       0.0074      0.00646       0.0208       0.0288        0.149          0.2
     64   220      0.00738      0.00472      0.00266       0.0141        0.023       0.0989        0.128
     64   230      0.00759      0.00677     0.000817       0.0167       0.0276       0.0593       0.0746
     64   231      0.00185      0.00043      0.00142      0.00576      0.00695        0.101        0.101

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     64     7       0.0768        0.058       0.0188       0.0257       0.0806        0.263        0.298


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              64 3312.297    0.005       0.0168        0.759        0.776       0.0237       0.0441        0.246         1.19
! Validation         64 3312.297    0.005       0.0147         6.13         6.14       0.0184       0.0351         0.69         3.46
Wall time: 3312.297664697282

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     65    10       0.0105      0.00881      0.00173       0.0229       0.0314       0.0407       0.0557
     65    20       0.0157       0.0154     0.000293       0.0309       0.0416       0.0327       0.0452
     65    30      0.00728      0.00706     0.000228       0.0211       0.0281       0.0269       0.0274
     65    40       0.0107      0.00997      0.00076        0.024       0.0334        0.043       0.0458
     65    50      0.00882      0.00823     0.000593       0.0239       0.0304       0.0486        0.051
     65    60       0.0111      0.00674      0.00436       0.0152       0.0275        0.102        0.114
     65    70      0.00904      0.00883     0.000211       0.0198       0.0315        0.028       0.0328
     65    80      0.00368      0.00162      0.00206      0.00663       0.0135       0.0909       0.0985
     65    90      0.00983       0.0093     0.000532       0.0254       0.0323        0.056       0.0618
     65   100       0.0046      0.00419     0.000411       0.0157       0.0217       0.0351       0.0543
     65   110      0.00208     0.000559      0.00152      0.00532      0.00792       0.0517       0.0536
     65   120       0.0127        0.012     0.000731       0.0231       0.0367       0.0413       0.0438
     65   130      0.00241      0.00227     0.000141       0.0108       0.0159       0.0194       0.0215
     65   140      0.00981       0.0077      0.00211       0.0238       0.0294       0.0687       0.0737
     65   150       0.0036       0.0016        0.002      0.00793       0.0134       0.0483       0.0599
     65   160       0.0134       0.0111      0.00223       0.0233       0.0353       0.0503       0.0634
     65   170      0.00699       0.0056      0.00139        0.015       0.0251       0.0424       0.0545
     65   180       0.0121      0.00952      0.00255       0.0182       0.0327        0.114        0.118
     65   190       0.0272       0.0018       0.0254        0.011       0.0142        0.263        0.275
     65   200         81.5       0.0272         81.5       0.0379       0.0553         6.34         12.1
     65   210       0.0773       0.0218       0.0556       0.0319       0.0494          0.4        0.508
     65   220       0.0399       0.0128       0.0271       0.0278       0.0378        0.262        0.274
     65   230        0.226      0.00171        0.225      0.00791       0.0138        0.825         1.26
     65   231        0.044       0.0137       0.0302       0.0307       0.0392        0.466        0.466

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     65     7       0.0915       0.0731       0.0184       0.0284       0.0905        0.267          0.3


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              65 3363.753    0.005       0.0127        0.752        0.765       0.0191       0.0373        0.199         1.18
! Validation         65 3363.753    0.005       0.0164         6.12         6.14       0.0179       0.0361        0.698         3.46
Wall time: 3363.7543822219595

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     66    10       0.0152      0.00316       0.0121       0.0129       0.0188        0.219        0.221
     66    20       0.0854      0.00622       0.0791       0.0203       0.0264        0.609        0.637
     66    30        0.121       0.0755       0.0459       0.0524        0.092        0.437        0.509
     66    40       0.0914        0.039       0.0523       0.0393       0.0662        0.474         0.53
     66    50       0.0423       0.0175       0.0248       0.0263       0.0442        0.237        0.246
     66    60         82.6      0.00846         82.6       0.0147       0.0308         6.36         12.2
     66    70       0.0295       0.0112       0.0182       0.0258       0.0355        0.245        0.283
     66    80       0.0659       0.0096       0.0563       0.0222       0.0328        0.358        0.373
     66    90       0.0325      0.00393       0.0286       0.0142        0.021        0.262        0.398
     66   100       0.0346       0.0155       0.0191       0.0272       0.0417         0.22        0.264
     66   110       0.0245       0.0189      0.00562       0.0334        0.046       0.0933        0.115
     66   120       0.0204       0.0146      0.00579       0.0293       0.0405        0.096        0.114
     66   130         1.17       0.0045         1.17       0.0158       0.0225        0.744         1.45
     66   140       0.0199       0.0195     0.000387       0.0293       0.0467       0.0468       0.0514
     66   150      0.00705      0.00396      0.00309       0.0153       0.0211       0.0984        0.142
     66   160       0.0163      0.00345       0.0128       0.0111       0.0197         0.18        0.186
     66   170       0.0097      0.00611      0.00359       0.0182       0.0262       0.0726       0.0855
     66   180      0.00863      0.00619      0.00243       0.0167       0.0264       0.0399       0.0669
     66   190       0.0103      0.00848      0.00185       0.0238       0.0308       0.0619       0.0758
     66   200       0.0055       0.0037       0.0018        0.012       0.0204       0.0944        0.113
     66   210       0.0243       0.0192      0.00513       0.0337       0.0464        0.134        0.159
     66   220        0.466       0.0519        0.414       0.0377       0.0763        0.924         1.72
     66   230       0.0416       0.0377      0.00392       0.0488        0.065       0.0702       0.0838
     66   231      0.00755      6.6e-05      0.00748      0.00226      0.00272        0.232        0.232

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     66     7        0.164         0.15       0.0132       0.0367         0.13        0.188        0.226


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              66 3415.324    0.005       0.0219        0.752        0.774       0.0241       0.0486        0.253         1.19
! Validation         66 3415.324    0.005       0.0274         6.15         6.18       0.0184       0.0438         0.62         3.47
Wall time: 3415.3249993054196

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     67    10       0.0528       0.0398        0.013       0.0458       0.0668        0.185         0.29
     67    20       0.0474       0.0362       0.0112       0.0455       0.0637        0.169        0.244
     67    30       0.0103      0.00558      0.00477       0.0174        0.025       0.0906       0.0948
     67    40        0.135       0.0178        0.117       0.0213       0.0447        0.677        0.714
     67    50        0.392       0.0235        0.369       0.0328       0.0513         1.03         1.57
     67    60       0.0535       0.0279       0.0257       0.0275       0.0559        0.207        0.215
     67    70       0.0159      0.00911       0.0068       0.0235        0.032       0.0819        0.111
     67    80       0.0146       0.0104      0.00422       0.0229       0.0342        0.143        0.174
     67    90      0.00834      0.00645      0.00188       0.0175       0.0269       0.0674        0.114
     67   100       0.0827      0.00786       0.0748       0.0221       0.0297        0.527        0.542
     67   110        0.064      0.00594       0.0581       0.0167       0.0258          0.4        0.448
     67   120       0.0547       0.0287       0.0261       0.0409       0.0567        0.305         0.38
     67   130        0.101       0.0888       0.0119       0.0686       0.0998        0.147        0.153
     67   140       0.0219      0.00899       0.0129       0.0215       0.0317        0.226        0.227
     67   150       0.0061      0.00106      0.00504      0.00741       0.0109        0.149        0.167
     67   160       0.0123      0.00737      0.00498       0.0195       0.0287       0.0833       0.0981
     67   170      0.00929      0.00641      0.00288        0.021       0.0268        0.121        0.135
     67   180       0.0071      0.00512      0.00198       0.0136        0.024        0.078        0.113
     67   190       0.0153       0.0107      0.00461       0.0197       0.0346        0.138        0.169
     67   200       0.0141       0.0127      0.00139       0.0271       0.0377       0.0467       0.0534
     67   210      0.00501      0.00377      0.00124       0.0147       0.0206        0.047       0.0515
     67   220       0.0151       0.0129      0.00213       0.0238       0.0381       0.0814        0.102
     67   230        0.015       0.0148     0.000281       0.0268       0.0407       0.0339       0.0411
     67   231      0.00436      0.00432     3.81e-05       0.0179        0.022       0.0165       0.0165

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     67     7        0.338        0.322       0.0166       0.0492         0.19        0.193        0.229


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              67 3466.949    0.005       0.0502        0.763        0.813       0.0296       0.0725        0.257          1.2
! Validation         67 3466.949    0.005       0.0523         6.15          6.2       0.0198       0.0576        0.617         3.47
Wall time: 3466.9499392872676

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     68    10       0.0106      0.00681      0.00383        0.015       0.0276       0.0935        0.097
     68    20      0.00859      0.00841      0.00018       0.0247       0.0307       0.0141       0.0182
     68    30      0.00274      0.00151      0.00123      0.00909        0.013       0.0807       0.0837
     68    40       0.0886       0.0331       0.0556       0.0399       0.0609        0.475        0.515
     68    50         1.01        0.951       0.0561        0.137        0.327        0.414        0.432
     68    60       0.0264       0.0045       0.0219       0.0103       0.0225        0.326        0.345
     68    70        0.022       0.0143      0.00763       0.0271       0.0401        0.132        0.141
     68    80      0.00793      0.00313       0.0048        0.015       0.0187       0.0639       0.0929
     68    90       0.0148      0.00846       0.0063       0.0195       0.0308       0.0983        0.108
     68   100       0.0122      0.00644      0.00572       0.0179       0.0269       0.0918        0.107
     68   110       0.0137       0.0124      0.00133       0.0266       0.0373       0.0763       0.0953
     68   120      0.00819      0.00793     0.000258       0.0184       0.0298       0.0234       0.0235
     68   130      0.00545      0.00451     0.000941       0.0136       0.0225        0.052       0.0565
     68   140       0.0175        0.017      0.00044       0.0334       0.0437       0.0407       0.0525
     68   150      0.00674      0.00616     0.000586       0.0175       0.0263       0.0505       0.0603
     68   160       0.0811      0.00504       0.0761       0.0165       0.0238        0.573        0.633
     68   170       0.0221        0.011       0.0111       0.0248        0.035        0.223         0.28
     68   180       0.0131      0.00602      0.00704       0.0188        0.026        0.164        0.225
     68   190       0.0184       0.0147      0.00366       0.0288       0.0406        0.103        0.145
     68   200       0.0134      0.00644      0.00696       0.0167       0.0269       0.0915        0.112
     68   210        0.012       0.0059      0.00612       0.0175       0.0257       0.0777        0.105
     68   220       0.0197       0.0156      0.00407       0.0297       0.0418        0.111        0.116
     68   230       0.0135       0.0111      0.00234       0.0261       0.0353       0.0489        0.065
     68   231      0.00123     0.000102      0.00113      0.00254      0.00338       0.0451       0.0451

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     68     7        0.132        0.119       0.0125       0.0325        0.116        0.203        0.231


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              68 3518.427    0.005       0.0158        0.761        0.777       0.0217       0.0423        0.231         1.19
! Validation         68 3518.427    0.005        0.023         6.14         6.17       0.0178       0.0408        0.638         3.46
Wall time: 3518.42820207309

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     69    10        0.294       0.0272        0.267       0.0337       0.0552        0.767         1.38
     69    20       0.0212       0.0198      0.00138       0.0335       0.0471       0.0408       0.0539
     69    30        0.145        0.114       0.0313       0.0834        0.113        0.276        0.474
     69    40       0.0166      0.00902      0.00757        0.022       0.0318        0.106        0.124
     69    50       0.0273       0.0242      0.00316       0.0367       0.0521        0.123        0.138
     69    60       0.0277       0.0266       0.0011       0.0417       0.0546       0.0451       0.0555
     69    70        0.052       0.0435      0.00844       0.0455       0.0699        0.168        0.174
     69    80       0.0312       0.0214      0.00979       0.0388        0.049        0.261        0.265
     69    90       0.0556       0.0491      0.00655       0.0534       0.0742        0.125          0.2
     69   100        0.052       0.0502      0.00178       0.0583        0.075        0.045       0.0566
     69   110       0.0182       0.0176     0.000592       0.0318       0.0444       0.0509         0.06
     69   120      0.00849      0.00714      0.00135       0.0147       0.0283       0.0318       0.0492
     69   130        0.028       0.0268      0.00114       0.0369       0.0548       0.0435       0.0478
     69   140      0.00254       0.0025     3.85e-05       0.0093       0.0167       0.0112       0.0135
     69   150       0.0236      0.00373       0.0199       0.0106       0.0205        0.231        0.376
     69   160        0.143      0.00605        0.137       0.0169        0.026        0.874        0.988
     69   170        0.188        0.035        0.153        0.044       0.0626        0.653        0.701
slurmstepd: error: *** JOB 10245747 ON gpu103 CANCELLED AT 2022-10-16T16:31:25 DUE TO TIME LIMIT ***
