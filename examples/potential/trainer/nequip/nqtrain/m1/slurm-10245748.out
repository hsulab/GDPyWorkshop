Torch device: cuda
Processing dataset...
Loaded data: Batch(atomic_numbers=[6144, 1], batch=[6144], cell=[1024, 3, 3], edge_cell_shift=[416666, 3], edge_index=[2, 416666], forces=[6144, 3], pbc=[1024, 3], pos=[6144, 3], ptr=[1025], total_energy=[1024, 1])
Cached processed data to disk
Done!
Successfully loaded the data set of type ASEDataset(1024)...
Replace string dataset_forces_rms to 0.3319755792617798
Replace string dataset_per_atom_total_energy_mean to -3.559055805206299
Atomic outputs are scaled by: [Cu: 0.331976], shifted by [Cu: -3.559056].
Replace string dataset_forces_rms to 0.3319755792617798
Initially outputs are globally scaled by: 0.3319755792617798, total_energy are globally shifted by None.
Successfully built the network...
Number of weights: 333368
! Starting training ...

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      0     7         1.46         1.05        0.406        0.224        0.341        0.996         1.43


  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Initial Validation          0    6.442    0.005         1.03         3.49         4.52        0.177        0.327         1.04         2.77
Wall time: 6.4427950363606215
! Best model        0    4.520

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      1    10        0.418        0.296        0.122        0.123        0.181        0.641        0.692
      1    20        0.404        0.321       0.0832        0.116        0.188        0.541        0.751
      1    30        0.272       0.0559        0.216       0.0534       0.0785        0.486        0.666
      1    40        0.117       0.0704       0.0465       0.0544       0.0881        0.234        0.286
      1    50        0.151       0.0154        0.135       0.0246       0.0412        0.579        0.609
      1    60       0.0519       0.0153       0.0365       0.0247       0.0411        0.309        0.324
      1    70       0.0825       0.0783      0.00426       0.0537       0.0929        0.103        0.126
      1    80        0.133       0.0753        0.058       0.0704       0.0911        0.359        0.369
      1    90        0.249         0.16       0.0883       0.0659        0.133        0.546        0.586
      1   100        0.114       0.0975        0.017       0.0771        0.104        0.284         0.32
      1   110          123       0.0599          123       0.0469       0.0813         7.44         14.7
      1   120        0.293        0.273       0.0202        0.108        0.173        0.246        0.258
      1   130        0.248        0.139        0.109       0.0907        0.124        0.533        0.761
      1   140        0.291        0.123        0.168       0.0599        0.116        0.678        0.789
      1   150        0.396         0.29        0.105        0.111        0.179        0.358        0.431
      1   160       0.0228       0.0138      0.00897       0.0226        0.039        0.127        0.141
      1   170       0.0429        0.015       0.0279       0.0229       0.0407        0.249        0.252
      1   180       0.0712       0.0688      0.00237        0.069       0.0871        0.107        0.115
      1   190       0.0979       0.0784       0.0195       0.0496       0.0929         0.28          0.3
      1   200       0.0677       0.0667      0.00104         0.06       0.0857       0.0619       0.0776
      1   210       0.0836       0.0814      0.00215       0.0732       0.0947       0.0846        0.103
      1   220        0.675       0.0154         0.66       0.0195       0.0412         1.07         1.08
      1   230         0.35         0.18        0.169       0.0951        0.141        0.845         1.03
      1   231        0.479     0.000459        0.479       0.0059      0.00711         1.84         1.84

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      1     7        0.154       0.0803       0.0741        0.063       0.0941        0.469        0.549


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               1   65.545    0.005        0.221         1.27          1.5       0.0755        0.155        0.562         1.61
! Validation          1   65.545    0.005       0.0831         3.11          3.2       0.0517       0.0925        0.785         2.49
Wall time: 65.54630151484162
! Best model        1    3.197

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      2    10       0.0303       0.0197       0.0106       0.0294       0.0466        0.212        0.227
      2    20        0.241        0.172       0.0693        0.104        0.138        0.394        0.568
      2    30       0.0581        0.027       0.0311       0.0276       0.0546        0.418        0.464
      2    40       0.0472       0.0433      0.00393        0.045       0.0691        0.106        0.153
      2    50        0.054       0.0449      0.00909       0.0492       0.0703        0.162        0.184
      2    60       0.0705       0.0642      0.00631       0.0496       0.0841       0.0966        0.128
      2    70        0.127       0.0936       0.0338       0.0686        0.102        0.364        0.365
      2    80       0.0141      0.00849      0.00558       0.0166       0.0306        0.132        0.147
      2    90       0.0364       0.0335      0.00293       0.0364       0.0608       0.0736       0.0878
      2   100        0.126        0.115       0.0118       0.0809        0.112        0.167        0.168
      2   110        0.815        0.399        0.416        0.112         0.21        0.848        0.857
      2   120        0.317        0.253       0.0641        0.109        0.167        0.585         0.64
      2   130        0.603        0.506       0.0972        0.113        0.236        0.536         0.75
      2   140        0.989         0.97       0.0194        0.191        0.327        0.265        0.319
      2   150        0.475        0.133        0.342       0.0765        0.121         1.54         1.55
      2   160        0.288       0.0188         0.27       0.0233       0.0455        0.998         1.06
      2   170       0.0896       0.0191       0.0705       0.0258       0.0459         0.48        0.566
      2   180        0.877        0.752        0.125        0.157        0.288        0.803         0.94
      2   190        0.134       0.0702       0.0635       0.0552       0.0879        0.346        0.353
      2   200       0.0711       0.0597       0.0113       0.0488       0.0811        0.202        0.209
      2   210       0.0122      0.00681      0.00543       0.0176       0.0274        0.152        0.178
      2   220        0.052       0.0434      0.00854       0.0371       0.0692        0.137        0.146
      2   230       0.0889       0.0819      0.00701       0.0673        0.095        0.141        0.145
      2   231       0.0842       0.0823      0.00186       0.0829       0.0952       0.0572       0.0572

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      2     7        0.127       0.0981       0.0291       0.0687        0.104        0.278         0.41


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               2  116.470    0.005        0.137         1.21         1.34       0.0628        0.123        0.518         1.55
! Validation          2  116.470    0.005       0.0823         3.12          3.2       0.0539       0.0915        0.489         2.46
Wall time: 116.47165475413203

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      3    10       0.0259        0.025      0.00092       0.0311       0.0524       0.0642       0.0806
      3    20        0.548       0.0272        0.521       0.0304       0.0547         1.38         1.43
      3    30        0.026      0.00587       0.0201       0.0183       0.0254        0.177        0.199
      3    40      0.00733      0.00159      0.00573      0.00711       0.0133        0.147        0.168
      3    50        0.112       0.0599       0.0525       0.0555       0.0813        0.326        0.342
      3    60       0.0429       0.0307       0.0122       0.0399       0.0582        0.172        0.287
      3    70        0.123       0.0851       0.0384       0.0565       0.0968        0.377        0.465
      3    80       0.0403      0.00952       0.0308       0.0192       0.0324        0.337        0.349
      3    90        0.193        0.031        0.162       0.0409       0.0584        0.731        0.792
      3   100        0.042       0.0283       0.0138       0.0367       0.0558        0.153        0.175
      3   110        0.211        0.182       0.0297       0.0643        0.142        0.126        0.229
      3   120       0.0311       0.0153       0.0157       0.0293       0.0411        0.263        0.283
      3   130       0.0537       0.0291       0.0246       0.0306       0.0567        0.297        0.337
      3   140       0.0994       0.0719       0.0275       0.0463        0.089        0.261        0.342
      3   150       0.0495       0.0366       0.0128       0.0365       0.0635        0.215        0.298
      3   160       0.0292       0.0224      0.00673       0.0276       0.0497        0.173        0.183
      3   170       0.0951       0.0858      0.00931       0.0707       0.0973        0.136        0.156
      3   180        0.034       0.0314      0.00261       0.0457       0.0588       0.0841       0.0862
      3   190        0.016        0.013      0.00301       0.0201       0.0378        0.111        0.126
      3   200        0.636       0.0314        0.604       0.0273       0.0588         2.03         2.06
      3   210        0.202         0.15       0.0521       0.0856        0.128        0.239        0.303
      3   220         0.12       0.0827       0.0371       0.0647       0.0954        0.436        0.511
      3   230        0.096       0.0221        0.074       0.0218       0.0493        0.458        0.504
      3   231        0.074     0.000409       0.0735      0.00578      0.00671         0.36         0.36

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      3     7       0.0876        0.064       0.0236        0.058        0.084        0.252        0.342


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               3  167.277    0.005       0.0625         1.22         1.28        0.046       0.0832        0.469         1.54
! Validation          3  167.277    0.005       0.0489         3.11         3.16       0.0407       0.0708        0.531         2.46
Wall time: 167.27855306956917
! Best model        3    3.160

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      4    10        0.165       0.0465        0.119       0.0397       0.0716        0.555        0.744
      4    20        0.118       0.0953       0.0222       0.0644        0.103        0.294        0.375
      4    30        0.175       0.0755       0.0995       0.0573       0.0912        0.621        0.721
      4    40        0.173       0.0464        0.126       0.0566       0.0715        0.508        0.625
      4    50       0.0559       0.0413       0.0147       0.0463       0.0675        0.239        0.321
      4    60         0.19       0.0133        0.177       0.0222       0.0382        0.943        0.964
      4    70       0.0648       0.0363       0.0285       0.0407       0.0632         0.37        0.449
      4    80       0.0157      0.00897      0.00678       0.0181       0.0314        0.178        0.216
      4    90       0.0249       0.0123       0.0127       0.0269       0.0368        0.144        0.163
      4   100       0.0384       0.0189       0.0195       0.0329       0.0457        0.161        0.185
      4   110        0.015      0.00896      0.00605       0.0169       0.0314        0.119        0.164
      4   120       0.0166       0.0146      0.00191       0.0249       0.0402        0.101        0.116
      4   130        0.081        0.071         0.01       0.0695       0.0884        0.114        0.135
      4   140       0.0474       0.0468     0.000631        0.051       0.0718       0.0449       0.0544
      4   150       0.0773        0.062       0.0153       0.0615       0.0826        0.242        0.256
      4   160        0.008      0.00625      0.00175       0.0144       0.0262       0.0418       0.0558
      4   170       0.0619       0.0613     0.000585       0.0397       0.0822       0.0456       0.0628
      4   180       0.0071     0.000353      0.00675      0.00413      0.00624        0.108        0.109
      4   190       0.0308       0.0297      0.00102       0.0438       0.0573       0.0728        0.085
      4   200       0.0136       0.0125      0.00115       0.0285       0.0371       0.0518       0.0553
      4   210        0.345       0.0816        0.263       0.0702       0.0948         1.15         1.17
      4   220        0.163       0.0589        0.104       0.0484       0.0806        0.606        0.637
      4   230        0.104       0.0715       0.0321        0.061       0.0888        0.312        0.471
      4   231       0.0193       0.0191     0.000221       0.0275       0.0459       0.0198       0.0198

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      4     7       0.0897       0.0515       0.0382       0.0521       0.0753        0.327        0.398


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               4  218.183    0.005       0.0474          1.2         1.24       0.0407       0.0721        0.406         1.51
! Validation          4  218.183    0.005       0.0406         3.11         3.15       0.0377        0.065        0.647         2.47
Wall time: 218.18371240142733
! Best model        4    3.148

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      5    10        0.681        0.108        0.573       0.0683        0.109         1.54         1.67
      5    20        0.048       0.0189       0.0291       0.0196       0.0457        0.274        0.306
      5    30        0.134         0.12       0.0133       0.0564        0.115          0.2         0.22
      5    40        0.258       0.0362        0.222       0.0434       0.0631        0.543        0.639
      5    50        0.283       0.0214        0.261       0.0352       0.0486        0.505        0.718
      5    60       0.0431       0.0289       0.0143         0.03       0.0564        0.213        0.259
      5    70        0.204        0.168       0.0359       0.0646        0.136        0.218        0.275
      5    80       0.0821       0.0479       0.0342       0.0567       0.0726        0.243        0.268
      5    90       0.0568       0.0462       0.0107       0.0425       0.0713        0.159        0.206
      5   100       0.0172       0.0109      0.00627       0.0216       0.0347        0.115        0.152
      5   110        0.086       0.0822       0.0038       0.0598       0.0952        0.116        0.141
      5   120       0.0407       0.0326      0.00806       0.0475         0.06       0.0951        0.119
      5   130       0.0395       0.0367      0.00287       0.0365       0.0636        0.104        0.142
      5   140       0.0132      0.00656      0.00667        0.013       0.0269        0.104        0.108
      5   150       0.0335       0.0317      0.00183        0.039       0.0591       0.0559       0.0653
      5   160         0.33       0.0175        0.312       0.0314       0.0439        0.689        0.747
      5   170         0.27       0.0805         0.19       0.0635       0.0942        0.808        0.896
      5   180       0.0391       0.0315      0.00762       0.0211       0.0589        0.155        0.162
      5   190       0.0374       0.0236       0.0138       0.0249        0.051        0.255        0.291
      5   200        0.111       0.0774       0.0339       0.0542       0.0924        0.295        0.341
      5   210          0.1       0.0964      0.00407       0.0873        0.103        0.134        0.166
      5   220        0.114       0.0959       0.0182       0.0673        0.103        0.288         0.33
      5   230       0.0318        0.023      0.00886       0.0319       0.0503        0.102        0.131
      5   231        0.412        0.226        0.187        0.116        0.158        0.574        0.574

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      5     7       0.0761       0.0549       0.0212        0.054       0.0778        0.239         0.32


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               5  269.030    0.005        0.054         1.21         1.26       0.0427       0.0764        0.443         1.53
! Validation          5  269.030    0.005       0.0397         3.11         3.15       0.0373       0.0642        0.534         2.46
Wall time: 269.030647800304

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      6    10       0.0285      0.00452        0.024      0.00919       0.0223        0.309        0.344
      6    20       0.0438       0.0428      0.00102       0.0511       0.0687       0.0499       0.0762
      6    30       0.0346        0.028      0.00653       0.0355       0.0556        0.143        0.212
      6    40       0.0159       0.0123      0.00353        0.025       0.0369        0.103        0.133
      6    50        0.146        0.017        0.129       0.0321       0.0433        0.596        0.652
      6    60        0.283       0.0408        0.243       0.0441       0.0671         0.89        0.897
      6    70          1.5        0.305          1.2        0.107        0.183        0.873         1.46
      6    80        0.184         0.17       0.0147          0.1        0.137         0.26        0.296
      6    90        0.014      0.00728      0.00671       0.0201       0.0283        0.121        0.125
      6   100       0.0383       0.0372      0.00115       0.0443        0.064       0.0547       0.0587
      6   110       0.0577       0.0519       0.0058       0.0605       0.0756        0.166        0.202
      6   120        0.197       0.0559        0.141       0.0564       0.0785        0.773        0.816
      6   130        0.079       0.0186       0.0604       0.0215       0.0452        0.411        0.469
      6   140        0.031       0.0216      0.00945       0.0277       0.0488        0.225        0.234
      6   150        0.104        0.025       0.0792       0.0369       0.0525        0.293         0.39
      6   160       0.0204       0.0121      0.00824       0.0233       0.0366        0.215        0.241
      6   170       0.0607       0.0554      0.00532       0.0501       0.0781        0.115        0.116
      6   180       0.0322       0.0288      0.00339        0.045       0.0564       0.0949        0.109
      6   190        0.142       0.0317         0.11       0.0397       0.0591        0.253        0.441
      6   200       0.0448        0.037      0.00775        0.042       0.0639        0.178        0.193
      6   210        0.397      0.00497        0.392       0.0127       0.0234         1.05         1.14
      6   220        0.135       0.0602       0.0749       0.0538       0.0815        0.474        0.543
      6   230        0.141       0.0324        0.108       0.0274       0.0598        0.314        0.448
      6   231        0.221       0.0985        0.123       0.0836        0.104        0.931        0.931

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      6     7       0.0641       0.0442       0.0198       0.0482       0.0698        0.236        0.303


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               6  319.847    0.005       0.0458         1.19         1.24       0.0402       0.0714        0.411          1.5
! Validation          6  319.847    0.005       0.0341         3.11         3.15       0.0352       0.0598        0.534         2.46
Wall time: 319.84866611100733
! Best model        6    3.146

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      7    10        0.126       0.0506       0.0751       0.0597       0.0747        0.356        0.476
      7    20        0.193       0.0159        0.177       0.0255       0.0418        0.807        0.864
      7    30        0.159       0.0348        0.124       0.0373       0.0619        0.748        0.803
      7    40       0.0192     0.000674       0.0185      0.00495      0.00862        0.266        0.278
      7    50       0.0334        0.028      0.00546       0.0313       0.0555        0.174        0.188
      7    60        0.279       0.0572        0.222       0.0518       0.0794        0.888        0.955
      7    70        0.226       0.0501        0.176       0.0501       0.0743        0.894        0.949
      7    80        0.111       0.0337       0.0772       0.0429       0.0609        0.552        0.598
      7    90       0.0252       0.0185      0.00668       0.0315       0.0452         0.14        0.203
      7   100       0.0635       0.0547      0.00878       0.0542       0.0777        0.198        0.201
      7   110       0.0168      0.00339       0.0134      0.00901       0.0193        0.181        0.217
      7   120       0.0666       0.0607      0.00592       0.0558       0.0818        0.115        0.123
      7   130       0.0294        0.024      0.00533       0.0261       0.0515        0.149         0.19
      7   140       0.0161       0.0145      0.00167       0.0285       0.0399       0.0462       0.0543
      7   150       0.0596       0.0539      0.00564       0.0459       0.0771        0.149        0.194
      7   160        0.113       0.0898       0.0233       0.0791       0.0995        0.282        0.396
      7   170       0.0231       0.0175      0.00555        0.027       0.0439        0.133        0.143
      7   180       0.0226       0.0168      0.00586       0.0266        0.043        0.139        0.153
      7   190      0.00673        0.006     0.000731       0.0168       0.0257       0.0415       0.0441
      7   200       0.0698       0.0687      0.00106       0.0402        0.087       0.0621       0.0692
      7   210       0.0597       0.0512      0.00849       0.0437       0.0752        0.128        0.141
      7   220       0.0146       0.0127      0.00189       0.0238       0.0374         0.09        0.113
      7   230       0.0136       0.0107       0.0029       0.0222       0.0343       0.0966        0.112
      7   231       0.0887       0.0818      0.00696       0.0807       0.0949        0.222        0.222

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      7     7       0.0598       0.0425       0.0173       0.0449       0.0684        0.215        0.273


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               7  370.685    0.005       0.0443         1.17         1.21        0.039         0.07        0.357         1.48
! Validation          7  370.685    0.005       0.0334         3.11         3.15       0.0346       0.0591        0.527         2.46
Wall time: 370.68573635444045

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      8    10       0.0109       0.0085      0.00235       0.0168       0.0306        0.115        0.125
      8    20       0.0145       0.0141     0.000419       0.0195       0.0394       0.0156       0.0274
      8    30       0.0209       0.0203     0.000577       0.0331       0.0473       0.0604       0.0638
      8    40      0.00195     0.000173      0.00178      0.00336      0.00437       0.0539        0.056
      8    50         0.19        0.171       0.0184       0.0849        0.137        0.171        0.209
      8    60      0.00317     0.000148      0.00303      0.00318      0.00405       0.0695       0.0897
      8    70       0.0351       0.0196       0.0155       0.0257       0.0464        0.187        0.192
      8    80        0.439       0.0619        0.377       0.0572       0.0826         1.21         1.38
      8    90        0.669      0.00823        0.661        0.018       0.0301         1.64          1.8
      8   100        0.161        0.125       0.0367       0.0808        0.117        0.394        0.508
      8   110        0.345        0.144        0.201       0.0728        0.126         0.39        0.595
      8   120        0.103       0.0391       0.0636        0.036       0.0656        0.352        0.399
      8   130        0.248       0.0684        0.179       0.0577       0.0868        0.614        0.615
      8   140       0.0981       0.0472       0.0509       0.0498       0.0721        0.379        0.431
      8   150       0.0624       0.0417       0.0206       0.0413       0.0678        0.212        0.221
      8   160       0.0358        0.028      0.00774       0.0355       0.0556        0.175        0.188
      8   170       0.0553       0.0423        0.013       0.0415       0.0683        0.269        0.303
      8   180       0.0127      0.00651      0.00619       0.0131       0.0268       0.0948        0.104
      8   190       0.0228       0.0174      0.00544       0.0248       0.0437        0.101        0.136
      8   200       0.0226       0.0151      0.00754       0.0247       0.0407        0.163        0.195
      8   210      0.00592      0.00266      0.00326      0.00899       0.0171       0.0724       0.0758
      8   220       0.0293       0.0269      0.00244       0.0266       0.0544       0.0898        0.126
      8   230       0.0258        0.022      0.00377       0.0327       0.0492       0.0974        0.116
      8   231      0.00204     0.000826      0.00121      0.00735      0.00954       0.0924       0.0924

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      8     7       0.0776       0.0482       0.0294       0.0495       0.0729        0.283        0.353


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               8  421.460    0.005       0.0851         1.18         1.27       0.0453       0.0956        0.409         1.51
! Validation          8  421.460    0.005        0.043         3.11         3.15       0.0376       0.0663        0.599         2.46
Wall time: 421.4614106770605

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      9    10       0.0382       0.0308      0.00746       0.0289       0.0582          0.1        0.115
      9    20        0.025       0.0249     9.78e-05       0.0362       0.0523       0.0107       0.0139
      9    30        0.316       0.0534        0.263       0.0496       0.0767        0.705         1.36
      9    40       0.0319       0.0224      0.00948       0.0288       0.0497        0.235        0.259
      9    50        0.067       0.0646      0.00244       0.0588       0.0844       0.0896        0.121
      9    60       0.0194       0.0185     0.000981       0.0261       0.0451       0.0499       0.0644
      9    70       0.0442       0.0412      0.00294       0.0391       0.0674       0.0976        0.105
      9    80        0.087       0.0285       0.0585       0.0416       0.0561        0.199        0.322
      9    90         1.08         0.59        0.488        0.115        0.255         1.83         1.86
      9   100       0.0926       0.0848      0.00777       0.0671       0.0967        0.171        0.191
      9   110         85.5      0.00805         85.5       0.0125       0.0298         6.26         12.3
      9   120       0.0861       0.0129       0.0732       0.0271       0.0377        0.506        0.605
      9   130        0.142       0.0396        0.102       0.0385       0.0661        0.792        0.849
      9   140       0.0505       0.0221       0.0284       0.0295       0.0494        0.344        0.447
      9   150       0.0808       0.0639       0.0169       0.0559       0.0839        0.265        0.301
      9   160      0.00922      0.00325      0.00596       0.0104       0.0189        0.101        0.105
      9   170       0.0151      0.00735      0.00778       0.0198       0.0285        0.135        0.161
      9   180       0.0422       0.0395      0.00269       0.0419        0.066       0.0902        0.125
      9   190       0.0117       0.0117     3.43e-05       0.0234       0.0359       0.0087       0.0155
      9   200         54.9       0.0401         54.9       0.0439       0.0665         5.86         9.95
      9   210        0.133       0.0332       0.0995       0.0448       0.0605        0.636        0.684
      9   220       0.0917        0.032       0.0597       0.0302       0.0594        0.385        0.441
      9   230       0.0769       0.0659        0.011       0.0578       0.0852        0.126        0.145
      9   231       0.0481       0.0442      0.00393       0.0497       0.0698       0.0833       0.0833

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
      9     7       0.0645       0.0424       0.0221        0.049       0.0683        0.241        0.314


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train               9  472.218    0.005       0.0478         1.19         1.24       0.0399       0.0728        0.374         1.49
! Validation          9  472.218    0.005       0.0334         3.11         3.14       0.0344       0.0589        0.554         2.46
Wall time: 472.2195578664541
! Best model        9    3.142

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     10    10       0.0578       0.0402       0.0176       0.0246       0.0665        0.224        0.251
     10    20          121       0.0535          121       0.0483       0.0768          7.5         14.6
     10    30        0.361       0.0281        0.333       0.0333       0.0557         1.18         1.28
     10    40         0.19       0.0293        0.161       0.0391       0.0568        0.519        0.562
     10    50        0.126       0.0153        0.111       0.0301       0.0411        0.487        0.609
     10    60       0.0614       0.0214         0.04       0.0299       0.0486        0.393        0.441
     10    70        0.135       0.0874       0.0473       0.0572       0.0982        0.407        0.491
     10    80       0.0628       0.0363       0.0265       0.0414       0.0633        0.324        0.337
     10    90       0.0149        0.014     0.000833       0.0301       0.0393       0.0556       0.0737
     10   100        0.078       0.0489        0.029       0.0531       0.0734        0.253        0.305
     10   110        0.133       0.0977       0.0349       0.0709        0.104        0.382        0.453
     10   120       0.0964       0.0587       0.0377       0.0512       0.0804        0.295        0.513
     10   130       0.0715       0.0666      0.00494       0.0568       0.0857        0.121        0.143
     10   140       0.0175       0.0144      0.00311       0.0203       0.0399       0.0944        0.144
     10   150       0.0167       0.0163     0.000412       0.0303       0.0424       0.0308       0.0313
     10   160        0.047       0.0454      0.00166       0.0414       0.0707       0.0555       0.0571
     10   170        0.133        0.122       0.0106       0.0643        0.116        0.203         0.27
     10   180      0.00989      0.00296      0.00693       0.0101       0.0181        0.164        0.178
     10   190       0.0126      0.00336      0.00922       0.0117       0.0193        0.111        0.127
     10   200      0.00445      0.00282      0.00163       0.0109       0.0176       0.0504       0.0683
     10   210       0.0217        0.016       0.0057        0.029        0.042         0.12          0.2
     10   220       0.0472       0.0329       0.0143       0.0465       0.0602        0.115        0.164
     10   230       0.0116       0.0112     0.000337       0.0234       0.0352       0.0242       0.0244
     10   231       0.0028     0.000133      0.00266      0.00255      0.00383       0.0685       0.0685

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     10     7        0.046       0.0373      0.00863       0.0467       0.0642        0.139          0.2


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              10  522.959    0.005        0.043         1.16          1.2       0.0374       0.0674         0.36         1.47
! Validation         10  522.959    0.005       0.0274         3.12         3.15       0.0319       0.0536        0.436         2.45
Wall time: 522.9602011311799

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     11    10        0.545       0.0212        0.524        0.033       0.0484         1.36         1.41
     11    20        0.131        0.117       0.0138       0.0535        0.114        0.222         0.24
     11    30        0.712        0.676       0.0359        0.149        0.273        0.223        0.252
     11    40        0.481        0.324        0.157        0.122        0.189        0.785        0.987
     11    50       0.0418       0.0241       0.0177       0.0309       0.0515        0.203        0.225
     11    60      0.00728      0.00523      0.00205      0.00999        0.024       0.0563       0.0762
     11    70       0.0443       0.0409      0.00342       0.0538       0.0671        0.123        0.128
     11    80      0.00512     0.000598      0.00452      0.00558      0.00812        0.128        0.141
     11    90       0.0289       0.0222      0.00665       0.0325       0.0495        0.138        0.163
     11   100       0.0144       0.0125       0.0019       0.0241       0.0371       0.0846        0.115
     11   110      0.00584      0.00396      0.00189       0.0127       0.0209       0.0737       0.0913
     11   120      0.00666      0.00201      0.00466      0.00684       0.0149        0.137        0.172
     11   130       0.0317       0.0266       0.0051       0.0386       0.0541       0.0902        0.107
     11   140       0.0164      0.00849      0.00791       0.0134       0.0306        0.166        0.175
     11   150       0.0313       0.0294       0.0019       0.0373       0.0569       0.0866        0.115
     11   160        0.079       0.0768       0.0021        0.065        0.092       0.0576       0.0658
     11   170        0.058       0.0236       0.0344       0.0319        0.051        0.193        0.265
     11   180        0.118       0.0252       0.0923       0.0301       0.0527        0.277        0.411
     11   190        0.467       0.0456        0.421       0.0529       0.0709        0.943         1.72
     11   200       0.0243       0.0194      0.00488       0.0322       0.0463        0.176        0.186
     11   210        0.282       0.0162        0.266       0.0214       0.0423        0.861        0.919
     11   220        0.226       0.0089        0.217       0.0195       0.0313        0.925         1.01
     11   230        0.125        0.069       0.0565       0.0483       0.0872        0.492        0.594
     11   231       0.0527       0.0527     6.15e-06       0.0564       0.0762      0.00329      0.00329

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     11     7       0.0735        0.044       0.0295       0.0498       0.0696        0.279        0.346


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              11  573.720    0.005       0.0588         1.19         1.25       0.0424       0.0802        0.374         1.51
! Validation         11  573.720    0.005       0.0359         3.11         3.15        0.034       0.0606        0.597         2.46
Wall time: 573.7216700660065

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     12    10        0.131       0.0728       0.0578       0.0661       0.0896        0.405        0.608
     12    20       0.0212       0.0189      0.00233       0.0302       0.0456       0.0479       0.0642
     12    30       0.0684       0.0401       0.0283       0.0417       0.0665        0.142        0.227
     12    40       0.0297      0.00978       0.0199       0.0185       0.0328        0.224        0.231
     12    50       0.0249      0.00794       0.0169       0.0132       0.0296        0.239        0.266
     12    60        0.585        0.025         0.56       0.0325       0.0525        0.991        0.993
     12    70        0.279        0.106        0.173       0.0476        0.108        0.639        0.679
     12    80        0.244        0.192       0.0518        0.108        0.145        0.436        0.572
     12    90       0.0435       0.0197       0.0237       0.0259       0.0466        0.362        0.409
     12   100        0.037       0.0155       0.0214       0.0177       0.0414        0.233        0.282
     12   110       0.0369       0.0102       0.0267       0.0196       0.0335         0.38        0.432
     12   120        0.021       0.0187       0.0023       0.0313       0.0454       0.0599       0.0717
     12   130      0.00176     0.000832     0.000927      0.00641      0.00958       0.0455       0.0488
     12   140       0.0211         0.02      0.00112       0.0357        0.047       0.0319       0.0444
     12   150        0.103        0.072       0.0309       0.0609       0.0891        0.298        0.417
     12   160       0.0183       0.0158      0.00247       0.0251       0.0417        0.111        0.129
     12   170       0.0339       0.0297      0.00418        0.043       0.0572       0.0859       0.0972
     12   180       0.0484       0.0401      0.00836       0.0485       0.0664        0.111         0.13
     12   190       0.0485       0.0142       0.0343       0.0161       0.0395        0.308        0.351
     12   200        0.188       0.0342        0.154       0.0379       0.0614        0.715        0.751
     12   210        0.104        0.022       0.0817       0.0295       0.0492        0.482        0.542
     12   220       0.0345        0.016       0.0185       0.0215        0.042        0.172        0.187
     12   230       0.0708       0.0632      0.00756       0.0462       0.0835        0.166        0.191
     12   231       0.0146     0.000156       0.0144      0.00354      0.00415         0.16         0.16

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     12     7       0.0949       0.0458        0.049       0.0512       0.0711        0.352        0.448


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              12  624.492    0.005       0.0434         1.19         1.23       0.0387       0.0694        0.412         1.51
! Validation         12  624.492    0.005       0.0319         3.11         3.14       0.0335       0.0572        0.696         2.47
Wall time: 624.4935469571501
! Best model       12    3.139

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     13    10       0.0124      0.00518      0.00724       0.0149       0.0239        0.218        0.226
     13    20       0.0235        0.018      0.00546       0.0293       0.0446        0.079       0.0981
     13    30         1.25       0.0173         1.23       0.0257       0.0436        0.882         1.48
     13    40       0.0464       0.0375      0.00884       0.0395       0.0643        0.132        0.144
     13    50       0.0811       0.0756      0.00556       0.0549       0.0913        0.105         0.11
     13    60       0.0329       0.0285      0.00438       0.0251       0.0561        0.157        0.176
     13    70       0.0935        0.039       0.0545       0.0417       0.0656        0.204        0.313
     13    80       0.0518       0.0494      0.00246       0.0547       0.0738       0.0658       0.0685
     13    90       0.0551       0.0437       0.0114       0.0478       0.0694        0.198        0.207
     13   100       0.0416       0.0402       0.0014       0.0387       0.0666       0.0761       0.0939
     13   110       0.0225        0.021      0.00144       0.0298       0.0482       0.0769       0.0976
     13   120        0.212       0.0823         0.13       0.0718       0.0952        0.709        0.817
     13   130         0.28       0.0254        0.255       0.0336       0.0529        0.753        0.757
     13   140        0.159       0.0128        0.146       0.0249       0.0376        0.658        0.673
     13   150       0.0407       0.0112       0.0296       0.0215       0.0351        0.313        0.362
     13   160       0.0156      0.00836      0.00728       0.0171       0.0304        0.131        0.178
     13   170       0.0614       0.0371       0.0243        0.046       0.0639        0.202        0.231
     13   180       0.0395       0.0262       0.0133       0.0371       0.0537        0.127        0.154
     13   190        0.016       0.0149      0.00102       0.0301       0.0406       0.0549       0.0665
     13   200        0.105       0.0899       0.0156       0.0745       0.0995         0.24        0.314
     13   210       0.0192       0.0161      0.00315       0.0256       0.0421          0.1        0.117
     13   220       0.0104      0.00457      0.00584       0.0124       0.0224        0.115        0.116
     13   230        0.258       0.0101        0.248       0.0222       0.0333        0.986         1.04
     13   231        0.308     4.01e-05        0.308      0.00163       0.0021         1.47         1.47

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     13     7       0.0477       0.0321       0.0156        0.043       0.0595        0.205         0.26


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              13  675.330    0.005       0.0472         1.16         1.21       0.0395        0.072        0.344         1.47
! Validation         13  675.330    0.005       0.0264         3.11         3.14       0.0305       0.0526        0.497         2.45
Wall time: 675.3312984611839

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     14    10        0.181        0.128       0.0538       0.0646        0.119        0.431        0.476
     14    20       0.0421       0.0144       0.0277       0.0235       0.0398        0.272          0.3
     14    30        0.461        0.263        0.198         0.11         0.17        0.873         1.13
     14    40          121        0.256          120        0.102        0.168         7.48         14.6
     14    50        0.327       0.0947        0.233       0.0589        0.102        0.629         0.64
     14    60        0.253        0.188       0.0653       0.0899        0.144        0.471        0.486
     14    70        0.229        0.137       0.0921        0.106        0.123        0.441         0.58
     14    80        0.103        0.089       0.0136       0.0773       0.0991        0.199        0.294
     14    90       0.0167       0.0138      0.00291       0.0256        0.039        0.104        0.117
     14   100       0.0721       0.0686      0.00348       0.0578        0.087        0.108        0.157
     14   110      0.00826      0.00436       0.0039        0.014       0.0219        0.159        0.166
     14   120        0.114       0.0396       0.0742       0.0393       0.0661        0.649        0.723
     14   130       0.0834       0.0434         0.04       0.0411       0.0692        0.514        0.531
     14   140       0.0332        0.019       0.0142       0.0238       0.0457        0.251        0.299
     14   150       0.0484       0.0362       0.0121       0.0323       0.0632        0.162        0.198
     14   160      0.00907      0.00593      0.00313      0.00935       0.0256        0.112        0.127
     14   170       0.0801       0.0526       0.0275       0.0591       0.0761        0.148        0.223
     14   180       0.0839       0.0451       0.0388       0.0525       0.0705        0.185        0.261
     14   190       0.0241       0.0162      0.00792       0.0251       0.0422        0.164        0.173
     14   200       0.0976       0.0568       0.0409       0.0594       0.0791        0.311        0.319
     14   210        0.182       0.0478        0.134       0.0567       0.0726         0.82        0.839
     14   220         0.11       0.0978       0.0122       0.0705        0.104        0.239        0.266
     14   230       0.0642       0.0516       0.0126       0.0553       0.0754        0.147        0.155
     14   231      0.00087     0.000713     0.000157      0.00705      0.00887       0.0333       0.0333

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     14     7       0.0666       0.0515       0.0151       0.0526       0.0754        0.201        0.263


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              14  726.043    0.005       0.0962         1.18         1.28       0.0492        0.103        0.407         1.49
! Validation         14  726.043    0.005       0.0437         3.12         3.16       0.0375       0.0671        0.491         2.46
Wall time: 726.0443229181692

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     15    10       0.0451       0.0391      0.00602       0.0427       0.0656        0.146        0.206
     15    20       0.0218        0.019      0.00284       0.0282       0.0457       0.0576       0.0713
     15    30        0.194        0.178       0.0161       0.0973         0.14        0.203         0.23
     15    40       0.0491       0.0451      0.00399       0.0455       0.0705        0.112        0.116
     15    50        0.013      0.00884      0.00414        0.013       0.0312       0.0823       0.0943
     15    60       0.0079      0.00486      0.00304       0.0108       0.0231       0.0903        0.113
     15    70       0.0231       0.0219      0.00124       0.0324       0.0491       0.0733       0.0843
     15    80       0.0096      0.00626      0.00334       0.0135       0.0263        0.103        0.113
     15    90       0.0302       0.0249      0.00526       0.0379       0.0524        0.171        0.185
     15   100       0.0362       0.0309      0.00531       0.0391       0.0583        0.112         0.12
     15   110       0.0128      0.00879      0.00404       0.0172       0.0311        0.141        0.168
     15   120       0.0191       0.0148      0.00425        0.026       0.0404        0.148        0.168
     15   130       0.0315       0.0299      0.00161       0.0411       0.0574        0.103        0.107
     15   140       0.0257       0.0241      0.00155       0.0313       0.0516       0.0468       0.0579
     15   150        0.392       0.0406        0.352       0.0499       0.0669         1.19         1.26
     15   160        0.124        0.076       0.0476       0.0565       0.0915        0.332         0.36
     15   170        0.052       0.0346       0.0174       0.0228       0.0617        0.259        0.279
     15   180       0.0563       0.0479      0.00841       0.0511       0.0726        0.198        0.228
     15   190       0.0808       0.0573       0.0235       0.0482       0.0795        0.289        0.369
     15   200        0.183        0.053         0.13       0.0466       0.0764        0.431        0.478
     15   210        0.114       0.0318       0.0827       0.0365       0.0592        0.636        0.739
     15   220          1.1       0.0221         1.08       0.0318       0.0493        0.925         1.38
     15   230       0.0788       0.0489         0.03       0.0432       0.0734        0.356        0.453
     15   231       0.0536     0.000167       0.0534      0.00381      0.00429        0.307        0.307

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     15     7       0.0683       0.0433        0.025       0.0499       0.0691        0.254        0.324


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              15  776.799    0.005       0.0694         1.18         1.25       0.0399       0.0847        0.345         1.48
! Validation         15  776.799    0.005       0.0301         3.11         3.14       0.0326       0.0556         0.57         2.46
Wall time: 776.8004973959178
! Best model       15    3.138

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     16    10        0.208      0.00113        0.207      0.00772       0.0112        0.752         1.21
     16    20       0.0592       0.0468       0.0124       0.0506       0.0718        0.192         0.24
     16    30       0.0664       0.0603      0.00612       0.0452       0.0815        0.139        0.153
     16    40       0.0459       0.0437       0.0022       0.0538       0.0694       0.0977        0.111
     16    50       0.0145      0.00847      0.00603       0.0118       0.0306        0.121        0.147
     16    60       0.0433       0.0405      0.00277       0.0503       0.0668       0.0628       0.0723
     16    70       0.0241       0.0225      0.00162       0.0345       0.0498       0.0681       0.0716
     16    80       0.0126      0.00787      0.00476       0.0202       0.0295       0.0981        0.102
     16    90       0.0358       0.0337      0.00209       0.0446        0.061       0.0808        0.119
     16   100       0.0304       0.0283      0.00209       0.0312       0.0558       0.0711       0.0855
     16   110       0.0136       0.0122      0.00133       0.0174       0.0367       0.0802       0.0948
     16   120       0.0219       0.0212     0.000669       0.0338       0.0483        0.043       0.0504
     16   130       0.0371       0.0323       0.0048       0.0413       0.0597        0.162        0.181
     16   140        0.177      0.00614        0.171       0.0122        0.026         0.81        0.847
     16   150       0.0525      0.00115       0.0514      0.00646       0.0112        0.578        0.602
     16   160        0.113       0.0489        0.064       0.0559       0.0734        0.219        0.336
     16   170        0.083       0.0611       0.0219       0.0534       0.0821        0.234        0.359
     16   180       0.0952       0.0168       0.0784       0.0301       0.0431        0.536        0.552
     16   190       0.0626      0.00729       0.0554       0.0115       0.0283        0.374         0.46
     16   200        0.062       0.0409       0.0211       0.0435       0.0672        0.263        0.316
     16   210        0.117        0.111      0.00625       0.0702        0.111        0.132        0.204
     16   220        0.183        0.146       0.0366       0.0829        0.127        0.323        0.336
     16   230       0.0925      0.00123       0.0913       0.0088       0.0116          0.4        0.401
     16   231       0.0758     0.000721       0.0751      0.00731      0.00891        0.364        0.364

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     16     7       0.0624       0.0425       0.0199       0.0486       0.0684        0.213        0.287


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              16  827.507    0.005       0.0483         1.17         1.22       0.0382       0.0716        0.335         1.46
! Validation         16  827.507    0.005       0.0296         3.11         3.14       0.0336       0.0557         0.53         2.45
Wall time: 827.5083138523623
! Best model       16    3.138

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     17    10        0.068       0.0614      0.00654       0.0627       0.0823        0.127        0.146
     17    20       0.0146      0.00513      0.00944       0.0136       0.0238        0.196        0.258
     17    30         0.47        0.444       0.0258        0.132        0.221         0.33        0.426
     17    40        0.281        0.271      0.00982        0.114        0.173       0.0925        0.132
     17    50        0.441        0.281         0.16         0.12        0.176        0.897        0.936
     17    60        0.119        0.112      0.00675       0.0806        0.111        0.174        0.216
     17    70        0.291        0.285      0.00651        0.105        0.177        0.159        0.179
     17    80      0.00975      0.00571      0.00404        0.016       0.0251       0.0934        0.106
     17    90       0.0861       0.0843      0.00177       0.0802       0.0964       0.0823       0.0981
     17   100        0.143        0.131       0.0121       0.0801         0.12        0.186        0.217
     17   110        0.036       0.0247       0.0113       0.0353       0.0522        0.212        0.282
     17   120       0.0388       0.0309      0.00792       0.0334       0.0583        0.111         0.12
     17   130       0.0161        0.012      0.00411       0.0227       0.0363        0.166         0.17
     17   140       0.0114      0.00867       0.0027       0.0201       0.0309       0.0778       0.0818
     17   150         1.38       0.0203         1.36       0.0319       0.0473        0.832         1.55
     17   160       0.0724       0.0702      0.00215       0.0596        0.088       0.0951        0.123
     17   170      0.00408      0.00254      0.00154       0.0112       0.0167       0.0633       0.0635
     17   180       0.0376         0.03      0.00761       0.0387       0.0575        0.167        0.223
     17   190       0.0417       0.0377      0.00401       0.0349       0.0644        0.158        0.168
     17   200       0.0252       0.0227       0.0025       0.0373         0.05        0.114        0.126
     17   210        0.133       0.0183        0.115       0.0292       0.0449        0.676        0.718
     17   220        0.221        0.082        0.139       0.0529        0.095        0.728        0.806
     17   230        0.112       0.0898       0.0221       0.0558       0.0995        0.306        0.393
     17   231         0.35        0.349      0.00136        0.143        0.196        0.049        0.049

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     17     7       0.0671       0.0507       0.0164       0.0511       0.0748        0.203        0.252


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              17  878.231    0.005        0.121         1.17         1.29       0.0525        0.113        0.329         1.47
! Validation         17  878.231    0.005       0.0513         3.12         3.17       0.0409        0.073        0.507         2.46
Wall time: 878.2319134706631

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     18    10        0.122       0.0236       0.0983       0.0283        0.051        0.634         0.68
     18    20       0.0366       0.0123       0.0243       0.0194       0.0368        0.294        0.355
     18    30         0.03       0.0189       0.0111       0.0236       0.0456        0.163        0.222
     18    40       0.0852       0.0256       0.0596       0.0368       0.0531        0.647        0.648
     18    50       0.0639      0.00969       0.0542       0.0235       0.0327        0.272        0.311
     18    60       0.0721        0.027       0.0451       0.0382       0.0545        0.288         0.31
     18    70       0.0841       0.0365       0.0476       0.0402       0.0634        0.264        0.299
     18    80       0.0156      0.00882      0.00677       0.0188       0.0312        0.117        0.122
     18    90          121       0.0669          121       0.0581       0.0858         7.35         14.6
     18   100       0.0871       0.0154       0.0717       0.0208       0.0412        0.441        0.495
     18   110        0.141        0.122       0.0185       0.0665        0.116         0.18        0.238
     18   120       0.0452        0.033       0.0122       0.0412       0.0603        0.141        0.159
     18   130       0.0226      0.00518       0.0174       0.0154       0.0239         0.29        0.294
     18   140        0.023       0.0172      0.00576       0.0275       0.0436        0.134        0.152
     18   150       0.0104      0.00739      0.00301       0.0217       0.0285       0.0633       0.0771
     18   160       0.0211       0.0191      0.00194       0.0304       0.0459       0.0464       0.0585
     18   170       0.0317       0.0213       0.0104       0.0318       0.0485        0.143        0.167
     18   180       0.0304        0.022      0.00833       0.0259       0.0493        0.171        0.182
     18   190         54.4       0.0171         54.4       0.0295       0.0434         4.98         9.79
     18   200       0.0755       0.0149       0.0605       0.0203       0.0405        0.485        0.564
     18   210        0.063       0.0426       0.0204       0.0428       0.0685        0.144         0.19
     18   220       0.0624       0.0238       0.0386       0.0296       0.0512        0.301        0.327
     18   230       0.0139      0.00795      0.00594       0.0212       0.0296        0.104         0.13
     18   231       0.0109     4.13e-05       0.0109      0.00192      0.00213        0.139        0.139

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     18     7       0.0465       0.0349       0.0116       0.0457        0.062        0.173        0.226


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              18  928.954    0.005        0.043         1.17         1.21       0.0374       0.0692        0.363         1.47
! Validation         18  928.954    0.005       0.0268         3.12         3.15       0.0317       0.0532        0.455         2.45
Wall time: 928.9555108267814

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     19    10        0.078       0.0446       0.0334        0.048       0.0701        0.271        0.292
     19    20       0.0899       0.0401       0.0498       0.0512       0.0664        0.364        0.371
     19    30       0.0283       0.0189      0.00936       0.0341       0.0457        0.195        0.225
     19    40       0.0453       0.0402      0.00504       0.0389       0.0666       0.0541       0.0949
     19    50       0.0335       0.0292      0.00426       0.0389       0.0568        0.131        0.163
     19    60      0.00536      0.00304      0.00231       0.0114       0.0183       0.0655       0.0726
     19    70        0.026       0.0209      0.00513       0.0297        0.048       0.0862       0.0951
     19    80       0.0117      0.00582       0.0059       0.0142       0.0253        0.135        0.163
     19    90       0.0149       0.0122      0.00271       0.0278       0.0366        0.117        0.122
     19   100       0.0102      0.00863      0.00162       0.0221       0.0308       0.0701       0.0846
     19   110       0.0642       0.0173       0.0469       0.0314       0.0437        0.435        0.463
     19   120        0.132       0.0237        0.108       0.0305       0.0511        0.499        0.605
     19   130        0.178       0.0621        0.116       0.0527       0.0827        0.742        0.835
     19   140       0.0695       0.0153       0.0543       0.0268        0.041        0.422        0.434
     19   150         0.26        0.236       0.0237       0.0995        0.161        0.179        0.206
     19   160       0.0337        0.028      0.00571       0.0332       0.0555       0.0844          0.1
     19   170       0.0194       0.0133      0.00603       0.0267       0.0383        0.116         0.13
     19   180       0.0177       0.0102      0.00751       0.0215       0.0335        0.102         0.12
     19   190       0.0316       0.0199       0.0117       0.0294       0.0469         0.16        0.166
     19   200      0.00431      0.00168      0.00262       0.0104       0.0136       0.0832       0.0851
     19   210       0.0595       0.0306        0.029       0.0373        0.058        0.287        0.451
     19   220       0.0204       0.0172      0.00323        0.022       0.0435        0.142        0.151
     19   230       0.0187        0.015      0.00372       0.0264       0.0406       0.0955       0.0963
     19   231       0.0121       0.0121     7.65e-07       0.0288       0.0366      0.00232      0.00232

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     19     7       0.0336       0.0259      0.00773       0.0397       0.0534        0.133        0.189


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              19  979.713    0.005       0.0297         1.16         1.19       0.0312       0.0566        0.334         1.46
! Validation         19  979.713    0.005       0.0218         3.13         3.15        0.029       0.0484        0.412         2.45
Wall time: 979.7145742792636

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     20    10       0.0247        0.022      0.00272       0.0313       0.0492       0.0708       0.0751
     20    20       0.0199       0.0179      0.00196       0.0207       0.0445       0.0698        0.118
     20    30       0.0727       0.0181       0.0547        0.025       0.0446        0.541        0.619
     20    40         0.12       0.0304       0.0898       0.0341       0.0578        0.548        0.583
     20    50       0.0478     0.000588       0.0472      0.00467      0.00805          0.5        0.515
     20    60        0.165         0.16      0.00522       0.0822        0.133        0.111        0.117
     20    70       0.0142      0.00854      0.00567       0.0124       0.0307        0.152        0.167
     20    80      0.00838      0.00465      0.00373       0.0122       0.0226        0.101        0.125
     20    90       0.0367       0.0348       0.0019       0.0422       0.0619       0.0409       0.0588
     20   100      0.00714      0.00486      0.00228       0.0128       0.0231       0.0825        0.101
     20   110        0.037        0.035      0.00197       0.0429       0.0621       0.0719        0.103
     20   120       0.0152      0.00988       0.0053       0.0186        0.033        0.159        0.182
     20   130       0.0174       0.0141      0.00329       0.0231       0.0394         0.12        0.142
     20   140       0.0234        0.018      0.00547       0.0271       0.0445        0.101        0.118
     20   150       0.0402       0.0391      0.00117       0.0475       0.0656       0.0731       0.0822
     20   160      0.00687      0.00181      0.00506      0.00971       0.0141       0.0909       0.0944
     20   170       0.0428       0.0372      0.00561       0.0426        0.064        0.134        0.169
     20   180       0.0156       0.0142      0.00144       0.0288       0.0395       0.0846       0.0998
     20   190       0.0206       0.0155      0.00513       0.0195       0.0413        0.149         0.19
     20   200        0.099       0.0325       0.0664       0.0469       0.0599        0.422        0.465
     20   210       0.0347       0.0148       0.0199       0.0271       0.0404        0.243        0.275
     20   220       0.0172       0.0116      0.00561       0.0181       0.0357        0.164        0.199
     20   230       0.0295       0.0157       0.0137        0.025       0.0416        0.189        0.205
     20   231       0.0408      0.00914       0.0317       0.0197       0.0317        0.236        0.236

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     20     7       0.0289        0.023      0.00589       0.0369       0.0504         0.12        0.158


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              20 1030.482    0.005       0.0277         1.16         1.19       0.0292       0.0542        0.265         1.45
! Validation         20 1030.482    0.005       0.0192         3.13         3.15       0.0269       0.0453        0.402         2.45
Wall time: 1030.4832326984033

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     21    10        0.465      0.00883        0.456       0.0192       0.0312        0.833        0.897
     21    20          0.2        0.109       0.0917       0.0678        0.109        0.378        0.417
     21    30       0.0248        0.012       0.0128        0.021       0.0364        0.153        0.157
     21    40        0.155        0.138       0.0172       0.0913        0.123        0.337        0.349
     21    50        0.127       0.0636       0.0636       0.0561       0.0837        0.441        0.494
     21    60         0.13       0.0496       0.0806       0.0296       0.0739        0.444        0.453
     21    70        0.041        0.028       0.0131       0.0391       0.0555        0.152        0.187
     21    80       0.0252       0.0135       0.0117        0.018       0.0386        0.169        0.203
     21    90      0.00818       0.0046      0.00358        0.013       0.0225       0.0674       0.0811
     21   100       0.0328         0.03       0.0028       0.0396       0.0575        0.124        0.132
     21   110       0.0599       0.0564      0.00347       0.0611       0.0788       0.0964        0.152
     21   120        0.102       0.0313       0.0705       0.0329       0.0587        0.622        0.653
     21   130        0.182       0.0255        0.156       0.0311        0.053        0.522        0.525
     21   140         0.12       0.0273       0.0924       0.0289       0.0549        0.375        0.404
     21   150       0.0206       0.0105       0.0101        0.026        0.034        0.102        0.134
     21   160       0.0201       0.0094       0.0107       0.0175       0.0322        0.171        0.212
     21   170       0.0117       0.0107     0.000973       0.0248       0.0344       0.0312       0.0415
     21   180       0.0499       0.0475      0.00243       0.0556       0.0723       0.0889        0.121
     21   190       0.0226       0.0165      0.00605       0.0317       0.0427        0.124        0.137
     21   200        0.039       0.0279       0.0111       0.0317       0.0555        0.192         0.28
     21   210        0.028       0.0263      0.00173       0.0393       0.0539       0.0442       0.0583
     21   220      0.00292      0.00267     0.000246        0.012       0.0172        0.025       0.0306
     21   230        0.114       0.0315       0.0829       0.0382       0.0589        0.543        0.761
     21   231       0.0525       0.0518     0.000685       0.0609       0.0756       0.0348       0.0348

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     21     7       0.0407       0.0289       0.0118       0.0403       0.0565        0.179        0.224


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              21 1081.187    0.005       0.0425         1.17         1.21       0.0358       0.0667        0.381         1.48
! Validation         21 1081.187    0.005       0.0207         3.12         3.14       0.0279       0.0467        0.467         2.45
Wall time: 1081.1876732474193

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     22    10      0.00946      0.00824      0.00122       0.0214       0.0301       0.0643       0.0927
     22    20       0.0405       0.0391      0.00142       0.0512       0.0656       0.0801       0.0979
     22    30       0.0114      0.00578      0.00561       0.0144       0.0252       0.0775        0.105
     22    40       0.0332       0.0297      0.00345       0.0454       0.0572        0.127        0.154
     22    50       0.0095       0.0079       0.0016       0.0218       0.0295       0.0674       0.0759
     22    60      0.00597      0.00524     0.000736       0.0172        0.024       0.0473       0.0716
     22    70         0.05       0.0178       0.0322       0.0254       0.0443        0.185        0.252
     22    80        0.017       0.0142      0.00284       0.0247       0.0395       0.0659       0.0744
     22    90       0.0228       0.0216      0.00123         0.03       0.0488        0.043       0.0481
     22   100       0.0113      0.00893      0.00241       0.0186       0.0314        0.121         0.13
     22   110      0.00257      0.00147       0.0011      0.00972       0.0127       0.0549        0.061
     22   120      0.00756      0.00681     0.000752       0.0171       0.0274       0.0431        0.063
     22   130       0.0204        0.019      0.00138        0.035       0.0458       0.0684       0.0818
     22   140        0.176      0.00868        0.167       0.0183       0.0309        0.689        0.745
     22   150        0.091       0.0111       0.0799       0.0228       0.0349         0.62        0.633
     22   160        0.025       0.0181      0.00689       0.0305       0.0447        0.152        0.215
     22   170        0.104       0.0143       0.0896       0.0226       0.0397        0.577        0.654
     22   180       0.0768        0.028       0.0488       0.0323       0.0556        0.376        0.514
     22   190       0.0802       0.0266       0.0536       0.0242       0.0542         0.27        0.307
     22   200       0.0199      0.00824       0.0116       0.0192       0.0301        0.129        0.148
     22   210       0.0241       0.0222      0.00189       0.0362       0.0495       0.0658       0.0993
     22   220       0.0612       0.0595      0.00172       0.0646        0.081       0.0717       0.0737
     22   230         1.58          1.5       0.0765        0.194        0.407          0.4        0.432
     22   231       0.0969        0.019       0.0778       0.0382       0.0458         0.37         0.37

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     22     7       0.0477       0.0297       0.0181       0.0402       0.0572        0.224        0.275


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              22 1131.855    0.005       0.0306         1.16         1.19       0.0306        0.057        0.284         1.46
! Validation         22 1131.855    0.005       0.0229         3.11         3.14       0.0289       0.0487         0.52         2.46
Wall time: 1131.8555995272473
! Best model       22    3.137

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     23    10       0.0799       0.0393       0.0405        0.047       0.0658        0.496        0.535
     23    20       0.0339      0.00786        0.026       0.0137       0.0294        0.259        0.303
     23    30      0.00755      0.00321      0.00435       0.0113       0.0188       0.0967        0.137
     23    40       0.0701       0.0676      0.00251       0.0537       0.0863       0.0718       0.0758
     23    50       0.0127      0.00599      0.00667       0.0157       0.0257       0.0934        0.108
     23    60       0.0209       0.0138       0.0071       0.0248       0.0389        0.149        0.151
     23    70       0.0138       0.0132     0.000645       0.0283       0.0381       0.0543       0.0572
     23    80      0.00706      0.00387      0.00319       0.0133       0.0207       0.0553        0.075
     23    90       0.0124       0.0104      0.00199       0.0209       0.0339       0.0757        0.107
     23   100        0.174       0.0136         0.16       0.0244       0.0387        0.669        0.718
     23   110       0.0602       0.0116       0.0486       0.0259       0.0357        0.503        0.527
     23   120       0.0581       0.0519      0.00613       0.0542       0.0757        0.089        0.107
     23   130       0.0132       0.0086      0.00458       0.0218       0.0308        0.103        0.161
     23   140       0.0742       0.0732     0.000988       0.0545       0.0898       0.0549       0.0624
     23   150        0.122        0.117      0.00475       0.0891        0.113        0.161        0.172
     23   160        0.113        0.108      0.00414       0.0613        0.109       0.0868        0.093
     23   170       0.0404       0.0352      0.00513       0.0362       0.0623        0.115         0.12
     23   180        0.122       0.0379       0.0841       0.0536       0.0646        0.552        0.577
     23   190        0.129       0.0537       0.0758       0.0539       0.0769        0.448        0.472
     23   200        0.105      0.00892       0.0961         0.02       0.0314        0.497        0.511
     23   210       0.0595       0.0241       0.0354       0.0365       0.0515        0.417        0.487
     23   220       0.0595       0.0446       0.0149       0.0426       0.0701        0.187        0.234
     23   230       0.0211      0.00143       0.0197      0.00726       0.0126        0.287        0.322
     23   231       0.0235       0.0212      0.00229       0.0408       0.0483        0.127        0.127

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     23     7       0.0421       0.0274       0.0147       0.0404       0.0549        0.196        0.248


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              23 1182.619    0.005       0.0555         1.16         1.22       0.0389       0.0757         0.36         1.47
! Validation         23 1182.619    0.005       0.0213         3.11         3.14       0.0288       0.0474        0.485         2.45
Wall time: 1182.619744680822
! Best model       23    3.136

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     24    10      0.00955     0.000499      0.00905      0.00491      0.00742        0.161        0.182
     24    20       0.0135      0.00597      0.00757       0.0133       0.0256        0.223        0.231
     24    30       0.0309       0.0262      0.00469       0.0394       0.0538        0.136        0.176
     24    40       0.0157      0.00276        0.013      0.00963       0.0174         0.21         0.22
     24    50      0.00334     0.000502      0.00284      0.00465      0.00744       0.0924         0.13
     24    60        0.017       0.0111      0.00592       0.0215        0.035         0.11        0.115
     24    70       0.0123      0.00909      0.00323       0.0205       0.0317        0.075       0.0858
     24    80      0.00511      0.00173      0.00338       0.0088       0.0138        0.117        0.132
     24    90       0.0198       0.0123      0.00751       0.0229       0.0368        0.105        0.124
     24   100       0.0446       0.0424      0.00216       0.0423       0.0684       0.0756        0.114
     24   110       0.0966       0.0255       0.0711       0.0373        0.053        0.438         0.47
     24   120        0.436      0.00175        0.434       0.0075       0.0139          1.3         1.37
     24   130        0.114       0.0626       0.0509       0.0539       0.0831        0.474        0.494
     24   140       0.0938       0.0531       0.0407       0.0597       0.0765        0.451        0.536
     24   150       0.0804       0.0604       0.0199       0.0591       0.0816        0.271         0.32
     24   160       0.0724       0.0514        0.021       0.0485       0.0753        0.327        0.354
     24   170       0.0231       0.0195      0.00368       0.0308       0.0463        0.104        0.118
     24   180       0.0225      0.00156        0.021      0.00911       0.0131        0.277        0.287
     24   190          1.2         1.14       0.0601        0.134        0.355        0.273        0.325
     24   200       0.0692       0.0648      0.00447        0.049       0.0845        0.106        0.125
     24   210       0.0688       0.0632      0.00559       0.0459       0.0835        0.147        0.182
     24   220       0.0601       0.0555      0.00464       0.0422       0.0782         0.12        0.181
     24   230       0.0948       0.0501       0.0446       0.0548       0.0743        0.422        0.488
     24   231        0.105       0.0392       0.0657        0.053       0.0657        0.681        0.681

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     24     7       0.0377       0.0295      0.00819       0.0412        0.057         0.14         0.19


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              24 1233.402    0.005       0.0429         1.16          1.2        0.035       0.0669         0.32         1.47
! Validation         24 1233.402    0.005       0.0229         3.12         3.15       0.0295        0.049        0.424         2.45
Wall time: 1233.402937816456

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     25    10       0.0491       0.0117       0.0375       0.0252       0.0358        0.451        0.498
     25    20        0.123       0.0579       0.0654       0.0596       0.0799        0.553        0.679
     25    30       0.0122      0.00491      0.00727      0.00982       0.0233        0.123        0.155
     25    40       0.0242       0.0199      0.00426       0.0332       0.0469       0.0694       0.0881
     25    50      0.00742      0.00429      0.00313       0.0093       0.0217        0.089        0.131
     25    60       0.0219       0.0211     0.000758       0.0383       0.0483       0.0414       0.0503
     25    70       0.0302       0.0294     0.000801       0.0329       0.0569       0.0369       0.0385
     25    80       0.0347       0.0331      0.00158       0.0501       0.0604       0.0767        0.088
     25    90       0.0282       0.0109       0.0173       0.0212       0.0347        0.228         0.23
     25   100        0.106       0.0216       0.0846       0.0323       0.0488         0.69         0.75
     25   110       0.0359      0.00877       0.0272       0.0215       0.0311         0.37        0.438
     25   120       0.0483       0.0346       0.0137       0.0338       0.0618        0.239        0.309
     25   130       0.0215       0.0119      0.00963       0.0222       0.0362        0.162        0.224
     25   140       0.0179       0.0149      0.00301       0.0222       0.0405       0.0827        0.094
     25   150       0.0708       0.0395       0.0313       0.0408        0.066        0.149        0.236
     25   160       0.0767       0.0551       0.0216       0.0537       0.0779        0.231        0.388
     25   170       0.0735       0.0711       0.0024        0.045       0.0885         0.11        0.113
     25   180       0.0153       0.0126      0.00274       0.0255       0.0372        0.101        0.107
     25   190      0.00642      0.00468      0.00174       0.0167       0.0227       0.0627       0.0654
     25   200      0.00874      0.00355      0.00519       0.0109       0.0198        0.137         0.15
     25   210       0.0266       0.0222      0.00439         0.03       0.0495       0.0728        0.088
     25   220        0.011       0.0102     0.000841       0.0228       0.0335       0.0554       0.0731
     25   230        0.342       0.0228        0.319       0.0339       0.0501         1.11         1.15
     25   231        0.321       0.0504        0.271       0.0502       0.0746         1.38         1.38

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     25     7       0.0304       0.0234      0.00694       0.0374       0.0508        0.128        0.164


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              25 1284.721    0.005       0.0297         1.15         1.18       0.0304       0.0571        0.295         1.45
! Validation         25 1284.721    0.005       0.0177         3.13         3.15       0.0262       0.0434        0.413         2.45
Wall time: 1284.7220561048016

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     26    10        0.165       0.0686       0.0959       0.0529        0.087        0.409        0.428
     26    20       0.0902       0.0724       0.0177       0.0526       0.0893        0.267        0.301
     26    30       0.0875       0.0703       0.0172       0.0382        0.088        0.172        0.174
     26    40       0.0919        0.065       0.0269       0.0515       0.0846        0.298        0.354
     26    50       0.0691       0.0479       0.0212       0.0404       0.0726        0.275        0.302
     26    60       0.0333       0.0271      0.00624       0.0338       0.0546       0.0751        0.111
     26    70       0.0229       0.0194      0.00353       0.0285       0.0462        0.109        0.131
     26    80       0.0682       0.0643      0.00393        0.054       0.0842        0.109        0.147
     26    90       0.0309       0.0264      0.00449       0.0408        0.054        0.113        0.134
     26   100       0.0276      0.00315       0.0244       0.0119       0.0186        0.172        0.208
     26   110        0.104       0.0864       0.0177       0.0729       0.0976        0.266        0.333
     26   120        0.376       0.0191        0.356       0.0283       0.0459        0.966            1
     26   130          0.2       0.0609        0.139       0.0472       0.0819        0.956        0.989
     26   140       0.0334       0.0187       0.0147       0.0295       0.0454        0.154        0.182
     26   150       0.0612       0.0524      0.00878       0.0493        0.076        0.122        0.124
     26   160       0.0469       0.0279        0.019       0.0314       0.0554        0.212        0.229
     26   170       0.0208      0.00912       0.0117       0.0149       0.0317        0.132        0.144
     26   180       0.0116      0.00197      0.00967      0.00879       0.0147        0.119        0.143
     26   190       0.0188       0.0162      0.00256       0.0256       0.0423        0.103        0.107
     26   200        0.112       0.0314       0.0806       0.0477       0.0589        0.743        0.754
     26   210       0.0849       0.0243       0.0606        0.034       0.0518        0.455        0.471
     26   220       0.0498       0.0223       0.0274       0.0295       0.0496        0.334        0.364
     26   230       0.0644       0.0373       0.0271       0.0457       0.0641        0.123        0.219
     26   231       0.0175       0.0147      0.00279       0.0274       0.0402         0.14         0.14

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     26     7       0.0589       0.0299        0.029       0.0406       0.0574        0.276        0.343


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              26 1336.112    0.005       0.0618         1.18         1.24       0.0407       0.0823        0.419         1.49
! Validation         26 1336.112    0.005       0.0249         3.11         3.13       0.0292       0.0507        0.587         2.46
Wall time: 1336.1132886204869
! Best model       26    3.135

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     27    10       0.0252       0.0189      0.00632       0.0286       0.0456        0.123        0.125
     27    20        0.303       0.0103        0.292       0.0144       0.0336        0.427         0.72
     27    30       0.0329       0.0233      0.00967       0.0242       0.0506        0.162        0.185
     27    40       0.0547       0.0404       0.0143       0.0518       0.0667        0.125        0.166
     27    50       0.0381       0.0304      0.00768       0.0373       0.0579        0.215        0.233
     27    60       0.0716       0.0632      0.00843       0.0448       0.0835        0.179        0.244
     27    70       0.0141      0.00795      0.00613       0.0185       0.0296        0.134        0.149
     27    80        0.108        0.014       0.0938       0.0223       0.0392        0.684        0.697
     27    90         1.17         1.12       0.0594        0.159        0.351        0.367        0.393
     27   100       0.0457       0.0128        0.033       0.0206       0.0375        0.328        0.355
     27   110       0.0681       0.0646      0.00354       0.0653       0.0844        0.109        0.144
     27   120        0.023       0.0169      0.00605       0.0279       0.0432        0.146        0.205
     27   130        0.044        0.033        0.011       0.0382       0.0603        0.122        0.139
     27   140       0.0199        0.017      0.00289       0.0309       0.0433       0.0672       0.0876
     27   150       0.0214       0.0202      0.00122       0.0369       0.0471       0.0462        0.059
     27   160        0.036       0.0286       0.0074       0.0379       0.0561        0.185        0.189
     27   170       0.0579      0.00469       0.0532       0.0134       0.0227        0.607        0.612
     27   180       0.0427       0.0259       0.0168       0.0372       0.0535        0.288        0.344
     27   190       0.0474       0.0216       0.0258       0.0273       0.0488        0.328        0.426
     27   200       0.0269        0.022       0.0049       0.0379       0.0493        0.086        0.093
     27   210        0.043        0.037      0.00601       0.0242       0.0639        0.136        0.162
     27   220      0.00628     0.000547      0.00574      0.00448      0.00776        0.115        0.117
     27   230          121       0.0204          121       0.0214       0.0474         7.39         14.6
     27   231       0.0242       0.0154      0.00882       0.0325       0.0412        0.125        0.125

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     27     7       0.0297       0.0209      0.00878       0.0356        0.048        0.143        0.193


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              27 1387.485    0.005       0.0392         1.16          1.2       0.0328       0.0645        0.292         1.45
! Validation         27 1387.485    0.005       0.0188         3.12         3.14       0.0275       0.0452        0.429         2.45
Wall time: 1387.4865256827325

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     28    10        0.106     0.000127        0.106      0.00276      0.00374         0.43        0.432
     28    20        0.103       0.0976      0.00553       0.0829        0.104        0.139        0.194
     28    30       0.0371       0.0194       0.0178       0.0262       0.0462        0.198        0.277
     28    40       0.0218      0.00818       0.0136       0.0152         0.03        0.147        0.157
     28    50       0.0287       0.0237      0.00495       0.0366       0.0511       0.0797       0.0971
     28    60       0.0164       0.0118      0.00462       0.0214        0.036        0.117        0.158
     28    70       0.0137       0.0116      0.00204        0.024       0.0358       0.0908        0.103
     28    80        0.077       0.0132       0.0638       0.0208       0.0381        0.592        0.661
     28    90       0.0489        0.043      0.00585       0.0503       0.0689        0.133        0.189
     28   100       0.0404       0.0234       0.0169       0.0339       0.0508        0.166        0.178
     28   110       0.0454       0.0249       0.0205       0.0336       0.0524        0.294        0.353
     28   120         0.02       0.0146      0.00544        0.024       0.0401        0.111        0.152
     28   130      0.00471      0.00271        0.002      0.00728       0.0173        0.101        0.103
     28   140       0.0729        0.072     0.000876       0.0653       0.0891       0.0616       0.0786
     28   150       0.0407       0.0211       0.0196        0.026       0.0483         0.32         0.35
     28   160       0.0419       0.0228       0.0191        0.029       0.0501         0.33        0.355
     28   170       0.0289      0.00177       0.0271      0.00985        0.014        0.317        0.332
     28   180       0.0698       0.0433       0.0265       0.0454       0.0691        0.292        0.424
     28   190       0.0295       0.0226      0.00687       0.0341       0.0499        0.166        0.201
     28   200        0.081       0.0276       0.0533       0.0308       0.0552        0.439         0.46
     28   210        0.097      0.00709       0.0899       0.0145        0.028        0.357        0.398
     28   220       0.0455       0.0217       0.0238       0.0306       0.0489        0.323        0.326
     28   230       0.0395       0.0257       0.0138       0.0396       0.0532        0.246        0.272
     28   231       0.0284     1.35e-05       0.0284     0.000918      0.00122        0.224        0.224

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     28     7       0.0337       0.0201       0.0136       0.0348       0.0471        0.192        0.236


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              28 1438.801    0.005       0.0259         1.17         1.19         0.03       0.0538        0.349         1.46
! Validation         28 1438.801    0.005       0.0173         3.12         3.13       0.0258       0.0432        0.477         2.45
Wall time: 1438.8023223532364

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     29    10       0.0384       0.0194        0.019       0.0288       0.0462        0.161        0.193
     29    20         0.02       0.0181       0.0019       0.0329       0.0447       0.0805       0.0827
     29    30       0.0483        0.016       0.0323       0.0236        0.042        0.296        0.371
     29    40       0.0929       0.0377       0.0552       0.0411       0.0644        0.267        0.312
     29    50       0.0488       0.0187         0.03       0.0246       0.0454        0.273        0.345
     29    60       0.0424       0.0303        0.012        0.034       0.0578        0.255        0.291
     29    70       0.0168       0.0137       0.0031       0.0231       0.0389        0.139        0.148
     29    80       0.0096      0.00702      0.00258       0.0163       0.0278       0.0983        0.131
     29    90      0.00453      0.00393     0.000598       0.0149       0.0208       0.0283       0.0325
     29   100      0.00251      0.00145      0.00106       0.0082       0.0126       0.0548       0.0664
     29   110       0.0374       0.0309      0.00644        0.033       0.0584        0.164        0.184
     29   120       0.0697       0.0332       0.0364       0.0295       0.0605        0.203        0.275
     29   130        0.019        0.015      0.00394       0.0281       0.0407        0.105        0.127
     29   140       0.0165       0.0155      0.00099       0.0275       0.0413       0.0413       0.0456
     29   150       0.0236       0.0173      0.00625       0.0251       0.0437        0.167         0.21
     29   160      0.00315      0.00174      0.00141      0.00943       0.0138       0.0474       0.0533
     29   170       0.0351       0.0299      0.00518       0.0434       0.0574        0.126        0.137
     29   180       0.0475       0.0433      0.00416       0.0525       0.0691        0.153        0.162
     29   190       0.0363      0.00905       0.0272       0.0156       0.0316         0.17        0.243
     29   200        0.092      0.00934       0.0826       0.0206       0.0321        0.555        0.576
     29   210          119       0.0563          119       0.0553       0.0788         7.43         14.5
     29   220        0.162       0.0087        0.153        0.022        0.031        0.862        0.884
     29   230       0.0416      0.00721       0.0344       0.0132       0.0282        0.267        0.283
     29   231        0.023      0.00186       0.0211       0.0126       0.0143        0.193        0.193

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     29     7       0.0341       0.0191        0.015        0.033       0.0459        0.201        0.242


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              29 1490.110    0.005       0.0269         1.15         1.18       0.0292       0.0536        0.303         1.45
! Validation         29 1490.110    0.005       0.0159         3.12         3.14       0.0247       0.0412        0.491         2.45
Wall time: 1490.110608623363

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     30    10       0.0208     8.76e-05       0.0207      0.00218      0.00311        0.183        0.191
     30    20       0.0291       0.0247      0.00441       0.0328       0.0522       0.0931        0.104
     30    30       0.0246       0.0139       0.0107       0.0231       0.0391         0.23        0.267
     30    40         0.15      0.00401        0.146       0.0145        0.021        0.749        0.781
     30    50        0.117     0.000415        0.117      0.00481      0.00676        0.897        0.908
     30    60        0.143        0.113       0.0296       0.0744        0.112        0.286        0.318
     30    70        0.211       0.0492        0.161       0.0454       0.0736        0.665        0.757
     30    80       0.0245      0.00387       0.0206       0.0129       0.0206        0.209        0.214
     30    90       0.0494       0.0384        0.011       0.0403       0.0651        0.169        0.207
     30   100        0.029       0.0239      0.00507        0.034       0.0513        0.145        0.182
     30   110       0.0135      0.00903      0.00445        0.013       0.0315        0.106        0.136
     30   120        0.023       0.0228     0.000207       0.0378       0.0501       0.0323       0.0382
     30   130       0.0228       0.0218     0.000953       0.0387        0.049       0.0374       0.0446
     30   140       0.0126      0.00711       0.0055        0.017        0.028        0.138        0.179
     30   150      0.00859      0.00387      0.00472       0.0122       0.0207       0.0651       0.0912
     30   160        0.177       0.0441        0.133       0.0502       0.0697        0.949        0.968
     30   170       0.0749       0.0566       0.0182       0.0604        0.079        0.248        0.253
     30   180       0.0397        0.034      0.00574       0.0296       0.0612         0.15        0.159
     30   190      0.00487     0.000229      0.00464      0.00348      0.00502        0.135        0.165
     30   200        0.247        0.012        0.235       0.0177       0.0364        0.767         1.28
     30   210       0.0176      0.00893      0.00868        0.019       0.0314        0.108        0.124
     30   220        0.466        0.461      0.00456        0.112        0.225        0.071       0.0897
     30   230       0.0139      0.00487        0.009       0.0117       0.0232        0.149        0.186
     30   231       0.0143     0.000101       0.0142      0.00268      0.00334        0.158        0.158

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     30     7        0.047       0.0282       0.0188       0.0394       0.0557        0.223        0.276


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              30 1541.523    0.005       0.0449         1.16         1.21       0.0356       0.0695        0.339         1.47
! Validation         30 1541.523    0.005       0.0213         3.11         3.13       0.0284       0.0473        0.516         2.45
Wall time: 1541.5241446513683
! Best model       30    3.134

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     31    10       0.0204       0.0162       0.0042       0.0206       0.0422        0.108        0.126
     31    20        0.158       0.0899       0.0679       0.0837       0.0995        0.387        0.482
     31    30       0.0128       0.0119     0.000888       0.0222       0.0362       0.0523       0.0616
     31    40         1.41       0.0605         1.35       0.0373       0.0817        0.811         1.54
     31    50        0.322        0.313      0.00873        0.065        0.186        0.187        0.199
     31    60       0.0198       0.0182      0.00152       0.0206       0.0448       0.0845          0.1
     31    70       0.0893       0.0188       0.0705       0.0245       0.0455        0.505        0.536
     31    80       0.0839       0.0258       0.0581       0.0342       0.0533        0.421        0.499
     31    90       0.0207       0.0163      0.00441       0.0325       0.0424       0.0798       0.0927
     31   100        0.026       0.0137       0.0124       0.0283       0.0388        0.186        0.238
     31   110       0.0211       0.0186      0.00248       0.0363       0.0453       0.0847       0.0929
     31   120       0.0275       0.0234      0.00415        0.037       0.0508        0.112        0.123
     31   130       0.0157       0.0143      0.00131       0.0276       0.0398       0.0386       0.0494
     31   140       0.0228       0.0198      0.00303       0.0316       0.0467       0.0916        0.114
     31   150       0.0455       0.0203       0.0252        0.031       0.0473        0.134        0.211
     31   160       0.0223       0.0213     0.000962       0.0369       0.0485       0.0314        0.043
     31   170      0.00187     0.000607      0.00126      0.00391      0.00818       0.0586       0.0678
     31   180       0.0316       0.0204       0.0112       0.0224       0.0474        0.217        0.275
     31   190        0.211       0.0364        0.175       0.0479       0.0633         0.98         1.02
     31   200        0.282        0.242       0.0407       0.0662        0.163        0.357        0.374
     31   210        0.041      0.00979       0.0312        0.024       0.0328        0.255        0.311
     31   220        0.202        0.161       0.0412        0.088        0.133        0.271        0.298
     31   230        0.074     0.000174       0.0738      0.00344      0.00437         0.36        0.361
     31   231       0.0382      0.00481       0.0334       0.0184        0.023        0.243        0.243

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     31     7       0.0575       0.0306       0.0269       0.0399       0.0581        0.261        0.326


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              31 1592.357    0.005       0.0439         1.17         1.21       0.0357       0.0692        0.313         1.46
! Validation         31 1592.357    0.005       0.0238         3.11         3.14       0.0291       0.0498         0.57         2.46
Wall time: 1592.3579093413427

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     32    10       0.0334       0.0321      0.00132       0.0452       0.0595       0.0651       0.0771
     32    20       0.0269       0.0169      0.00997        0.028       0.0432         0.19        0.265
     32    30      0.00777      0.00741      0.00036        0.016       0.0286       0.0261       0.0306
     32    40        0.017       0.0143      0.00273       0.0232       0.0397       0.0843       0.0955
     32    50       0.0551       0.0234       0.0316       0.0329       0.0508         0.16        0.243
     32    60      0.00659     0.000264      0.00633       0.0041       0.0054        0.134        0.155
     32    70       0.0109      0.00216      0.00874      0.00722       0.0154        0.158        0.177
     32    80        0.071       0.0565       0.0145       0.0517       0.0789         0.22        0.292
     32    90       0.0222       0.0214     0.000764       0.0302       0.0486       0.0262       0.0371
     32   100       0.0223       0.0163      0.00599       0.0288       0.0424        0.149        0.161
     32   110       0.0119      0.00976      0.00215       0.0222       0.0328       0.0794        0.109
     32   120      0.00213      0.00167     0.000456      0.00716       0.0136       0.0432       0.0547
     32   130      0.00652     0.000398      0.00613       0.0034      0.00662       0.0996        0.104
     32   140       0.0171       0.0109      0.00619       0.0204       0.0346        0.142        0.168
     32   150         85.6       0.0114         85.6       0.0234       0.0354         6.21         12.3
     32   160        0.264       0.0125        0.252       0.0232       0.0371        0.819        0.919
     32   170        0.181       0.0305         0.15       0.0403       0.0579        0.626        0.649
     32   180        0.071       0.0348       0.0363       0.0379       0.0619        0.238        0.253
     32   190        0.116       0.0352       0.0812       0.0478       0.0623        0.481         0.56
     32   200        0.117       0.0751       0.0416       0.0483        0.091         0.32        0.393
     32   210       0.0654       0.0297       0.0358       0.0368       0.0572        0.225        0.254
     32   220       0.0207       0.0153      0.00542        0.029        0.041        0.167        0.196
     32   230       0.0204       0.0155      0.00484       0.0318       0.0414        0.102        0.112
     32   231      0.00318      0.00249     0.000695       0.0124       0.0166         0.07         0.07

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     32     7       0.0437       0.0252       0.0186        0.037       0.0527        0.227         0.28


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              32 1643.224    0.005       0.0324         1.16         1.19       0.0315       0.0601        0.315         1.46
! Validation         32 1643.224    0.005       0.0195         3.11         3.13        0.026       0.0449        0.519         2.46
Wall time: 1643.2248551081866
! Best model       32    3.133

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     33    10       0.0568       0.0357       0.0211       0.0446       0.0627        0.226        0.334
     33    20       0.0296        0.028       0.0017       0.0424       0.0555       0.0468       0.0579
     33    30        0.014      0.00611      0.00786       0.0127       0.0259        0.131        0.144
     33    40       0.0205       0.0181      0.00236       0.0288       0.0447       0.0674       0.0748
     33    50       0.0104      0.00511      0.00528       0.0138       0.0237        0.118         0.15
     33    60       0.0134       0.0116      0.00174       0.0213       0.0358       0.0596       0.0657
     33    70       0.0317       0.0262      0.00551        0.039       0.0537       0.0547       0.0986
     33    80       0.0338       0.0264      0.00738        0.045        0.054        0.118        0.228
     33    90          121       0.0386          121       0.0314       0.0652         7.38         14.6
     33   100        0.146       0.0238        0.122        0.028       0.0513         0.67        0.705
     33   110       0.0472        0.021       0.0261       0.0379       0.0482        0.298        0.304
     33   120       0.0302       0.0137       0.0165       0.0214       0.0389        0.163         0.17
     33   130      0.00908      0.00816     0.000911       0.0195         0.03        0.052       0.0639
     33   140       0.0855       0.0623       0.0232       0.0614       0.0829        0.262        0.389
     33   150         0.24     0.000231         0.24      0.00356      0.00505        0.787         1.29
     33   160       0.0251       0.0208      0.00428       0.0251       0.0479       0.0975        0.107
     33   170      0.00521      0.00301       0.0022      0.00895       0.0182       0.0689       0.0719
     33   180       0.0117       0.0106      0.00115       0.0234       0.0341       0.0377       0.0461
     33   190       0.0738       0.0424       0.0314       0.0437       0.0684        0.384        0.386
     33   200         83.7       0.0184         83.7       0.0339       0.0451         6.41         12.2
     33   210        0.341       0.0163        0.325       0.0276       0.0424         1.11         1.41
     33   220       0.0687       0.0364       0.0324       0.0464       0.0633        0.205        0.245
     33   230        0.238        0.214       0.0249       0.0627        0.153        0.173         0.21
     33   231       0.0022     0.000315      0.00189      0.00467      0.00589       0.0577       0.0577

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     33     7       0.0371       0.0199       0.0172       0.0328       0.0468        0.213        0.262


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              33 1693.936    0.005       0.0225         1.15         1.18       0.0271       0.0497        0.312         1.45
! Validation         33 1693.936    0.005       0.0161         3.12         3.13       0.0243       0.0414        0.505         2.45
Wall time: 1693.9367463383824
! Best model       33    3.132

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     34    10       0.0598       0.0485       0.0113       0.0315       0.0731        0.219        0.237
     34    20        0.154        0.146      0.00805       0.0798        0.127        0.213        0.238
     34    30       0.0561       0.0476      0.00851       0.0443       0.0724        0.151        0.165
     34    40       0.0181      0.00768       0.0104       0.0202       0.0291        0.151        0.154
     34    50         0.18       0.0266        0.154       0.0397       0.0541        0.706        0.752
     34    60         0.21       0.0688        0.141       0.0336       0.0871        0.762        0.833
     34    70       0.0863       0.0518       0.0345       0.0506       0.0756         0.32        0.374
     34    80       0.0689         0.06       0.0089       0.0565       0.0813        0.167        0.232
     34    90       0.0159      0.00221       0.0136       0.0095       0.0156        0.254        0.271
     34   100       0.0411       0.0301       0.0111       0.0433       0.0576        0.176        0.177
     34   110       0.0262        0.013       0.0132       0.0209       0.0378         0.14        0.153
     34   120      0.00103     0.000534     0.000493       0.0053      0.00767       0.0157       0.0295
     34   130       0.0274       0.0257      0.00172       0.0356       0.0532        0.099         0.11
     34   140      0.00432      0.00202       0.0023        0.011       0.0149        0.079       0.0791
     34   150      0.00719      0.00307      0.00412       0.0114       0.0184        0.108        0.136
     34   160        0.032       0.0284      0.00358       0.0408        0.056       0.0718       0.0799
     34   170       0.0127       0.0114      0.00138       0.0197       0.0354       0.0584       0.0701
     34   180       0.0178       0.0161      0.00169       0.0329       0.0421       0.0807        0.108
     34   190      0.00804      0.00688      0.00115       0.0152       0.0275       0.0797       0.0901
     34   200        0.168       0.0525        0.116       0.0434        0.076        0.578        0.648
     34   210        0.114      0.00298        0.111       0.0121       0.0181        0.652        0.692
     34   220       0.0918       0.0521       0.0397       0.0527       0.0758        0.214        0.272
     34   230       0.0434       0.0348      0.00864       0.0317       0.0619        0.102        0.123
     34   231       0.0201       0.0149      0.00511       0.0339       0.0406       0.0949       0.0949

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     34     7       0.0425       0.0214       0.0212       0.0329       0.0485        0.235        0.292


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              34 1744.671    0.005        0.033         1.16          1.2       0.0332       0.0607        0.332         1.47
! Validation         34 1744.671    0.005       0.0181         3.11         3.13       0.0254       0.0436        0.534         2.45
Wall time: 1744.6725721489638
! Best model       34    3.131

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     35    10        0.015      0.00759      0.00743       0.0125       0.0289        0.133        0.177
     35    20       0.0156       0.0131      0.00251       0.0254       0.0379       0.0592       0.0684
     35    30       0.0456       0.0224       0.0232       0.0291       0.0497        0.297        0.401
     35    40       0.0251        0.017      0.00811       0.0296       0.0433       0.0851        0.121
     35    50        0.018       0.0166      0.00135       0.0315       0.0428       0.0602       0.0665
     35    60       0.0319         0.03      0.00188       0.0416       0.0575       0.0727        0.106
     35    70       0.0188       0.0119      0.00684       0.0193       0.0363        0.163        0.189
     35    80       0.0312       0.0301      0.00115       0.0402       0.0576       0.0389       0.0451
     35    90       0.0366       0.0269      0.00971       0.0316       0.0544        0.124        0.145
     35   100       0.0142        0.012      0.00224       0.0222       0.0363       0.0585       0.0657
     35   110        0.029       0.0189       0.0102       0.0336       0.0456        0.174        0.179
     35   120      0.00761      0.00435      0.00326       0.0143       0.0219        0.103        0.133
     35   130        0.186      0.00233        0.184      0.00909        0.016        0.857        0.912
     35   140       0.0929       0.0394       0.0535       0.0445       0.0659        0.419         0.43
     35   150       0.0435       0.0135         0.03       0.0227       0.0386        0.295        0.348
     35   160       0.0123      0.00787      0.00443       0.0159       0.0295        0.117         0.16
     35   170       0.0419       0.0367      0.00518       0.0407       0.0636        0.116        0.147
     35   180      0.00784      0.00727     0.000572         0.02       0.0283       0.0379       0.0606
     35   190        0.406       0.0143        0.392       0.0288       0.0397        0.891         1.66
     35   200       0.0309       0.0116       0.0194       0.0214       0.0357        0.316        0.355
     35   210       0.0935        0.071       0.0225       0.0545       0.0885        0.287        0.299
     35   220         1.07            1       0.0668        0.149        0.332        0.302        0.382
     35   230       0.0321      0.00795       0.0242       0.0163       0.0296        0.348        0.372
     35   231       0.0623       0.0465       0.0158       0.0664       0.0716        0.167        0.167

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     35     7       0.0354        0.019       0.0164       0.0317       0.0458        0.203        0.254


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              35 1795.336    0.005       0.0269         1.16         1.18       0.0295       0.0544        0.288         1.45
! Validation         35 1795.336    0.005        0.017         3.12         3.13       0.0247       0.0425        0.496         2.45
Wall time: 1795.3375730793923

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     36    10       0.0482      0.00189       0.0463      0.00931       0.0144        0.406        0.423
     36    20       0.0549       0.0404       0.0145       0.0456       0.0667        0.183         0.21
     36    30       0.0117      0.00169      0.00997      0.00776       0.0137        0.192        0.265
     36    40        0.427       0.0256        0.401       0.0364       0.0531        0.925         1.67
     36    50       0.0757       0.0308        0.045       0.0452       0.0582        0.555        0.563
     36    60       0.0582       0.0158       0.0424       0.0284       0.0418        0.431        0.473
     36    70       0.0256     0.000934       0.0247      0.00725       0.0101         0.33         0.35
     36    80        0.014      0.00101        0.013       0.0063       0.0106        0.235        0.294
     36    90       0.0228       0.0162      0.00658       0.0255       0.0423         0.18        0.186
     36   100      0.00676      0.00498      0.00178       0.0166       0.0234       0.0682        0.086
     36   110       0.0336       0.0307      0.00295       0.0423       0.0582       0.0934        0.103
     36   120      0.00887      0.00628      0.00259       0.0111       0.0263       0.0902        0.111
     36   130       0.0785       0.0355        0.043        0.048       0.0626        0.201        0.276
     36   140      0.00476      0.00148      0.00328      0.00651       0.0128       0.0975        0.121
     36   150        0.023       0.0213      0.00173       0.0373       0.0485       0.0947        0.108
     36   160       0.0194       0.0176      0.00179       0.0305       0.0441       0.0763       0.0906
     36   170       0.0136       0.0129     0.000623        0.028       0.0378       0.0338       0.0382
     36   180       0.0104      0.00889      0.00152       0.0218       0.0313       0.0748       0.0889
     36   190       0.0779        0.003       0.0749       0.0119       0.0182        0.362        0.363
     36   200       0.0433      0.00728        0.036       0.0185       0.0283        0.365         0.43
     36   210         1.13      0.00979         1.12       0.0232       0.0329        0.962         1.43
     36   220        0.164        0.126       0.0382       0.0592        0.118        0.399        0.495
     36   230       0.0872       0.0807      0.00647       0.0675       0.0943        0.144        0.148
     36   231       0.0123      0.00745      0.00483       0.0211       0.0287        0.185        0.185

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     36     7       0.0324       0.0177       0.0147       0.0321       0.0442        0.191        0.241


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              36 1846.099    0.005       0.0243         1.15         1.18       0.0279        0.051        0.313         1.45
! Validation         36 1846.099    0.005       0.0153         3.12         3.13       0.0238       0.0406        0.483         2.45
Wall time: 1846.1003448190168

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     37    10       0.0419        0.013       0.0289       0.0212       0.0378        0.314        0.339
     37    20       0.0602       0.0533       0.0069       0.0545       0.0767        0.105        0.133
     37    30       0.0274       0.0129       0.0144       0.0206       0.0378        0.131         0.16
     37    40       0.0987       0.0329       0.0658       0.0319       0.0602        0.466        0.501
     37    50       0.0601       0.0131       0.0469       0.0222       0.0381        0.338        0.344
     37    60       0.0248       0.0188      0.00597       0.0315       0.0456        0.115        0.119
     37    70       0.0586        0.054      0.00458       0.0504       0.0772        0.111        0.177
     37    80       0.0157       0.0101      0.00553       0.0227       0.0334        0.146        0.192
     37    90       0.0519       0.0312       0.0207        0.036       0.0586        0.273        0.364
     37   100       0.0182        0.012      0.00612       0.0209       0.0364        0.207        0.208
     37   110       0.0173       0.0136      0.00372       0.0265       0.0387        0.106        0.131
     37   120       0.0593       0.0363        0.023       0.0441       0.0633        0.179        0.213
     37   130       0.0239       0.0229      0.00105       0.0334       0.0502       0.0525       0.0583
     37   140       0.0188       0.0185     0.000322       0.0308       0.0451       0.0368       0.0462
     37   150       0.0398       0.0087       0.0311       0.0194        0.031        0.261        0.293
     37   160       0.0429       0.0147       0.0282        0.026       0.0403        0.291        0.311
     37   170       0.0294       0.0187       0.0107       0.0368       0.0454        0.133        0.154
     37   180       0.0426       0.0106       0.0319       0.0222       0.0342        0.308         0.41
     37   190        0.153       0.0637       0.0893       0.0548       0.0838        0.588        0.615
     37   200        0.064       0.0136       0.0504       0.0216       0.0387        0.462        0.487
     37   210        0.023      0.00682       0.0162       0.0186       0.0274        0.235        0.265
     37   220        0.036        0.026       0.0101       0.0341       0.0535        0.144        0.152
     37   230        0.015      0.00942      0.00554        0.024       0.0322       0.0893        0.103
     37   231       0.0466       0.0362       0.0104       0.0513       0.0632        0.271        0.271

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     37     7       0.0274        0.016       0.0114       0.0306       0.0419        0.172        0.219


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              37 1896.817    0.005        0.027         1.15         1.18       0.0295       0.0543        0.304         1.45
! Validation         37 1896.817    0.005       0.0151         3.12         3.13       0.0241       0.0405        0.451         2.45
Wall time: 1896.8183010620996

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     38    10       0.0281       0.0229      0.00522       0.0313       0.0503        0.141        0.147
     38    20       0.0394       0.0393      0.00015       0.0507       0.0658       0.0164       0.0176
     38    30       0.0141        0.013       0.0011       0.0243       0.0379       0.0578       0.0615
     38    40        0.206        0.203      0.00329       0.0648         0.15        0.103        0.152
     38    50        0.137        0.124       0.0133        0.089        0.117        0.282        0.306
     38    60       0.0602       0.0217       0.0385       0.0289       0.0489         0.51        0.521
     38    70       0.0726        0.016       0.0566       0.0179       0.0421        0.409         0.45
     38    80       0.0627        0.048       0.0147       0.0477       0.0727        0.262        0.307
     38    90        0.115       0.0449       0.0701       0.0502       0.0703        0.522          0.6
     38   100        0.078       0.0392       0.0388       0.0486       0.0658        0.332         0.38
     38   110       0.0718       0.0501       0.0217       0.0548       0.0743         0.21        0.251
     38   120       0.0236       0.0166      0.00692       0.0273       0.0428        0.136        0.141
     38   130       0.0203       0.0146      0.00576       0.0186       0.0401        0.137        0.178
     38   140      0.00937      0.00532      0.00406       0.0128       0.0242       0.0882        0.097
     38   150       0.0197        0.017      0.00271       0.0301       0.0433        0.103        0.137
     38   160      0.00952      0.00729      0.00223       0.0151       0.0283       0.0919        0.124
     38   170        0.014       0.0121      0.00194       0.0211       0.0364       0.0672       0.0773
     38   180       0.0219       0.0198      0.00213       0.0294       0.0467       0.0702       0.0799
     38   190      0.00841      0.00622      0.00219       0.0178       0.0262       0.0845       0.0918
     38   200      0.00946      0.00634      0.00312       0.0166       0.0264       0.0668       0.0804
     38   210       0.0387       0.0325      0.00629       0.0389       0.0598        0.095        0.106
     38   220       0.0338       0.0242      0.00961       0.0329       0.0517        0.166        0.237
     38   230        0.104       0.0393       0.0642       0.0443       0.0658        0.592        0.634
     38   231        0.083      0.00683       0.0761       0.0234       0.0274        0.733        0.733

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     38     7       0.0213        0.014      0.00733       0.0283       0.0392        0.136        0.169


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              38 1947.480    0.005       0.0311         1.15         1.18       0.0311       0.0579        0.281         1.45
! Validation         38 1947.480    0.005       0.0143         3.13         3.14       0.0233       0.0395        0.408         2.45
Wall time: 1947.4811009019613

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     39    10       0.0429      0.00666       0.0362       0.0177       0.0271        0.387        0.443
     39    20       0.0241       0.0238      0.00022       0.0367       0.0513       0.0291       0.0338
     39    30       0.0373       0.0174       0.0199       0.0209       0.0438        0.295        0.327
     39    40       0.0147        0.014     0.000677       0.0283       0.0393       0.0428       0.0481
     39    50       0.0175      0.00993      0.00755       0.0206       0.0331        0.142         0.19
     39    60      0.00598     0.000795      0.00518      0.00569      0.00936       0.0857       0.0956
     39    70       0.0314       0.0295      0.00185       0.0388        0.057       0.0724        0.079
     39    80        0.103     0.000629        0.102      0.00424      0.00833        0.635        0.672
     39    90       0.0338      0.00784        0.026       0.0192       0.0294        0.275        0.289
     39   100       0.0146       0.0111      0.00346       0.0249        0.035          0.1        0.116
     39   110       0.0189       0.0101      0.00882       0.0236       0.0333        0.127         0.13
     39   120       0.0402       0.0356      0.00462       0.0448       0.0627        0.122         0.18
     39   130      0.00918      0.00071      0.00847      0.00658      0.00885        0.114        0.122
     39   140       0.0135       0.0121      0.00139       0.0248       0.0365        0.073       0.0889
     39   150       0.0125       0.0105      0.00204       0.0195        0.034       0.0921        0.116
     39   160        0.238      0.00795         0.23       0.0198       0.0296        0.571        0.637
     39   170       0.0996       0.0423       0.0572        0.053       0.0683        0.449        0.464
     39   180       0.0419       0.0144       0.0275        0.022       0.0398        0.269        0.296
     39   190       0.0239       0.0188      0.00503       0.0262       0.0456        0.117        0.188
     39   200       0.0431      0.00365       0.0395       0.0134       0.0201        0.509        0.528
     39   210       0.0584       0.0122       0.0463       0.0215       0.0366         0.38        0.386
     39   220       0.0234      0.00339       0.0201       0.0124       0.0193        0.168        0.188
     39   230       0.0122      0.00552      0.00666       0.0181       0.0247        0.164        0.192
     39   231       0.0257       0.0241      0.00156         0.04       0.0516       0.0524       0.0524

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     39     7       0.0298       0.0145       0.0153       0.0288         0.04        0.195         0.25


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              39 1998.208    0.005       0.0321         1.16         1.19       0.0302       0.0572        0.334         1.46
! Validation         39 1998.208    0.005       0.0144         3.12         3.13       0.0232       0.0395         0.49         2.45
Wall time: 1998.2091433741152
! Best model       39    3.130

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     40    10       0.0708       0.0536       0.0173       0.0434       0.0768        0.194        0.199
     40    20         0.63        0.569       0.0617        0.133         0.25        0.497        0.603
     40    30        0.155        0.111       0.0441       0.0648        0.111        0.325        0.385
     40    40       0.0442       0.0297       0.0145       0.0344       0.0572        0.221        0.228
     40    50       0.0711       0.0514       0.0197       0.0499       0.0753        0.305        0.364
     40    60      0.00821      0.00305      0.00516      0.00759       0.0183        0.144        0.175
     40    70      0.00755       0.0035      0.00405       0.0106       0.0196        0.118        0.139
     40    80       0.0243       0.0147      0.00967       0.0229       0.0402        0.163         0.17
     40    90       0.0185       0.0164      0.00209       0.0236       0.0425       0.0683       0.0751
     40   100       0.0303       0.0258      0.00452       0.0339       0.0533        0.119        0.122
     40   110       0.0167        0.011      0.00571       0.0227       0.0347       0.0933        0.104
     40   120       0.0951       0.0345       0.0606       0.0414       0.0617        0.607        0.654
     40   130       0.0387      0.00249       0.0362       0.0102       0.0166        0.411        0.414
     40   140         0.38        0.373      0.00676        0.099        0.203         0.15        0.161
     40   150        0.166        0.157      0.00877       0.0874        0.132        0.102        0.131
     40   160       0.0285       0.0126       0.0159       0.0262       0.0373        0.197        0.201
     40   170       0.0196       0.0143      0.00526       0.0216       0.0397        0.103         0.11
     40   180       0.0555       0.0519      0.00351       0.0539       0.0757        0.117        0.128
     40   190       0.0345       0.0328      0.00178       0.0415       0.0601        0.107        0.112
     40   200       0.0172       0.0151       0.0021       0.0272       0.0408       0.0528       0.0608
     40   210       0.0181       0.0139      0.00425       0.0222       0.0391         0.15        0.162
     40   220       0.0952      0.00984       0.0854       0.0233       0.0329        0.487        0.523
     40   230       0.0454       0.0109       0.0345       0.0191       0.0347        0.428        0.473
     40   231       0.0439      0.00806       0.0358       0.0257       0.0298        0.251        0.251

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     40     7       0.0297       0.0181       0.0116       0.0303       0.0446        0.177        0.213


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              40 2048.887    0.005       0.0478         1.15          1.2       0.0351       0.0713        0.307         1.45
! Validation         40 2048.887    0.005       0.0162         3.12         3.14       0.0245       0.0413        0.455         2.45
Wall time: 2048.8880038615316

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     41    10        0.111        0.025       0.0858       0.0369       0.0525        0.538        0.552
     41    20       0.0679       0.0327       0.0352        0.046         0.06        0.355        0.386
     41    30       0.0389       0.0314      0.00745        0.032       0.0589        0.149        0.206
     41    40       0.0178       0.0101      0.00767       0.0211       0.0334        0.102        0.116
     41    50        0.102        0.065       0.0369       0.0535       0.0846        0.244        0.292
     41    60       0.0217       0.0188      0.00288       0.0348       0.0455       0.0689        0.081
     41    70      0.00931      0.00849     0.000824       0.0217       0.0306       0.0565       0.0588
     41    80       0.0226       0.0159      0.00674       0.0271       0.0418        0.102        0.109
     41    90       0.0131      0.00994      0.00313       0.0209       0.0331       0.0878        0.113
     41   100      0.00962      0.00587      0.00375       0.0174       0.0254        0.109         0.12
     41   110       0.0184       0.0162      0.00225       0.0303       0.0422       0.0589       0.0795
     41   120       0.0248      0.00693       0.0179       0.0184       0.0276        0.111        0.178
     41   130       0.0184      0.00216       0.0163      0.00975       0.0154        0.213        0.265
     41   140       0.0633       0.0112       0.0521       0.0226       0.0351        0.487        0.543
     41   150       0.0951      0.00211        0.093      0.00652       0.0152        0.482        0.571
     41   160       0.0274      0.00678       0.0206       0.0184       0.0273        0.299        0.337
     41   170       0.0378       0.0276       0.0102       0.0373       0.0552        0.248        0.268
     41   180       0.0245       0.0165      0.00798       0.0278       0.0426        0.168        0.189
     41   190       0.0193       0.0105      0.00879       0.0186        0.034        0.188        0.246
     41   200      0.00541     0.000256      0.00516      0.00354      0.00531        0.123        0.123
     41   210       0.0492       0.0457       0.0035       0.0536        0.071          0.1        0.157
     41   220        0.012       0.0107      0.00136       0.0233       0.0343       0.0919        0.098
     41   230      0.00884      0.00579      0.00305       0.0156       0.0253        0.115        0.146
     41   231       0.0142       0.0137     0.000495       0.0331       0.0389       0.0295       0.0295

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     41     7       0.0237       0.0159      0.00777       0.0297       0.0419        0.137        0.179


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              41 2099.632    0.005       0.0204         1.15         1.17        0.026       0.0474        0.292         1.45
! Validation         41 2099.632    0.005       0.0136         3.12         3.14       0.0229       0.0382         0.41         2.45
Wall time: 2099.633363209665

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     42    10       0.0416       0.0296        0.012       0.0416       0.0571        0.172        0.277
     42    20        0.184       0.0175        0.166       0.0325       0.0439        0.825        0.884
     42    30       0.0431       0.0129       0.0303       0.0276       0.0377        0.355        0.356
     42    40       0.0512       0.0464      0.00486       0.0531       0.0715        0.116        0.125
     42    50       0.0639       0.0554      0.00845       0.0587       0.0782         0.13        0.159
     42    60       0.0217       0.0193       0.0024       0.0271       0.0461        0.071       0.0952
     42    70       0.0276       0.0244      0.00317       0.0383       0.0518         0.12        0.123
     42    80      0.00926      0.00828      0.00098       0.0164       0.0302       0.0657       0.0746
     42    90       0.0114       0.0113     0.000102       0.0262       0.0353       0.0172       0.0246
     42   100       0.0479      0.00917       0.0388       0.0256       0.0318        0.313        0.329
     42   110       0.0406       0.0142       0.0264        0.026       0.0396        0.196        0.216
     42   120       0.0332       0.0139       0.0193       0.0217       0.0392        0.172        0.184
     42   130        0.017      0.00726      0.00971       0.0176       0.0283        0.232        0.258
     42   140       0.0569       0.0201       0.0368       0.0331       0.0471        0.376        0.395
     42   150       0.0345       0.0153       0.0192       0.0268        0.041        0.246        0.307
     42   160        0.026       0.0189      0.00706       0.0314       0.0456        0.155        0.185
     42   170       0.0293       0.0185       0.0108       0.0267       0.0452        0.221        0.264
     42   180       0.0139       0.0053      0.00857       0.0153       0.0242        0.131        0.141
     42   190       0.0289       0.0143       0.0146       0.0196       0.0397        0.204        0.226
     42   200       0.0312       0.0302      0.00102       0.0438       0.0577       0.0413       0.0445
     42   210       0.0134      0.00873      0.00471        0.017        0.031        0.141        0.169
     42   220      0.00262      0.00174     0.000886       0.0103       0.0138       0.0583       0.0723
     42   230       0.0139      0.00813      0.00579       0.0161       0.0299        0.155        0.183
     42   231       0.0254       0.0228      0.00259       0.0406       0.0501       0.0676       0.0676

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     42     7       0.0227       0.0141      0.00864       0.0288       0.0394        0.145         0.19


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              42 2150.261    0.005        0.022         1.16         1.18       0.0266       0.0492        0.306         1.45
! Validation         42 2150.261    0.005       0.0135         3.12         3.13        0.023       0.0383        0.421         2.45
Wall time: 2150.262056272477

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     43    10       0.0145      0.00864      0.00588       0.0158       0.0309        0.157        0.193
     43    20       0.0146      0.00676      0.00789       0.0139       0.0273         0.14        0.173
     43    30       0.0232       0.0154      0.00778       0.0324       0.0412        0.134        0.145
     43    40       0.0637       0.0329       0.0307       0.0381       0.0602        0.324        0.334
     43    50       0.0205      0.00824       0.0123       0.0172       0.0301        0.198        0.235
     43    60       0.0179      0.00739       0.0105       0.0141       0.0285         0.11        0.136
     43    70       0.0186      0.00971      0.00892       0.0203       0.0327        0.164        0.202
     43    80      0.00916      0.00218      0.00698       0.0105       0.0155        0.133        0.146
     43    90      0.00312       0.0025     0.000624       0.0124       0.0166       0.0415       0.0455
     43   100       0.0738      0.00109       0.0727      0.00801       0.0109        0.437        0.453
     43   110       0.0745       0.0478       0.0267       0.0498       0.0726        0.273        0.273
     43   120       0.0475       0.0387      0.00883       0.0468       0.0653        0.177        0.215
     43   130       0.0759       0.0463       0.0296       0.0414       0.0714        0.329        0.403
     43   140        0.237        0.217       0.0194       0.0872        0.155        0.213        0.218
     43   150        0.119       0.0926       0.0263       0.0706        0.101        0.362          0.4
     43   160        0.123       0.0638       0.0591         0.05       0.0839        0.291        0.337
     43   170       0.0882       0.0241       0.0642       0.0347       0.0515        0.582        0.611
     43   180       0.0372       0.0175       0.0197       0.0239       0.0439        0.231        0.271
     43   190       0.0186       0.0104      0.00818       0.0213       0.0339        0.125        0.134
     43   200       0.0413      0.00605       0.0352       0.0182       0.0258        0.161         0.25
     43   210       0.0348       0.0329      0.00185       0.0346       0.0602       0.0589       0.0701
     43   220       0.0212       0.0202      0.00107       0.0273       0.0471       0.0654        0.087
     43   230      0.00961       0.0093     0.000306       0.0212        0.032       0.0197       0.0233
     43   231      0.00355     0.000145      0.00341      0.00354      0.00399       0.0775       0.0775

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     43     7        0.037       0.0285      0.00853       0.0383        0.056        0.137        0.188


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              43 2200.984    0.005       0.0415         1.16          1.2       0.0338       0.0667        0.298         1.45
! Validation         43 2200.984    0.005       0.0312         3.12         3.15       0.0302       0.0564         0.42         2.45
Wall time: 2200.985057574697

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     44    10       0.0153      0.00519       0.0101       0.0128       0.0239        0.137        0.158
     44    20       0.0109      0.00844      0.00248       0.0188       0.0305       0.0975        0.131
     44    30       0.0771      0.00565       0.0715       0.0151        0.025        0.528        0.559
     44    40       0.0469       0.0305       0.0164        0.045        0.058        0.286        0.309
     44    50       0.0228       0.0168      0.00605        0.031        0.043         0.12        0.128
     44    60       0.0494       0.0395      0.00984       0.0307        0.066       0.0991        0.132
     44    70      0.00982     0.000188      0.00963      0.00335      0.00455        0.156        0.163
     44    80       0.0214        0.018      0.00344       0.0288       0.0445       0.0942        0.147
     44    90         0.03       0.0293     0.000714       0.0394       0.0568       0.0403       0.0624
     44   100        0.117       0.0932       0.0237       0.0491        0.101        0.146        0.208
     44   110       0.0318       0.0229      0.00882       0.0334       0.0503        0.177        0.211
     44   120        0.158       0.0424        0.116       0.0429       0.0683        0.571        0.626
     44   130        0.132       0.0385       0.0937       0.0465       0.0652        0.341        0.412
     44   140       0.0207       0.0105       0.0102       0.0185        0.034        0.168        0.203
     44   150       0.0537        0.027       0.0267       0.0375       0.0546        0.281        0.301
     44   160       0.0597     0.000846       0.0589      0.00587      0.00966        0.538        0.555
     44   170       0.0675      0.00213       0.0654      0.00917       0.0153        0.337         0.34
     44   180       0.0341      0.00061       0.0335      0.00593       0.0082        0.306        0.328
     44   190       0.0301       0.0204      0.00972       0.0357       0.0474        0.206        0.237
     44   200       0.0208       0.0115       0.0093       0.0236       0.0357        0.164        0.203
     44   210       0.0114       0.0023      0.00908      0.00911       0.0159        0.124        0.129
     44   220       0.0443       0.0298       0.0145        0.033       0.0573         0.21        0.315
     44   230      0.00609      0.00213      0.00396      0.00906       0.0153        0.115        0.125
     44   231       0.0151       0.0139      0.00128       0.0322       0.0391       0.0949       0.0949

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     44     7       0.0232       0.0151      0.00806         0.03       0.0408        0.136        0.186


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              44 2251.679    0.005       0.0395         1.15         1.19       0.0309       0.0665        0.299         1.45
! Validation         44 2251.679    0.005       0.0149         3.12         3.14       0.0244       0.0399        0.412         2.45
Wall time: 2251.6797848325223

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     45    10        0.009      0.00483      0.00416       0.0121       0.0231        0.101        0.104
     45    20       0.0407        0.036      0.00471       0.0446        0.063        0.119        0.182
     45    30       0.0156       0.0138      0.00179       0.0266       0.0391       0.0558       0.0667
     45    40         0.02       0.0168      0.00321        0.027        0.043         0.12        0.131
     45    50       0.0125      0.00399      0.00851       0.0122        0.021        0.159        0.181
     45    60       0.0462      0.00684       0.0393       0.0181       0.0275        0.188        0.267
     45    70       0.0135       0.0095      0.00405       0.0192       0.0324        0.103        0.109
     45    80       0.0129       0.0114      0.00146       0.0175       0.0355       0.0686       0.0738
     45    90         0.03       0.0292     0.000863       0.0334       0.0567       0.0752        0.078
     45   100         54.5      0.00134         54.5      0.00902       0.0121         4.91          9.8
     45   110       0.0912      0.00726       0.0839       0.0156       0.0283         0.57        0.601
     45   120       0.0367       0.0208       0.0159        0.033       0.0479        0.214        0.224
     45   130       0.0205       0.0149       0.0056       0.0257       0.0405         0.11        0.119
     45   140        0.226        0.114        0.112       0.0642        0.112        0.785        0.835
     45   150       0.0983       0.0575       0.0408        0.047       0.0796        0.369        0.407
     45   160        0.024      0.00135       0.0227      0.00819       0.0122        0.198        0.208
     45   170       0.0111      0.00565       0.0054       0.0161        0.025        0.112        0.113
     45   180       0.0233       0.0185      0.00486       0.0276       0.0451       0.0822       0.0937
     45   190       0.0641        0.011       0.0531       0.0242       0.0348        0.474        0.523
     45   200        0.032       0.0237      0.00834       0.0376       0.0511        0.197        0.242
     45   210       0.0315       0.0151       0.0164       0.0267       0.0407         0.26        0.313
     45   220       0.0306       0.0216        0.009       0.0269       0.0487        0.188        0.225
     45   230       0.0317       0.0195       0.0122       0.0314       0.0463        0.169        0.205
     45   231       0.0222     5.43e-05       0.0222      0.00214      0.00245        0.198        0.198

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     45     7       0.0271       0.0164       0.0107        0.029       0.0425        0.171        0.213


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              45 2302.397    0.005       0.0211         1.16         1.18       0.0262       0.0486        0.293         1.45
! Validation         45 2302.397    0.005       0.0131         3.12         3.13       0.0222       0.0374        0.444         2.45
Wall time: 2302.3983904672787

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     46    10       0.0122      0.00487      0.00734       0.0154       0.0232        0.199        0.208
     46    20      0.00998      0.00668       0.0033       0.0152       0.0271         0.13        0.153
     46    30       0.0185       0.0163      0.00217       0.0316       0.0424       0.0591       0.0682
     46    40       0.0637        0.023       0.0407       0.0295       0.0503         0.27        0.277
     46    50       0.0743       0.0469       0.0274       0.0486       0.0719        0.358        0.383
     46    60       0.0339       0.0255       0.0084       0.0408        0.053        0.139        0.146
     46    70       0.0349       0.0248       0.0101       0.0368       0.0522        0.167        0.211
     46    80       0.0521       0.0433      0.00883       0.0524       0.0691       0.0916        0.127
     46    90       0.0109      0.00652      0.00439       0.0148       0.0268        0.137        0.162
     46   100        0.014       0.0116      0.00239       0.0246       0.0358        0.112        0.117
     46   110        0.027       0.0255      0.00149       0.0367        0.053       0.0774       0.0861
     46   120       0.0291       0.0259       0.0032       0.0411       0.0534        0.137         0.15
     46   130       0.0927       0.0189       0.0738       0.0291       0.0457        0.359        0.361
     46   140        0.046     0.000118       0.0459      0.00263      0.00361        0.415        0.444
     46   150       0.0292       0.0168       0.0124       0.0246       0.0431         0.19        0.252
     46   160       0.0149      0.00838      0.00653       0.0222       0.0304        0.108        0.152
     46   170       0.0322        0.029      0.00317       0.0458       0.0565       0.0545       0.0755
     46   180       0.0114      0.00205      0.00936      0.00802        0.015        0.129        0.132
     46   190       0.0371       0.0305      0.00666       0.0372       0.0579        0.133        0.142
     46   200       0.0349       0.0304      0.00451       0.0328       0.0579        0.131        0.166
     46   210       0.0066      0.00437      0.00223       0.0123       0.0219       0.0824        0.107
     46   220       0.0858        0.023       0.0628       0.0306       0.0503        0.488         0.51
     46   230       0.0313       0.0103       0.0209       0.0237       0.0338        0.327        0.384
     46   231      0.00537      0.00483     0.000543       0.0176       0.0231       0.0309       0.0309

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     46     7       0.0249       0.0144       0.0105       0.0265       0.0399        0.165        0.204


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              46 2353.202    0.005       0.0234         1.15         1.17       0.0278       0.0508        0.282         1.44
! Validation         46 2353.202    0.005        0.013         3.12         3.13       0.0223       0.0376        0.437         2.45
Wall time: 2353.203229549341

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     47    10       0.0348       0.0208        0.014       0.0324       0.0479        0.148        0.166
     47    20       0.0551       0.0275       0.0277       0.0379        0.055        0.176        0.222
     47    30       0.0585        0.049      0.00945       0.0426       0.0735        0.113        0.129
     47    40       0.0225       0.0169      0.00557       0.0336       0.0432        0.155        0.173
     47    50       0.0238       0.0197      0.00404       0.0302       0.0466       0.0777       0.0844
     47    60       0.0719       0.0517       0.0202       0.0502       0.0755        0.309        0.317
     47    70       0.0931       0.0368       0.0563       0.0455       0.0637        0.485        0.613
     47    80        0.327        0.304        0.023       0.0837        0.183        0.197        0.222
     47    90       0.0413        0.036      0.00523       0.0437        0.063        0.145        0.189
     47   100        0.013      0.00647      0.00658       0.0156       0.0267          0.1        0.108
     47   110        0.038      0.00782       0.0301       0.0144       0.0294        0.326        0.337
     47   120       0.0819       0.0245       0.0575        0.034        0.052        0.396        0.423
     47   130        0.101       0.0346       0.0661       0.0349       0.0617        0.298        0.341
     47   140       0.0175       0.0088      0.00865       0.0184       0.0311        0.179        0.188
     47   150      0.00735      0.00137      0.00598      0.00732       0.0123        0.113        0.124
     47   160       0.0288       0.0153       0.0135       0.0232        0.041        0.299        0.309
     47   170       0.0604      0.00222       0.0582      0.00733       0.0156        0.309         0.32
     47   180       0.0251       0.0113       0.0137       0.0263       0.0353        0.128        0.157
     47   190       0.0491        0.029         0.02        0.032       0.0566        0.222        0.281
     47   200       0.0123      0.00786      0.00448       0.0206       0.0294       0.0737       0.0949
     47   210       0.0143        0.005      0.00929       0.0156       0.0235        0.175        0.226
     47   220       0.0314      0.00958       0.0218       0.0151       0.0325        0.242        0.258
     47   230       0.0108      0.00811      0.00266       0.0221       0.0299        0.115        0.135
     47   231       0.0466       0.0293       0.0173         0.04       0.0568        0.349        0.349

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     47     7       0.0263       0.0164      0.00995       0.0291       0.0425        0.161        0.204


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              47 2403.966    0.005        0.029         1.15         1.18       0.0298       0.0559        0.307         1.44
! Validation         47 2403.966    0.005       0.0141         3.12         3.13        0.023       0.0387        0.426         2.45
Wall time: 2403.966817582026

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     48    10       0.0239      0.00551       0.0184       0.0141       0.0246        0.156         0.18
     48    20        0.165        0.089       0.0757        0.066        0.099        0.322        0.366
     48    30        0.144        0.132       0.0121       0.0868        0.121        0.124        0.146
     48    40       0.0214         0.01       0.0113       0.0221       0.0333        0.236        0.283
     48    50       0.0621       0.0396       0.0226       0.0437        0.066        0.285        0.389
     48    60       0.0214       0.0127      0.00867       0.0266       0.0374        0.213        0.227
     48    70       0.0285        0.027      0.00143       0.0431       0.0546       0.0764       0.0985
     48    80       0.0135       0.0101      0.00343        0.017       0.0333        0.108        0.126
     48    90      0.00933      0.00731      0.00202       0.0164       0.0284       0.0728       0.0747
     48   100       0.0106      0.00888       0.0017       0.0215       0.0313       0.0723        0.106
     48   110       0.0087      0.00787     0.000831       0.0219       0.0295       0.0677       0.0751
     48   120       0.0738       0.0537       0.0201       0.0539       0.0769         0.16         0.21
     48   130       0.0868       0.0265       0.0603       0.0361        0.054        0.408        0.434
     48   140       0.0426      0.00278       0.0398       0.0102       0.0175        0.406        0.443
     48   150       0.0324       0.0286      0.00376       0.0397       0.0562       0.0785       0.0932
     48   160        0.256     0.000248        0.256      0.00359      0.00523         0.86         1.34
     48   170       0.0345       0.0315        0.003       0.0414       0.0589       0.0661       0.0756
     48   180       0.0139       0.0105       0.0034       0.0211       0.0341        0.111        0.133
     48   190       0.0175       0.0168     0.000718       0.0335        0.043       0.0466       0.0505
     48   200       0.0897       0.0379       0.0518       0.0428       0.0646        0.366        0.441
     48   210       0.0486       0.0143       0.0343        0.027       0.0397        0.283        0.288
     48   220        0.101       0.0178       0.0834       0.0309       0.0443        0.372        0.439
     48   230       0.0245      0.00655       0.0179        0.015       0.0269        0.247        0.252
     48   231       0.0145      0.00191       0.0126       0.0105       0.0145        0.149        0.149

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     48     7       0.0286       0.0159       0.0126        0.031       0.0419         0.18         0.23


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              48 2454.753    0.005       0.0402         1.16          1.2       0.0319       0.0648        0.295         1.45
! Validation         48 2454.753    0.005        0.013         3.12         3.13       0.0224       0.0374        0.467         2.45
Wall time: 2454.753696056083
! Best model       48    3.129

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     49    10        0.022       0.0175      0.00454       0.0275       0.0439        0.145        0.156
     49    20       0.0567       0.0555       0.0012       0.0484       0.0782       0.0657       0.0861
     49    30        0.122       0.0935       0.0283       0.0763        0.102        0.186        0.223
     49    40       0.0553       0.0537      0.00159       0.0568        0.077        0.052       0.0597
     49    50       0.0891         0.04       0.0491       0.0421       0.0664        0.514        0.537
     49    60       0.0556       0.0326        0.023       0.0453       0.0599        0.358        0.403
     49    70       0.0437       0.0299       0.0137       0.0391       0.0574        0.147         0.16
     49    80       0.0344       0.0316      0.00281       0.0416        0.059          0.1        0.117
     49    90       0.0153      0.00629      0.00896       0.0138       0.0263        0.157        0.196
     49   100       0.0937       0.0114       0.0822       0.0253       0.0355         0.76        0.762
     49   110       0.0413       0.0153        0.026       0.0198        0.041        0.262        0.323
     49   120       0.0586       0.0514      0.00721       0.0564       0.0752        0.145         0.16
     49   130       0.0566       0.0517      0.00486       0.0534       0.0755        0.078        0.094
     49   140       0.0311       0.0186       0.0125       0.0285       0.0453        0.227        0.261
     49   150       0.0613        0.012       0.0494       0.0266       0.0363        0.427        0.461
     49   160        0.177      0.00919        0.168       0.0239       0.0318        0.728         1.09
     49   170       0.0344       0.0174        0.017       0.0365       0.0438        0.261        0.328
     49   180       0.0502       0.0488      0.00138       0.0563       0.0734       0.0742       0.0925
     49   190       0.0149      0.00498      0.00996       0.0127       0.0234        0.162        0.225
     49   200       0.0136      0.00687      0.00672       0.0195       0.0275       0.0784        0.109
     49   210       0.0477       0.0118        0.036       0.0238        0.036         0.36        0.499
     49   220       0.0114       0.0105     0.000849       0.0231        0.034       0.0634       0.0645
     49   230       0.0123      0.00759      0.00476       0.0173       0.0289        0.121        0.123
     49   231        0.072       0.0694      0.00263       0.0656       0.0874        0.136        0.136

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     49     7       0.0225       0.0135      0.00901       0.0268       0.0386        0.148        0.194


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              49 2505.519    0.005       0.0358         1.15         1.19       0.0306       0.0619        0.309         1.45
! Validation         49 2505.519    0.005        0.012         3.12         3.13       0.0217       0.0359         0.42         2.45
Wall time: 2505.519787014462

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     50    10       0.0231       0.0155      0.00764       0.0278       0.0413        0.148        0.211
     50    20        0.155      0.00662        0.149        0.014        0.027        0.354        0.524
     50    30      0.00668      0.00451      0.00216       0.0135       0.0223        0.083       0.0912
     50    40        0.123       0.0426       0.0802       0.0469       0.0686         0.57        0.612
     50    50       0.0564        0.037       0.0194       0.0451       0.0638         0.27        0.313
     50    60        0.105        0.097      0.00788       0.0741        0.103        0.117        0.118
     50    70       0.0192      0.00508       0.0141       0.0125       0.0237        0.148        0.158
     50    80       0.0357       0.0326      0.00311       0.0401       0.0599         0.11        0.119
     50    90        0.104       0.0256       0.0788       0.0339       0.0531        0.644         0.66
     50   100       0.0512       0.0263       0.0249       0.0379       0.0539        0.226        0.235
     50   110       0.0652       0.0158       0.0493       0.0337       0.0418        0.326        0.338
     50   120       0.0418      0.00787       0.0339       0.0166       0.0295        0.242        0.262
     50   130        0.053       0.0193       0.0338       0.0296       0.0461        0.287        0.319
     50   140       0.0718       0.0557       0.0162       0.0413       0.0783        0.222        0.262
     50   150       0.0189      0.00955      0.00939       0.0208       0.0324        0.119        0.136
     50   160       0.0123      0.00754      0.00481       0.0201       0.0288        0.162        0.184
     50   170        0.054       0.0478      0.00626        0.035       0.0726        0.134        0.176
     50   180       0.0193       0.0109      0.00837       0.0206       0.0347        0.105        0.122
     50   190       0.0136       0.0102      0.00335       0.0171       0.0336       0.0887       0.0958
     50   200       0.0208       0.0202     0.000615       0.0356       0.0472       0.0455       0.0529
     50   210       0.0221       0.0201      0.00203       0.0315        0.047       0.0587       0.0652
     50   220       0.0326       0.0224       0.0102       0.0231       0.0496        0.196        0.237
     50   230       0.0161       0.0151     0.000933       0.0287       0.0408       0.0695       0.0811
     50   231      0.00525      0.00429     0.000959       0.0161       0.0218       0.0411       0.0411

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     50     7       0.0186       0.0119      0.00669       0.0248       0.0363        0.128        0.167


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              50 2556.330    0.005       0.0246         1.14         1.17       0.0277       0.0526        0.286         1.44
! Validation         50 2556.330    0.005       0.0115         3.13         3.14       0.0209       0.0351        0.391         2.45
Wall time: 2556.3315273104236

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     51    10       0.0725      0.00448        0.068       0.0134       0.0222        0.525         0.56
     51    20       0.0498      0.00574       0.0441       0.0168       0.0252        0.472        0.507
     51    30       0.0433       0.0257       0.0175       0.0369       0.0532        0.167        0.176
     51    40       0.0576       0.0478      0.00983       0.0425       0.0726        0.151        0.172
     51    50       0.0154      0.00604      0.00933       0.0163       0.0258         0.12        0.128
     51    60       0.0206       0.0151       0.0054       0.0285       0.0409        0.142        0.185
     51    70       0.0183       0.0164      0.00186       0.0295       0.0425       0.0452       0.0573
     51    80       0.0169      0.00891      0.00797       0.0199       0.0313        0.133        0.135
     51    90       0.0966       0.0397       0.0569       0.0347       0.0662        0.253        0.322
     51   100       0.0255       0.0198      0.00574       0.0247       0.0467       0.0977        0.101
     51   110       0.0291       0.0134       0.0156       0.0215       0.0385        0.241        0.313
     51   120       0.0586       0.0137       0.0449       0.0161       0.0389        0.502         0.53
     51   130       0.0174     0.000754       0.0166      0.00687      0.00912        0.238         0.27
     51   140      0.00878      0.00297      0.00581      0.00977       0.0181       0.0903        0.104
     51   150       0.0219       0.0147      0.00716       0.0255       0.0403        0.103        0.114
     51   160       0.0105     0.000536      0.00996      0.00435      0.00769        0.156         0.16
     51   170       0.0253        0.022      0.00334       0.0345       0.0492       0.0669       0.0802
     51   180        0.015       0.0103      0.00477       0.0195       0.0336       0.0868       0.0986
     51   190       0.0171       0.0135      0.00366       0.0275       0.0385        0.117        0.119
     51   200       0.0316       0.0241      0.00758       0.0415       0.0515        0.168        0.174
     51   210       0.0119      0.00949      0.00244       0.0195       0.0323       0.0856       0.0893
     51   220       0.0195     0.000648       0.0189      0.00515      0.00845        0.215        0.218
     51   230       0.0561       0.0041        0.052        0.012       0.0213        0.519        0.531
     51   231        0.102       0.0636       0.0384        0.057       0.0837         0.52         0.52

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     51     7       0.0174       0.0112       0.0062       0.0245       0.0351        0.119        0.157


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              51 2607.088    0.005       0.0225         1.15         1.17       0.0261       0.0494        0.272         1.44
! Validation         51 2607.088    0.005       0.0106         3.13         3.14       0.0203       0.0338        0.392         2.45
Wall time: 2607.0891409777105

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     52    10        0.984        0.816        0.167        0.203          0.3        0.816        0.881
     52    20         0.38       0.0498        0.331       0.0505       0.0741         1.13         1.33
     52    30       0.0756        0.034       0.0416        0.044       0.0613        0.419        0.426
     52    40       0.0169      0.00228       0.0146      0.00845       0.0159        0.241        0.301
     52    50       0.0374       0.0344      0.00303        0.042       0.0616        0.105        0.124
     52    60        0.033       0.0118       0.0212       0.0242        0.036        0.211        0.212
     52    70       0.0222       0.0183      0.00392       0.0329       0.0449       0.0769       0.0837
     52    80      0.00895       0.0055      0.00345        0.018       0.0246        0.055        0.078
     52    90       0.0231       0.0212      0.00189       0.0348       0.0483       0.0905        0.104
     52   100       0.0343       0.0193        0.015       0.0322       0.0462        0.143        0.186
     52   110       0.0366       0.0337      0.00285       0.0386        0.061       0.0685       0.0709
     52   120      0.00538      0.00459     0.000794       0.0168       0.0225        0.047       0.0527
     52   130       0.0307       0.0274      0.00335       0.0247       0.0549        0.107        0.148
     52   140       0.0162      0.00792      0.00824       0.0163       0.0295       0.0986        0.133
     52   150      0.00877      0.00628      0.00249       0.0174       0.0263       0.0721       0.0754
     52   160      0.00645      0.00553     0.000916        0.015       0.0247       0.0686       0.0804
     52   170       0.0103      0.00873      0.00159       0.0202        0.031        0.089       0.0932
     52   180        0.104        0.014       0.0904       0.0224       0.0393        0.701        0.783
     52   190       0.0684       0.0222       0.0462       0.0265       0.0495         0.35        0.414
     52   200         0.25        0.229       0.0206        0.122        0.159        0.237        0.339
     52   210       0.0614       0.0403       0.0211       0.0453       0.0667        0.282        0.295
     52   220       0.0628       0.0503       0.0125       0.0518       0.0744        0.181        0.239
     52   230       0.0939       0.0224       0.0715       0.0341       0.0497        0.228        0.355
     52   231       0.0091      0.00119      0.00791      0.00908       0.0114        0.236        0.236

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     52     7       0.0276       0.0151       0.0126        0.029       0.0407        0.186        0.231


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              52 2657.833    0.005       0.0425         1.14         1.18       0.0315       0.0658        0.308         1.44
! Validation         52 2657.833    0.005       0.0119         3.12         3.13       0.0222       0.0356        0.457         2.45
Wall time: 2657.834293594584
! Best model       52    3.128

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     53    10       0.0118      0.00402      0.00777      0.00898        0.021        0.136        0.166
     53    20       0.0197       0.0187     0.000962        0.035       0.0454       0.0721       0.0822
     53    30       0.0773      0.00749       0.0698       0.0202       0.0287        0.538        0.585
     53    40        0.101     0.000134        0.101      0.00244      0.00385         0.53        0.564
     53    50        0.286       0.0964        0.189       0.0713        0.103        0.774         1.09
     53    60       0.0789       0.0546       0.0243        0.043       0.0776        0.316        0.382
     53    70       0.0765       0.0252       0.0514       0.0357       0.0527        0.375        0.397
     53    80         1.59         1.17        0.426        0.159        0.359         1.08          1.6
     53    90       0.0406       0.0194       0.0212       0.0322       0.0463        0.343        0.387
     53   100       0.0147      0.00315       0.0115       0.0124       0.0186        0.121        0.146
     53   110       0.0138      0.00945       0.0044       0.0236       0.0323        0.113        0.116
     53   120       0.0185       0.0138      0.00466       0.0264        0.039        0.153        0.181
     53   130       0.0118       0.0041      0.00767       0.0139       0.0213        0.146        0.161
     53   140       0.0107      0.00479      0.00586        0.014        0.023        0.175        0.203
     53   150       0.0055     0.000828      0.00467       0.0052      0.00955        0.151        0.173
     53   160          120       0.0913          120        0.054          0.1         7.32         14.5
     53   170        0.144       0.0833       0.0607       0.0697       0.0958        0.323        0.327
     53   180       0.0542       0.0373        0.017       0.0427       0.0641        0.207        0.212
     53   190        0.036     0.000228       0.0358       0.0026      0.00501        0.435        0.448
     53   200       0.0221       0.0119       0.0102       0.0199       0.0362        0.208        0.234
     53   210       0.0129      0.00354      0.00941       0.0104       0.0198        0.135        0.145
     53   220       0.0869       0.0166       0.0702       0.0311       0.0428        0.427        0.679
     53   230       0.0131      0.00938      0.00373         0.02       0.0321       0.0691       0.0811
     53   231      0.00982     5.16e-05      0.00977      0.00204      0.00238        0.131        0.131

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     53     7       0.0269       0.0128       0.0142       0.0267       0.0375        0.187        0.241


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              53 2708.659    0.005       0.0366         1.15         1.19       0.0312       0.0618        0.311         1.45
! Validation         53 2708.659    0.005       0.0134         3.11         3.13       0.0233       0.0382        0.477         2.45
Wall time: 2708.6597883990034
! Best model       53    3.127

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     54    10      0.00499      0.00396      0.00103       0.0154       0.0209       0.0398       0.0444
     54    20       0.0192       0.0181       0.0011       0.0314       0.0446       0.0605       0.0881
     54    30       0.0304        0.027      0.00336       0.0418       0.0546       0.0766       0.0827
     54    40      0.00989      0.00894     0.000956       0.0172       0.0314       0.0458       0.0516
     54    50      0.00483       0.0038      0.00103        0.012       0.0205       0.0435         0.05
     54    60        0.127        0.018        0.109       0.0308       0.0445        0.783        0.821
     54    70       0.0647        0.033       0.0318        0.035       0.0603        0.316        0.368
     54    80       0.0975       0.0409       0.0565       0.0397       0.0672        0.379        0.407
     54    90        0.232       0.0282        0.204       0.0444       0.0557        0.384         0.61
     54   100       0.0391        0.026       0.0131       0.0323       0.0535         0.18        0.192
     54   110       0.0118      0.00554      0.00629       0.0154       0.0247        0.131        0.144
     54   120        0.019       0.0156      0.00345       0.0314       0.0414         0.11        0.114
     54   130      0.00953      0.00443       0.0051         0.01       0.0221        0.156         0.19
     54   140       0.0114      0.00927      0.00213       0.0207        0.032       0.0617       0.0724
     54   150       0.0209       0.0153      0.00557       0.0327        0.041        0.143        0.198
     54   160       0.0166       0.0135      0.00304       0.0221       0.0386        0.124        0.137
     54   170       0.0128       0.0113      0.00147       0.0261       0.0353        0.074       0.0928
     54   180      0.00206     0.000714      0.00135       0.0061      0.00887       0.0746        0.086
     54   190        0.139        0.012        0.127       0.0202       0.0363        0.727        0.791
     54   200        0.127       0.0689       0.0581       0.0573       0.0871        0.388        0.573
     54   210       0.0401       0.0131        0.027       0.0207       0.0381          0.3        0.358
     54   220        0.198       0.0584        0.139       0.0377       0.0802        0.398        0.522
     54   230       0.0166      0.00574       0.0109       0.0141       0.0251        0.119        0.138
     54   231       0.0128     0.000626       0.0122      0.00654      0.00831        0.293        0.293

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     54     7       0.0338        0.019       0.0148       0.0314       0.0458        0.198        0.248


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              54 2759.427    0.005       0.0345         1.16         1.19       0.0287       0.0615        0.295         1.46
! Validation         54 2759.427    0.005       0.0138         3.11         3.13       0.0232       0.0379        0.483         2.45
Wall time: 2759.4283813238144

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     55    10        0.052       0.0394       0.0126       0.0454       0.0659        0.183        0.205
     55    20       0.0128      0.00897      0.00383       0.0156       0.0314       0.0652       0.0921
     55    30       0.0727       0.0547        0.018        0.044       0.0776        0.171        0.178
     55    40        0.117        0.021       0.0965       0.0225       0.0481        0.597        0.629
     55    50        0.225        0.166       0.0592       0.0871        0.135        0.392        0.405
     55    60        0.186        0.171        0.015       0.0822        0.137        0.173        0.184
     55    70       0.0565       0.0491      0.00735       0.0537       0.0736        0.125        0.151
     55    80       0.0427       0.0394      0.00327       0.0469       0.0659       0.0996        0.119
     55    90       0.0206       0.0203     0.000238       0.0301       0.0474       0.0407        0.041
     55   100       0.0178       0.0133      0.00445       0.0253       0.0383        0.109        0.177
     55   110       0.0166       0.0143      0.00224        0.028       0.0397       0.0821        0.097
     55   120       0.0798       0.0765      0.00329       0.0581       0.0918        0.117        0.149
     55   130       0.0141       0.0129       0.0012       0.0218       0.0377       0.0694       0.0783
     55   140       0.0217       0.0198      0.00186       0.0278       0.0468       0.0895        0.106
     55   150      0.00827      0.00605      0.00222       0.0121       0.0258       0.0775       0.0807
     55   160          118       0.0877          118       0.0505       0.0983          7.6         14.4
     55   170        0.204      0.00721        0.197       0.0179       0.0282        0.856        0.884
     55   180       0.0855        0.065       0.0205       0.0586       0.0846        0.241        0.322
     55   190        0.148       0.0454        0.103       0.0494       0.0708        0.568        0.796
     55   200       0.0403       0.0269       0.0134       0.0338       0.0544        0.124        0.154
     55   210       0.0192       0.0161      0.00313       0.0305       0.0421         0.13        0.139
     55   220       0.0196      0.00801       0.0115       0.0197       0.0297        0.267        0.285
     55   230       0.0175      0.00861      0.00891       0.0156       0.0308        0.159          0.2
     55   231       0.0119     0.000446       0.0115      0.00572      0.00701        0.142        0.142

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     55     7       0.0354       0.0198       0.0156       0.0326       0.0467        0.202        0.254


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              55 2810.151    0.005       0.0605         1.15         1.21       0.0382       0.0791        0.323         1.46
! Validation         55 2810.151    0.005       0.0187         3.11         3.13       0.0262        0.045        0.491         2.45
Wall time: 2810.1521171815693

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     56    10           54      0.00589           54       0.0149       0.0255         4.97         9.76
     56    20       0.0432      0.00256       0.0407       0.0122       0.0168        0.305        0.308
     56    30        0.046       0.0204       0.0257       0.0298       0.0474        0.289        0.318
     56    40       0.0385       0.0075        0.031       0.0209       0.0287        0.244        0.317
     56    50        0.036       0.0306      0.00533       0.0373       0.0581        0.089        0.104
     56    60       0.0301       0.0231      0.00703       0.0294       0.0504        0.121        0.219
     56    70         84.9       0.0123         84.9       0.0221       0.0368         6.13         12.2
     56    80       0.0487       0.0117        0.037       0.0241       0.0359        0.438         0.47
     56    90       0.0427       0.0112       0.0315        0.027       0.0352        0.391        0.418
     56   100       0.0642       0.0156       0.0486       0.0309       0.0415        0.318        0.319
     56   110       0.0521        0.025       0.0271        0.036       0.0525        0.298        0.326
     56   120       0.0471       0.0412      0.00586       0.0508       0.0674        0.201        0.203
     56   130       0.0456       0.0397      0.00592       0.0505       0.0661       0.0793        0.102
     56   140       0.0258        0.018      0.00781       0.0373       0.0445        0.134        0.159
     56   150       0.0281       0.0184       0.0097       0.0342       0.0451        0.254        0.262
     56   160        0.021      0.00402        0.017       0.0108       0.0211        0.276        0.299
     56   170       0.0163      0.00496       0.0113       0.0133       0.0234        0.168        0.199
     56   180        0.079         0.01       0.0689       0.0211       0.0333        0.381        0.689
     56   190       0.0134        0.012      0.00137       0.0234       0.0364       0.0708       0.0815
     56   200       0.0104      0.00618      0.00425       0.0176       0.0261       0.0669       0.0868
     56   210       0.0215       0.0165      0.00506        0.024       0.0426        0.127        0.143
     56   220       0.0144      0.00846      0.00594       0.0183       0.0305        0.183        0.198
     56   230       0.0576       0.0542      0.00347       0.0564       0.0773        0.137        0.149
     56   231     0.000102     7.66e-05      2.5e-05      0.00248      0.00291      0.00664      0.00664

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     56     7       0.0216       0.0145      0.00702       0.0272         0.04        0.125        0.171


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              56 2860.946    0.005       0.0301         1.14         1.17       0.0272       0.0571        0.293         1.44
! Validation         56 2860.946    0.005       0.0136         3.12         3.14       0.0233       0.0388        0.405         2.45
Wall time: 2860.947115454823

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     57    10       0.0636       0.0574      0.00622       0.0531       0.0795        0.182        0.209
     57    20       0.0386       0.0367      0.00193       0.0355       0.0636        0.102        0.114
     57    30         0.11       0.0961       0.0144       0.0442        0.103        0.194        0.208
     57    40       0.0832       0.0398       0.0434       0.0395       0.0663        0.425        0.491
     57    50       0.0706       0.0371       0.0335       0.0444       0.0639        0.406        0.486
     57    60       0.0576       0.0296       0.0279       0.0307       0.0572        0.284        0.313
     57    70       0.0063      0.00183      0.00447      0.00957       0.0142        0.145        0.178
     57    80       0.0297       0.0229      0.00684       0.0372       0.0502        0.176        0.209
     57    90       0.0188       0.0105      0.00827       0.0231        0.034        0.125        0.132
     57   100       0.0299      0.00756       0.0223       0.0192       0.0289         0.24        0.249
     57   110       0.0208       0.0184      0.00241       0.0315        0.045       0.0592       0.0779
     57   120      0.00417     0.000874       0.0033      0.00622      0.00982        0.094        0.134
     57   130      0.00824      0.00673      0.00151       0.0185       0.0272       0.0388       0.0519
     57   140       0.0093      0.00873     0.000568       0.0231        0.031       0.0566       0.0625
     57   150        0.034        0.006        0.028       0.0169       0.0257        0.273        0.284
     57   160        0.122       0.0135        0.108       0.0261       0.0385        0.751        0.766
     57   170        0.126       0.0408       0.0849       0.0357        0.067        0.473        0.535
     57   180       0.0367       0.0163       0.0204       0.0259       0.0424        0.276        0.311
     57   190        0.265        0.249       0.0153       0.0724        0.166        0.176        0.229
     57   200        0.041       0.0298       0.0112       0.0473       0.0573        0.146        0.149
     57   210        0.027       0.0226      0.00439        0.037       0.0499        0.126        0.171
     57   220       0.0516       0.0402       0.0114       0.0544       0.0665        0.202        0.265
     57   230       0.0259       0.0231      0.00278       0.0316       0.0504       0.0859        0.123
     57   231       0.0219       0.0218     0.000148       0.0365        0.049       0.0323       0.0323

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     57     7        0.023       0.0125       0.0105       0.0254       0.0372        0.162        0.205


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              57 2911.652    0.005       0.0219         1.15         1.17       0.0264       0.0494        0.297         1.45
! Validation         57 2911.652    0.005       0.0112         3.12         3.13       0.0209       0.0351        0.441         2.45
Wall time: 2911.6535339895636

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     58    10       0.0281       0.0102       0.0179       0.0224       0.0336        0.285        0.305
     58    20       0.0347       0.0107        0.024       0.0248       0.0343        0.248         0.25
     58    30        0.219       0.0121        0.207        0.018       0.0365        0.784          1.2
     58    40       0.0294       0.0163        0.013       0.0236       0.0424        0.163        0.164
     58    50       0.0232        0.022      0.00126        0.034       0.0492       0.0699       0.0893
     58    60      0.00739       0.0042      0.00319        0.017       0.0215       0.0669       0.0805
     58    70       0.0205       0.0191      0.00141       0.0358       0.0458       0.0736       0.0832
     58    80      0.00808      0.00638       0.0017       0.0158       0.0265       0.0643       0.0697
     58    90      0.00779      0.00528       0.0025        0.016       0.0241       0.0771       0.0932
     58   100       0.0124        0.012     0.000448       0.0247       0.0363       0.0347       0.0381
     58   110       0.0188       0.0146       0.0042       0.0308       0.0401        0.106        0.132
     58   120       0.0189        0.016      0.00289       0.0304       0.0421       0.0808       0.0825
     58   130        0.021       0.0201     0.000996       0.0301        0.047       0.0637       0.0838
     58   140        0.019       0.0176      0.00139       0.0304        0.044       0.0469       0.0538
     58   150      0.00709      0.00451      0.00258       0.0115       0.0223       0.0782       0.0927
     58   160       0.0109       0.0097      0.00118       0.0202       0.0327       0.0525        0.073
     58   170      0.00921      0.00832      0.00089       0.0183       0.0303       0.0345       0.0396
     58   180        0.052       0.0201       0.0319       0.0318        0.047        0.299        0.323
     58   190        0.316       0.0284        0.288       0.0454        0.056         1.09         1.17
     58   200        0.108       0.0396       0.0683       0.0461       0.0661        0.661        0.694
     58   210       0.0325      0.00746        0.025       0.0159       0.0287        0.228        0.231
     58   220       0.0652       0.0231       0.0421       0.0279       0.0505        0.379        0.512
     58   230       0.0215       0.0101       0.0114         0.02       0.0334        0.167        0.227
     58   231       0.0149     0.000743       0.0142      0.00687      0.00905        0.158        0.158

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     58     7       0.0245       0.0138       0.0107        0.025       0.0391        0.175        0.207


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              58 2962.400    0.005       0.0263         1.15         1.17       0.0259       0.0525        0.289         1.45
! Validation         58 2962.400    0.005       0.0118         3.12         3.13        0.021       0.0354        0.441         2.45
Wall time: 2962.401342416182

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     59    10       0.0223       0.0169      0.00538       0.0268       0.0431       0.0915         0.11
     59    20       0.0127      0.00746      0.00523       0.0219       0.0287       0.0903        0.096
     59    30       0.0895       0.0165        0.073       0.0207       0.0426         0.45         0.71
     59    40      0.00583       0.0022      0.00363      0.00873       0.0156        0.138         0.16
     59    50       0.0529       0.0278       0.0251       0.0331       0.0554        0.297        0.315
     59    60       0.0469      0.00915       0.0378         0.02       0.0318        0.441        0.516
     59    70       0.0314       0.0111       0.0203       0.0182        0.035        0.327        0.348
     59    80       0.0239      0.00952       0.0144       0.0224       0.0324        0.235        0.248
     59    90       0.0155       0.0132      0.00231       0.0288       0.0381        0.073        0.089
     59   100       0.0101     0.000577      0.00956      0.00589      0.00798        0.123         0.13
     59   110      0.00954      0.00667      0.00287       0.0198       0.0271       0.0645       0.0771
     59   120         0.11       0.0124       0.0974        0.026       0.0369        0.512        0.537
     59   130        0.101       0.0267       0.0741       0.0343       0.0543        0.498        0.524
     59   140        0.189        0.165       0.0234       0.0974        0.135        0.222        0.232
     59   150       0.0261       0.0138       0.0123       0.0276        0.039        0.178        0.221
     59   160       0.0385       0.0235       0.0149       0.0358       0.0509        0.192        0.275
     59   170       0.0491       0.0126       0.0365       0.0225       0.0372        0.372        0.384
     59   180       0.0177      0.00999      0.00773       0.0244       0.0332         0.14        0.152
     59   190        0.019      0.00902      0.00996       0.0165       0.0315        0.213        0.224
     59   200      0.00784      0.00447      0.00337       0.0135       0.0222       0.0952        0.132
     59   210       0.0102      0.00821      0.00201        0.018       0.0301       0.0564       0.0624
     59   220      0.00864      0.00773     0.000913       0.0178       0.0292       0.0456       0.0496
     59   230      0.00451      0.00262      0.00189       0.0111        0.017       0.0865        0.111
     59   231      0.00862      0.00858     3.53e-05       0.0237       0.0308       0.0158       0.0158

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     59     7       0.0237       0.0131       0.0106       0.0239        0.038         0.18        0.215


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              59 3013.159    0.005       0.0223         1.14         1.17       0.0265       0.0494        0.294         1.44
! Validation         59 3013.159    0.005       0.0114         3.12         3.13       0.0214        0.035        0.446         2.45
Wall time: 3013.159302389249

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     60    10       0.0162       0.0144      0.00172       0.0255       0.0399       0.0718       0.0878
     60    20       0.0205       0.0203     0.000187       0.0215       0.0473        0.024       0.0253
     60    30      0.00396      0.00377     0.000193        0.012       0.0204       0.0276       0.0305
     60    40      0.00559       0.0048      0.00079       0.0134        0.023       0.0428       0.0574
     60    50       0.0267       0.0203      0.00638        0.024       0.0473        0.169        0.187
     60    60       0.0581     0.000353       0.0578      0.00236      0.00624        0.636        0.638
     60    70       0.0859        0.013       0.0729       0.0167       0.0378         0.42        0.495
     60    80       0.0421       0.0135       0.0286       0.0214       0.0386        0.246        0.246
     60    90       0.0285       0.0238      0.00472       0.0338       0.0512        0.074        0.102
     60   100      0.00854      0.00466      0.00388       0.0161       0.0227       0.0722       0.0884
     60   110       0.0205       0.0139      0.00657       0.0255       0.0392        0.131         0.16
     60   120        0.014       0.0114      0.00255        0.023       0.0355       0.0872        0.134
     60   130       0.0117      0.00935      0.00234       0.0216       0.0321       0.0532       0.0642
     60   140      0.00803      0.00743     0.000605         0.02       0.0286       0.0479       0.0496
     60   150       0.0465       0.0401      0.00637        0.046       0.0665         0.14        0.174
     60   160       0.0175       0.0131      0.00443       0.0259        0.038        0.132        0.158
     60   170       0.0189       0.0183     0.000566       0.0285        0.045         0.04       0.0534
     60   180        0.104        0.046        0.058       0.0467       0.0712        0.478        0.504
     60   190         84.4       0.0729         84.3       0.0621       0.0896         6.22         12.2
     60   200       0.0782       0.0339       0.0443       0.0416       0.0611         0.34        0.359
     60   210       0.0241       0.0179      0.00617       0.0362       0.0445        0.148        0.206
     60   220          1.5        0.335         1.17       0.0705        0.192        0.877         1.44
     60   230       0.0462     0.000272       0.0459      0.00307      0.00547        0.356        0.377
     60   231       0.0326       0.0264      0.00622       0.0497       0.0539        0.105        0.105

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     60     7       0.0248       0.0118        0.013        0.024       0.0361        0.191        0.229


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              60 3063.947    0.005       0.0224         1.14         1.17       0.0258       0.0495        0.278         1.44
! Validation         60 3063.947    0.005        0.011         3.12         3.13       0.0207       0.0345        0.464         2.45
Wall time: 3063.9486086526886

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     61    10        0.026      0.00626       0.0198       0.0123       0.0263        0.174        0.187
     61    20       0.0232       0.0158      0.00741       0.0318       0.0417        0.193        0.198
     61    30       0.0161      0.00807      0.00807       0.0197       0.0298         0.18        0.226
     61    40       0.0113      0.00451      0.00684       0.0137       0.0223        0.131        0.184
     61    50       0.0156       0.0091      0.00647       0.0183       0.0317        0.129        0.166
     61    60      0.00775      0.00411      0.00364       0.0142       0.0213       0.0982        0.142
     61    70       0.0182      0.00887      0.00933       0.0215       0.0313         0.17        0.181
     61    80        0.197       0.0049        0.192        0.014       0.0232        0.812         1.14
     61    90        0.025      0.00992       0.0151       0.0234       0.0331        0.216         0.27
     61   100       0.0349       0.0126       0.0223       0.0239       0.0373        0.196        0.198
     61   110       0.0127      0.00827      0.00442         0.02       0.0302       0.0858       0.0926
     61   120        0.346       0.0475        0.298       0.0492       0.0723        0.435        0.725
     61   130       0.0235       0.0191      0.00436       0.0321       0.0459       0.0892        0.103
     61   140       0.0118      0.00882      0.00298       0.0222       0.0312       0.0632       0.0724
     61   150        0.112       0.0563       0.0554       0.0603       0.0788        0.621        0.625
     61   160       0.0797       0.0468       0.0329       0.0548       0.0718        0.472        0.482
     61   170       0.0485       0.0373       0.0112       0.0478       0.0641        0.178        0.204
     61   180       0.0345       0.0305      0.00405        0.044        0.058       0.0946        0.102
     61   190       0.0221       0.0135      0.00865       0.0178       0.0386         0.19        0.231
     61   200       0.0129      0.00475      0.00818       0.0142       0.0229        0.164        0.191
     61   210       0.0183        0.014      0.00429       0.0266       0.0393       0.0949        0.104
     61   220        0.022        0.001        0.021      0.00695       0.0105        0.253          0.3
     61   230       0.0178      0.00928      0.00851       0.0244        0.032        0.212        0.245
     61   231       0.0193       0.0103      0.00891        0.027       0.0338        0.125        0.125

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     61     7         0.02       0.0111      0.00891       0.0225        0.035        0.158        0.192


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              61 3114.658    0.005       0.0209         1.14         1.16       0.0249       0.0456         0.28         1.44
! Validation         61 3114.658    0.005       0.0105         3.12         3.13       0.0209       0.0341        0.423         2.45
Wall time: 3114.6591829285026

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     62    10       0.0165      0.00861      0.00791        0.023       0.0308        0.108        0.129
     62    20       0.0129      0.00669      0.00624       0.0206       0.0272        0.133        0.154
     62    30       0.0218      0.00907       0.0128       0.0155       0.0316         0.21        0.248
     62    40       0.0219       0.0128      0.00915       0.0272       0.0376        0.128         0.14
     62    50           85       0.0122           85       0.0214       0.0367         6.18         12.2
     62    60       0.0498       0.0125       0.0374       0.0284       0.0371        0.388         0.43
     62    70       0.0673        0.024       0.0433       0.0283       0.0515        0.273        0.276
     62    80        0.079       0.0415       0.0374       0.0354       0.0677        0.308        0.384
     62    90       0.0323       0.0188       0.0135       0.0353       0.0456        0.203        0.247
     62   100       0.0675       0.0145        0.053       0.0247         0.04        0.243        0.306
     62   110       0.0241       0.0125       0.0117       0.0203       0.0371        0.242        0.259
     62   120       0.0143       0.0117      0.00266       0.0238       0.0358       0.0948        0.123
     62   130       0.0121        0.011      0.00111       0.0256       0.0348        0.061       0.0626
     62   140      0.00868      0.00691      0.00177       0.0196       0.0276       0.0769        0.082
     62   150       0.0226       0.0214      0.00121       0.0325       0.0486       0.0858       0.0923
     62   160       0.0166       0.0157     0.000862       0.0279       0.0416       0.0567       0.0661
     62   170       0.0119       0.0109     0.000955        0.021       0.0347       0.0484       0.0618
     62   180       0.0397      0.00611       0.0335        0.015        0.026        0.421        0.439
     62   190       0.0509       0.0225       0.0284         0.03       0.0497        0.282        0.347
     62   200         0.05       0.0288       0.0212       0.0403       0.0564        0.183        0.193
     62   210       0.0226      0.00912       0.0135       0.0199       0.0317        0.141         0.16
     62   220       0.0189       0.0177      0.00121       0.0315       0.0442       0.0496        0.055
     62   230      0.00844      0.00241      0.00603       0.0101       0.0163        0.144        0.181
     62   231      0.00987      0.00829      0.00158       0.0253       0.0302       0.0529       0.0529

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     62     7       0.0244       0.0142       0.0103       0.0257       0.0395        0.163        0.206


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              62 3165.279    0.005       0.0192         1.14         1.16       0.0247       0.0468        0.279         1.44
! Validation         62 3165.279    0.005       0.0119         3.12         3.13       0.0218       0.0359         0.43         2.45
Wall time: 3165.279886492528

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     63    10       0.0132        0.012      0.00116       0.0253       0.0364       0.0262       0.0453
     63    20       0.0173       0.0131      0.00419       0.0253        0.038       0.0939        0.109
     63    30       0.0146      0.00834      0.00625       0.0233       0.0303        0.129        0.175
     63    40      0.00833      0.00572      0.00261        0.018       0.0251       0.0598       0.0679
     63    50      0.00704      0.00606     0.000982       0.0167       0.0258       0.0638       0.0694
     63    60       0.0477      0.00929       0.0384       0.0199        0.032         0.37        0.381
     63    70       0.0686       0.0345        0.034        0.041       0.0617        0.359        0.437
     63    80       0.0324       0.0147       0.0177       0.0271       0.0402        0.233        0.261
     63    90       0.0256      0.00983       0.0158         0.02       0.0329        0.231        0.247
     63   100       0.0298       0.0185       0.0114       0.0251       0.0451        0.197        0.202
     63   110       0.0196      0.00964      0.00997       0.0206       0.0326        0.101        0.133
     63   120       0.0529      0.00532       0.0476       0.0126       0.0242        0.433        0.461
     63   130       0.0169      0.00974      0.00714       0.0269       0.0328        0.198        0.224
     63   140       0.0176      0.00336       0.0142       0.0119       0.0192        0.116        0.158
     63   150       0.0256       0.0201      0.00549       0.0359       0.0471       0.0772       0.0994
     63   160        0.113       0.0516       0.0612       0.0522       0.0754        0.639        0.657
     63   170       0.0744       0.0543         0.02       0.0594       0.0774        0.237        0.284
     63   180        0.169         0.01        0.159        0.021       0.0332        0.444        0.564
     63   190       0.0225        0.021      0.00154       0.0405       0.0481        0.062       0.0783
     63   200       0.0289       0.0235      0.00545        0.036       0.0509       0.0867        0.104
     63   210        0.181        0.174      0.00786       0.0816        0.138        0.129         0.16
     63   220       0.0122      0.00616      0.00605       0.0189       0.0261        0.182        0.207
     63   230       0.0187       0.0111      0.00759       0.0229        0.035        0.156        0.182
     63   231       0.0132     0.000144       0.0131      0.00298      0.00398        0.304        0.304

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     63     7       0.0222       0.0131       0.0091        0.026        0.038        0.152        0.195


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              63 3216.041    0.005        0.023         1.14         1.17       0.0256       0.0496        0.286         1.44
! Validation         63 3216.041    0.005       0.0111         3.12         3.13       0.0212       0.0346        0.422         2.45
Wall time: 3216.0424930639565

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     64    10       0.0163      0.00431        0.012       0.0108       0.0218        0.167        0.203
     64    20       0.0422       0.0204       0.0218       0.0344       0.0474        0.314        0.392
     64    30        0.151       0.0116        0.139       0.0268       0.0357        0.294        0.496
     64    40       0.0173       0.0145      0.00282       0.0255       0.0399       0.0867        0.111
     64    50       0.0403       0.0134       0.0269       0.0298       0.0384        0.148        0.224
     64    60       0.0818       0.0122       0.0696       0.0276       0.0366        0.402        0.688
     64    70      0.00965      0.00499      0.00466       0.0138       0.0235        0.137        0.155
     64    80       0.0413       0.0235       0.0178       0.0383       0.0509        0.247        0.254
     64    90       0.0789       0.0666       0.0123       0.0618       0.0856         0.19        0.217
     64   100       0.0448       0.0271       0.0177       0.0308       0.0546        0.266        0.293
     64   110       0.0371       0.0169       0.0201       0.0201       0.0432        0.327        0.335
     64   120       0.0166       0.0033       0.0133        0.013       0.0191        0.135        0.153
     64   130       0.0613       0.0292       0.0321       0.0361       0.0567        0.424        0.447
     64   140       0.0367       0.0195       0.0172       0.0283       0.0464        0.159        0.174
     64   150       0.0894      0.00371       0.0857       0.0128       0.0202        0.587        0.625
     64   160       0.0791      0.00645       0.0726       0.0154       0.0267        0.522        0.543
     64   170       0.0388      0.00233       0.0364       0.0104        0.016        0.282        0.309
     64   180       0.0282      0.00719        0.021       0.0197       0.0281        0.204         0.21
     64   190       0.0427       0.0328      0.00987        0.042       0.0602        0.189         0.25
     64   200       0.0619       0.0573      0.00458       0.0403       0.0795        0.116        0.144
     64   210        0.321        0.263       0.0585        0.114         0.17        0.355        0.411
     64   220       0.0313       0.0219      0.00943       0.0367       0.0491        0.219        0.258
     64   230       0.0235       0.0227     0.000846       0.0392         0.05       0.0446       0.0589
     64   231       0.0433        0.033       0.0103       0.0543       0.0603        0.134        0.134

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     64     7       0.0251       0.0159      0.00923       0.0279       0.0418        0.155        0.198


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              64 3266.802    0.005       0.0447         1.14         1.18       0.0323       0.0682        0.302         1.44
! Validation         64 3266.802    0.005       0.0137         3.12         3.13       0.0226       0.0381        0.416         2.45
Wall time: 3266.8032500538975

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     65    10      0.00985       0.0072      0.00265       0.0165       0.0282       0.0986        0.125
     65    20       0.0172        0.014      0.00323        0.028       0.0392       0.0699       0.0852
     65    30        0.012       0.0092      0.00284       0.0165       0.0318       0.0661       0.0762
     65    40      0.00701      0.00235      0.00466      0.00897       0.0161        0.113        0.132
     65    50       0.0115      0.00456      0.00697       0.0148       0.0224       0.0947        0.117
     65    60      0.00655      0.00557     0.000987       0.0186       0.0248       0.0622       0.0688
     65    70      0.00935      0.00589      0.00346       0.0146       0.0255       0.0892       0.0921
     65    80      0.00658      0.00523      0.00136       0.0134        0.024       0.0598       0.0682
     65    90      0.00597      0.00472      0.00125       0.0109       0.0228       0.0447       0.0498
     65   100       0.0997       0.0239       0.0758       0.0378       0.0513        0.395        0.731
     65   110       0.0501       0.0117       0.0384       0.0249       0.0359         0.46        0.481
     65   120        0.221        0.036        0.185       0.0355        0.063        0.952         1.01
     65   130       0.0604      0.00645        0.054       0.0185       0.0267        0.409        0.416
     65   140       0.0448       0.0126       0.0321       0.0197       0.0373        0.366        0.445
     65   150       0.0111      0.00216      0.00893      0.00929       0.0154        0.203        0.248
     65   160       0.0168      0.00629       0.0106       0.0174       0.0263        0.136        0.146
     65   170       0.0314       0.0108       0.0206       0.0158       0.0345        0.318        0.328
     65   180       0.0458        0.031       0.0148       0.0322       0.0584         0.25        0.278
     65   190      0.00986      0.00721      0.00265        0.018       0.0282       0.0632       0.0726
     65   200      0.00739       0.0046      0.00279       0.0142       0.0225       0.0609       0.0714
     65   210       0.0431       0.0292       0.0139       0.0409       0.0567         0.24        0.304
     65   220       0.0119       0.0115     0.000382       0.0247       0.0356       0.0272       0.0291
     65   230       0.0439       0.0297       0.0142       0.0394       0.0572        0.235        0.272
     65   231       0.0663        0.022       0.0443       0.0409       0.0493        0.559        0.559

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     65     7       0.0203       0.0131      0.00723       0.0247        0.038         0.14        0.178


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              65 3317.575    0.005       0.0207         1.15         1.17       0.0239       0.0471        0.269         1.45
! Validation         65 3317.575    0.005       0.0129         3.12         3.14       0.0206       0.0365        0.402         2.45
Wall time: 3317.576016788371

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     66    10       0.0918       0.0134       0.0784       0.0219       0.0384        0.457         0.48
     66    20        0.083       0.0513       0.0317       0.0576       0.0752        0.375        0.473
     66    30       0.0422       0.0368      0.00538       0.0409       0.0637       0.0949        0.105
     66    40       0.0155       0.0115      0.00402       0.0219       0.0356        0.104        0.124
     66    50       0.0233       0.0178      0.00548       0.0295       0.0443       0.0991        0.113
     66    60       0.0346      0.00794       0.0267       0.0224       0.0296        0.268        0.284
     66    70       0.0429       0.0116       0.0314       0.0245       0.0357        0.423         0.47
     66    80       0.0368      0.00831       0.0285       0.0217       0.0303         0.25        0.264
     66    90       0.0175       0.0171     0.000335       0.0318       0.0435       0.0374       0.0424
     66   100       0.0171     0.000974       0.0161      0.00532       0.0104        0.167        0.168
     66   110         0.01      0.00444       0.0056       0.0151       0.0221       0.0931        0.105
     66   120       0.0143      0.00867      0.00564       0.0185       0.0309        0.145        0.161
     66   130      0.00857      0.00592      0.00265       0.0153       0.0255       0.0995        0.116
     66   140      0.00788      0.00451      0.00337       0.0145       0.0223         0.11        0.138
     66   150       0.0468       0.0401      0.00668       0.0297       0.0665        0.183        0.217
     66   160       0.0171       0.0153      0.00176       0.0273       0.0411       0.0868       0.0978
     66   170       0.0258      0.00247       0.0233      0.00978       0.0165        0.282        0.344
     66   180         0.16       0.0031        0.157       0.0133       0.0185        0.496        0.584
     66   190       0.0183      0.00617       0.0121       0.0192       0.0261         0.19        0.231
     66   200        0.016       0.0106      0.00544       0.0241       0.0342        0.125        0.196
     66   210       0.0549       0.0291       0.0258       0.0341       0.0566        0.343        0.363
     66   220        0.026      0.00408       0.0219        0.013       0.0212         0.24        0.292
     66   230       0.0393       0.0265       0.0128       0.0353        0.054        0.141        0.157
     66   231       0.0224       0.0174      0.00504       0.0402       0.0438        0.189        0.189

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     66     7       0.0244        0.014       0.0104       0.0251       0.0393        0.169        0.207


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              66 3368.301    0.005       0.0221         1.15         1.17        0.026       0.0486        0.297         1.44
! Validation         66 3368.301    0.005       0.0124         3.12         3.13       0.0217       0.0363        0.438         2.45
Wall time: 3368.3016796642914

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     67    10       0.0203       0.0141      0.00624       0.0307       0.0394        0.157        0.186
     67    20        0.233        0.231      0.00223        0.115        0.159       0.0587       0.0627
     67    30         1.69         1.56        0.131        0.186        0.415        0.585        0.619
     67    40        0.111       0.0165       0.0942       0.0262       0.0426        0.584        0.662
     67    50       0.0326      0.00704       0.0256       0.0207       0.0278         0.26        0.288
     67    60       0.0315        0.021       0.0105       0.0308       0.0481         0.23        0.272
     67    70        0.011      0.00378      0.00723         0.01       0.0204        0.137        0.191
     67    80        0.401       0.0309         0.37       0.0484       0.0584        0.886         1.61
     67    90       0.0266       0.0144       0.0121       0.0272       0.0399        0.245        0.261
     67   100       0.0245       0.0144       0.0101       0.0239       0.0398        0.172        0.182
     67   110      0.00573      0.00295      0.00277       0.0113        0.018       0.0993        0.105
     67   120      0.00721      0.00535      0.00185       0.0124       0.0243       0.0532       0.0572
     67   130       0.0164        0.013      0.00341       0.0174       0.0378        0.118        0.135
     67   140       0.0206       0.0196        0.001       0.0275       0.0465       0.0704       0.0824
     67   150       0.0319       0.0265      0.00536       0.0335       0.0541       0.0808        0.099
     67   160        0.141       0.0142        0.127       0.0199       0.0396        0.719        0.802
     67   170          0.2       0.0132        0.186       0.0249       0.0381        0.828        0.858
     67   180       0.0702      0.00746       0.0627        0.019       0.0287        0.411        0.432
     67   190       0.0316      0.00603       0.0256       0.0158       0.0258        0.361        0.425
     67   200       0.0173      0.00521       0.0121       0.0172        0.024        0.233        0.246
     67   210      0.00972      0.00392       0.0058       0.0131       0.0208        0.132        0.161
     67   220       0.0681       0.0372       0.0309       0.0455        0.064        0.151        0.237
     67   230       0.0214       0.0148      0.00666       0.0263       0.0403        0.119        0.129
     67   231      0.00805      0.00406        0.004       0.0176       0.0211       0.0839       0.0839

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     67     7       0.0402       0.0143        0.026       0.0274       0.0396        0.274        0.326


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              67 3418.958    0.005       0.0425         1.13         1.18       0.0314       0.0671        0.342         1.45
! Validation         67 3418.958    0.005       0.0142         3.11         3.12       0.0234       0.0388        0.562         2.46
Wall time: 3418.959601096809
! Best model       67    3.124

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     68    10       0.0196       0.0177       0.0019       0.0286       0.0442       0.0495       0.0579
     68    20       0.0589       0.0539      0.00497       0.0472       0.0771        0.132        0.172
     68    30       0.0876       0.0858      0.00173       0.0666       0.0973       0.0778       0.0913
     68    40       0.0863       0.0149       0.0715        0.024       0.0405        0.609        0.625
     68    50       0.0841       0.0323       0.0518       0.0358       0.0597        0.601        0.605
     68    60         1.28       0.0244         1.26       0.0232       0.0518         1.01         1.49
     68    70       0.0629       0.0179       0.0449        0.027       0.0444        0.241        0.332
     68    80       0.0138      0.00137       0.0125      0.00801       0.0123        0.146        0.152
     68    90       0.0226       0.0155      0.00718       0.0247       0.0413         0.11        0.122
     68   100       0.0179       0.0101      0.00787       0.0234       0.0333        0.181        0.213
     68   110       0.0978       0.0125       0.0853        0.024        0.037        0.775        0.776
     68   120        0.116      0.00826        0.108       0.0179       0.0302        0.745         0.77
     68   130       0.0502       0.0144       0.0358        0.026       0.0399         0.34        0.348
     68   140       0.0478       0.0346       0.0132       0.0449       0.0618        0.174        0.236
     68   150       0.0256      0.00778       0.0178       0.0168       0.0293         0.28          0.3
     68   160       0.0117      0.00577       0.0059       0.0173       0.0252        0.126        0.171
     68   170       0.0529        0.044      0.00885       0.0399       0.0697         0.13        0.141
     68   180       0.0293       0.0181       0.0112       0.0317       0.0447         0.16        0.281
     68   190       0.0349       0.0143       0.0207       0.0181       0.0397        0.181        0.191
     68   200       0.0347       0.0266      0.00809       0.0409       0.0542          0.2        0.229
     68   210       0.0119      0.00356      0.00834       0.0117       0.0198         0.13        0.142
     68   220       0.0214       0.0162      0.00524       0.0313       0.0422       0.0857        0.102
     68   230       0.0128      0.00876      0.00407       0.0217       0.0311       0.0677       0.0855
     68   231      0.00223      0.00205      0.00018       0.0117        0.015       0.0178       0.0178

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     68     7       0.0256       0.0155       0.0102       0.0277       0.0413         0.17        0.209


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              68 3469.811    0.005       0.0277         1.15         1.18       0.0278        0.055        0.316         1.45
! Validation         68 3469.811    0.005       0.0135         3.12         3.13        0.022       0.0375        0.438         2.45
Wall time: 3469.8124131038785

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     69    10      0.00766      0.00497       0.0027       0.0153       0.0234       0.0908       0.0987
     69    20        0.055      0.00619       0.0488       0.0187       0.0261        0.358        0.371
     69    30       0.0599        0.018        0.042       0.0332       0.0445        0.391        0.452
     69    40       0.0147      0.00501       0.0097       0.0171       0.0235         0.19        0.226
     69    50       0.0201       0.0158      0.00428       0.0285       0.0417        0.123        0.155
     69    60       0.0489       0.0363       0.0125       0.0436       0.0633        0.194        0.243
     69    70       0.0632       0.0302        0.033       0.0428       0.0577        0.168        0.242
     69    80       0.0129      0.00895      0.00394       0.0232       0.0314        0.124        0.158
     69    90       0.0112       0.0048      0.00642       0.0161        0.023        0.126        0.155
     69   100      0.00904      0.00275      0.00628       0.0093       0.0174        0.133        0.156
     69   110       0.0423        0.041      0.00126       0.0466       0.0672        0.073       0.0751
     69   120       0.0356       0.0204       0.0152       0.0293       0.0474        0.228        0.239
     69   130       0.0657       0.0516       0.0141       0.0476       0.0754        0.214         0.25
     69   140       0.0185       0.0071       0.0114       0.0168        0.028          0.2        0.231
     69   150        0.017       0.0154      0.00163       0.0274       0.0412       0.0898        0.102
     69   160       0.0164       0.0044        0.012        0.011        0.022        0.257        0.291
     69   170      0.00943     0.000788      0.00864      0.00586      0.00932        0.104        0.123
     69   180       0.0256       0.0226      0.00304       0.0273       0.0499       0.0592       0.0748
     69   190       0.0234       0.0199      0.00355       0.0357       0.0468        0.094        0.105
     69   200       0.0274       0.0135        0.014       0.0249       0.0385        0.138        0.168
     69   210       0.0322       0.0075       0.0247       0.0164       0.0287        0.253        0.263
     69   220       0.0298        0.017       0.0128       0.0327       0.0433        0.256        0.271
     69   230       0.0231       0.0077       0.0154       0.0143       0.0291        0.258        0.316
     69   231       0.0253       0.0226      0.00274       0.0388       0.0499        0.139        0.139

validation
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     69     7       0.0207       0.0135      0.00721       0.0255       0.0386        0.138        0.171


  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse
! Train              69 3520.664    0.005       0.0207         1.14         1.16       0.0251       0.0468        0.265         1.44
! Validation         69 3520.664    0.005       0.0113         3.12         3.14       0.0211       0.0349        0.403         2.45
Wall time: 3520.6653679450974

training
# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae       e_rmse
     70    10       0.0422     0.000148       0.0421      0.00256      0.00404        0.272        0.272
     70    20       0.0532      0.00869       0.0445       0.0164        0.031        0.475        0.501
     70    30       0.0268     0.000646       0.0262      0.00607      0.00844        0.278        0.303
     70    40       0.0265      0.00653       0.0199       0.0153       0.0268        0.241        0.268
     70    50       0.0191       0.0117       0.0074       0.0259       0.0359        0.115        0.125
     70    60       0.0181       0.0146      0.00344       0.0321       0.0402       0.0835       0.0962
     70    70       0.0183       0.0131      0.00525        0.025        0.038        0.087       0.0995
     70    80      0.00956      0.00536       0.0042       0.0164       0.0243        0.114        0.165
     70    90       0.0123       0.0102      0.00214       0.0185       0.0335       0.0582       0.0736
     70   100       0.0149       0.0128      0.00216       0.0275       0.0375       0.0645       0.0794
     70   110      0.00851      0.00313      0.00537       0.0123       0.0186         0.12        0.154
     70   120        0.119       0.0508       0.0683       0.0507       0.0748        0.453        0.516
     70   130       0.0636       0.0385       0.0251       0.0505       0.0651        0.274        0.357
     70   140       0.0386       0.0168       0.0218       0.0254        0.043        0.342        0.392
     70   150       0.0338       0.0238         0.01       0.0344       0.0512        0.197        0.262
     70   160       0.0673       0.0135       0.0538       0.0301       0.0386        0.446        0.616
slurmstepd: error: *** JOB 10245748 ON gpu103 CANCELLED AT 2022-10-16T16:31:25 DUE TO TIME LIMIT ***
